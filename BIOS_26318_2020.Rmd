--- 
title: "Fundamentals of Biological Data Analysis"
author: "Dmitry Kondrashov and Stefano Allesina"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib]
biblio-style: apalike
link-citations: yes
description: "Course material for Fundamentals of Biological Data Analysis, BIOS 26318, AY 2020-2021"
---

# Organization of the class {-}

## Learning goals {-}

* `R` tools for visualizing and analyzing data
   - exploration of `tidyverse` 
   - `dplyr`, `tidyr` and `readr` for data wrangling and organization
   - `ggplot2` for visualization
   - specific packages and functions for statistical analysis

* Theory to perform statistical inference
   - assumptions of different methods
   - hypothesis testing
   - estimation of parameters
   - model building and selection
 
* Avoiding common errors
   - when (not) to use a statistical method
   - sneaky paradoxes
   - phantom effects

* Work on your own data
   - analyze data
   - produce graphics
   - write up a report
   - present to class

## Approach {-}

* Mix of theory and practice
* Apply what you're learning to your own data

## Materials {-}

#### Week 0 {-}

- `R` refresher \@ref(refresher)

#### Week 1 {-}

- Using `ggplot2` to produce publication-ready figures
- Review of probability

#### Week 2 {-}

- Data wrangling in `tidyverse`
- Probability distributions

#### Week 3 {-}

- Hypothesis testing
- Linear algebra refresher

#### Week 4 {-}

- Linear models
- Likelihood

#### Week 5 {-}

- Analysis of variance
- Modeling timeseries data

#### Week 6 {-}

- Generalized Linear Models
- Model selection

#### Week 7 {-}

- Principal Component Analysis
- Multidimensional Scaling and Clustering

#### Week 8 {-}

- Phylogenetic reconstruction
- Machine Learning and cross validation

#### Week 9 {-}

Thanksgiving break

#### Week 10 {-}

- Student presentations 1
- Student presentations 2

## Acknowledgements {-}

Zach Miller for TAing the first iteration of the class, and for contributing materials and comments, Julia Smith for TAing the current version. Development of the class was partially supported by the Burroughs Wellcome Fund through the program ["Quantitative and statistical thinking in the life sciences"](https://www.bwfund.org/grant-programs/institutional-programs/quantitative-and-statistical-thinking-life-sciences/grant) (Stefano Allesina, PI).

<!--chapter:end:index.Rmd-->

# `R`efresher {#refresher}

## Goal

Introduce the stastical software `R`, and show how it can be used to analyze biological data in an automated, replicable way. Showcase the `RStudio` development environment, illustrate the notion of assignment, present the main data structures available in `R`. Show how to read and write data, how to execute simple programs, and how to modify the stream of execution of a program through conditional branching and looping. Introduce the use of packages and  user-defined functions. 

## Motivation

When it comes to analyzing data, there are two competing paradigms. First, one could use point-and-click software with a graphical user interface, such as Excel, to perform calculations and draw graphs; second, one could write programs that can be run to perform the analysis of the data, the generation of tables and statistics, and the production of figures automatically. 

This latter approach is to be preferred, because it allows for the automation of analysis, it requires a good documentation of the procedures, and is completely replicable. 

A few motivating examples:

- You have written code to analyze your data. You receive from your collaborators a new batch of data. With simple modifications of your code, you can update your results, tables and figures automatically.

- A new student joins the laboratory. The new student can read the code and understand the analysis without the need of a lab mate showing the procedure step-by-step.

- The reviewers of your manuscript ask you to slightly alter the analysis. Rather than having to start over, you can modify a few lines of code and satisfy the reviewers.

Here we introduce `R`, which can help you write simple programs to analyze your data, perform statistical analysis, and draw beautiful figures.

## Before we start
To follow this tutorial, you will need to install `R` and `RStudio`

* Install `R`: download and install `R` from [this page](https://cran.r-project.org/). Choose the right architecture (Windows, Mac, Linux). If possible, install the latest release.
* Install `RStudio`: go to [this page](https://www.rstudio.com/products/rstudio/download/) and download the "RStudio Desktop Open Source License".
* Install `R` packages: launch `RStudio`. Click on "Packages" in the bottow-right panel. Click on "Install": a dialog window will open. Type `tidyverse` in the field "Packages" and click on "Install". This might take a few minutes, and ask you to download further packages.

## What is R?

`R` is a statistical software that is completely programmable. This means that one can write a program (script) containing a series of commands for the analysis of data, and execute them automatically. This approach is especially good as it makes the analysis of data well-documented, and completely replicable. 

`R` is free software: anyone can download its source code, modify it, and improve it. The `R` community of users is vast and very active. In particular, scientists have enthusiastically embraced the program, creating thousands of packages to perform specific types of analysis, and adding many new capabilities. You can find a list of official packages (which have been vetted by `R` core developers) [here](https://cran.r-project.org/web/packages/available_packages_by_name.html); many more are available on GitHub and other websites.

The main hurdle new users face when approaching `R` is that it is based on a command line interface: when you launch `R`, you simply open a console with the character `>` signaling that `R` is ready to accept an input. When you write a command and press `Enter`, the command is interpreted by `R`, and the result is printed immediately after the command. For example,

```{r}
1 + 1
```

A little history: `R` was modeled after the commercial statistical software `S` by Robert Gentleman and Ross Ihaka. The project was started in 1992, first released in 1994, and the first stable version appeared in 2000. Today, `R` is managed by the *R Core Team*.

## RStudio

For this introduction, we're going to use `RStudio`, an Integrated Development Environment (IDE) for `R`. The main advantage is that the environment will look identical irrespective of your computer architecture (Linux, Windows, Mac). Also, `RStudio` makes writing code much easier by automatically completing commands and file names (simply type the beginning of the name and press `Tab`), and allowing you to easily inspect data and code.

Typically, an `RStudio` window contains four panels:

- **Console** This is a panel containing an instance of `R`. For this tutorial, we will work mainly in this panel.
- **Source code** In this panel, you can write a program, save it to a file pressing `Ctrl + S` and then execute it by pressing `Ctrl + Shift + S`.
- **Environment** This panel lists all the variables you created (more on this later); another tab shows you the history of the commands you typed.
- **Plots** This panel shows you all the plots you drew. Other tabs allow you to access the list of packages you have loaded, and the help page for commands (just type `help(name_of_command)` in the Console) and packages.

## How to write a simple program

An `R` program is simply a list of commands, which are executed one after the other. The commands are witten in a text file (with extension `.R`). When `R` executes the program, it will start from the beginning of the file and proceed toward the end of the file. Every time `R` encounters a command, it will execute it. Special commands can modify this basic flow of the program by, for example, executing a series of commands only when a condition is met, or repeating the execution of a series of commands multiple times.

Note that if you were to copy and paste (or type) the code into the **Console** you would obtain exactly the same result. Writing a program is advatageous, however, because the analysis can be automated, and the code shared with other researchers. Moreover, after a while you will have a large code base, so that you can recycle much of your code.

We start by working on the console, and then start writing simple scripts.

### The most basic operation: assignment

The most basic operation in any programming language is the assignment. In `R`, assignment is marked by the operator `<-` (can be typed quickly using `Alt -`). When you type a command in `R`, it is executed, and the output is printed in the **Console**. For example:

```{r}
sqrt(9)
```

If we want to save the result of this operation, we can assign it to a variable. For example:

```{r}
x <- sqrt(9)
x
```

What has happened? We wrote a command containing an assignment operator (`<-`). `R` has evaluated the right-hand-side of the command (`sqrt(9)`), and has stored the result (`3`) in a newly created variable called `x`. Now we can use `x` in our commands: every time the command needs to be evaluated, the program will look up which value is associated with the variable `x`, and substitute it. For example:

```{r}
x * 2 
```

### Data types

`R` provides different types of data that can be used in your programs. For each variable `x`, calling `class(x)` prints the type of the variable. The basic data types are:

- `logical`, taking only two possible values: `TRUE` and `FALSE`

```{r}
v <- TRUE
class(v)
```

- `numeric`, storing real numbers (actually, their approximations, as computers have limited memory and thus [cannot store](https://www.exploringbinary.com/why-0-point-1-does-not-exist-in-floating-point/) numbers like Ï€, or even `0.2`)

```{r}
v <- 3.77
class(v)
```

- Real numbers can also be specified using scientific notation:

```{r}
v <- 6.022e23 # 6.022â‹…10^23 (Avogadro's number)
class(v)
```

- `integer`, storing whole numbers 

```{r}
v <- 23L # the L signals that this should be stored as integer
class(v)
```

- `complex`, storing complex numbers (i.e., with a real and an imaginary part)

```{r}
v <- 23 + 5i # the i marks the imaginary part
class(v)
```

- `character`, for strings, characters and text

```{r}
v <- 'a string' # you can use single or double quotes
class(v)
```

In `R`, the value and type of a variable are evaluated at runtime. This means that you can recycle the names of variables. This is very handy, but can make your programs more difficult to read and to debug (i.e., find mistakes). For example:

```{r}
x <- '2.3' # this is a string
x
x <- 2.3 # this is numeric
x
```

### Operators and functions

Each data type supports a certain number of operators and functions. For example, numeric variables can be combined with `+` (addition), `-` (subtraction), `*` (multiplication), `/` (division), and `^` (or `**`, exponentiation). A possibly unfamiliar operator is the modulo (`%%`), calculating the remainder of an integer division:

```{r}
5 %% 3
```

meaning that `5 %/% 3` (5 integer divided by 3) is 1 with a remainder of 2

The modulo operator is useful to determine whether a number is divisible for another: if `y` is divisible by `x`, then `y %% x` is 0.

`R` provides many built-in functions: each functions has a name, followed by round parentheses surrounding the (possibly optional) function *arguments*. For example, these functions operate on `numeric` variables:

- `abs(x)` absolute value
- `sqrt(x)` square root
- `round(x, digits = 3)` round `x` to three decimal digits
- `cos(x)` cosine (also supported are all the usual trigonometric functions)
- `log(x)` natural logarithm (use `log10` for base 10 logarithms)
- `exp(x)` calculating $e^x$

Similarly, `character` variables have their own set of functions, such as: 

- `toupper(x)` make uppercase
- `nchar(x)` count the number of characters in the string
- `paste(x, y, sep = "_")` concatenate strings, joining them using the separator `_`
- `strsplit(x, "_")` separate the string using the separator `_`

Calling a function meant for a certain data type on another will cause errors. If sensible, you can convert a type into another. For example:

```{r}
v <- "2.13"
class(v)
# if we call v * 2, we get an error.
# to avoid it, we can convert v to numeric:
as.numeric(v) * 2 
```

If sensible, you can use the comparison operators `>` (greater), `<` (lower), `==` (equals), `!=` (differs), `>=` and `<=`, returning a logical value:

```{r}
2 == sqrt(4)
2 < sqrt(4)
2 <= sqrt(4)
```

> **Exercise:** 
> 
> Why are two equal signs (`==`) used to check that two values are equal? What happens if you use only one `=` sign?

Similarly, you can concatenate several comparison and logical variables using `&` (and), `|` (or), and `!` (not):

```{r}
(2 > 3) & (3 > 1)
(2 > 3) | (3 > 1)
```

### Getting help

If you want to know more about a function, type `?my_function_name` in the console (e.g., `?abs`). This will open the help page in one of the panels on the right. The same can be accomplished calling `help(abs)`. For more complex questions, check out stackoverflow. 

### Data structures

Besides these simple types, `R` provides structured data types, meant to collect and organize multiple values.

#### Vectors

The most basic data structure in `R` is the vector, which is an ordered collection of values of the same type. Vectors can be created by concatenating different values with the function `c()` ("combine"):

```{r}
x <- c(2, 3, 5, 27, 31, 13, 17, 19) 
x
```

You can access the elements of a vector by their index: the first element is indexed at 1, the second at 2, etc.
```{r}
x[3]
x[8]
x[9] # what if the element does not exist?
```

`NA` stands for "Not Available". Other special values are `NaN` (Not a Number, e.g., `0/0`), `Inf` (Infinity, e.g., `1/0`), and `NULL` (variable undefined). You can test for special values using `is.na(x)`, `is.infinite(x)`, `is.null(x)`, etc.

Note that in `R` a single number (string, logical) is a vector of length 1 by default. That's why if you type `3` in the console you see `[1] 3` in the output.

You can extract several elements at once (i.e., create another vector), using the colon (`:`) command, or by concatenating the indices:

```{r}
x[1:3]
x[4:7]
x[c(1,3,5)]
```

You can also use a vector of logical variables to extract values from vectors. For example, suppose we have two vectors:
```{r}
sex <- c("M", "M", "F", "M", "F") # sex of Drosophila
weight <- c(0.230, 0.281, 0.228, 0.260, 0.231) # weigth in mg
```

and that we want to extract only the weights for the males. 

```{r}
sex == "M"
```

returns a vector of logical values, which we can use to subset the data:
```{r}
weight[sex == "M"]
```

Given that `R` was born for statistics, there are many statistical
functions you can perform on vectors:
```{r}
length(x)
min(x)
max(x)
sum(x) # sum all elements
prod(x) # multiply all elements
median(x) # median value
mean(x) # arithmetic mean
var(x) # unbiased sample variance
mean(x ^ 2) - mean(x) ^ 2 # population variance
summary(x) # print a summary
```

You can generate vectors of sequential numbers using the colon
command:
```{r}
x <- 1:10
x
```

For more complex sequences, use `seq`:
```{r}
seq(from = 1, to = 5, by = 0.5)
```

To repeat a value or a sequence several times, use `rep`:
```{r}
rep("abc", 3)
rep(c(1, 2, 3), 3)
```

> **Exercise:** 
> 
> - Create a vector containing all the even numbers between 2 and 100 (inclusive) and store it in variable `z`.
> - Extract all the elements of `z` that are divisible by 12. How many elements match this criterion?
> - What is the sum of all the elements of `z`?
> - Is it equal to $51 \cdot 50$?
> - What is the product of elements 5, 10 and 15 of `z`?
> - Does `seq(2, 100, by = 2)` produce the same vector as `(1:50) * 2`?
> - What happens if you type `z ^ 2`?

#### Matrices

A matrix is a two-dimensional table of values. In case of numeric values, you can perform the usual operations on matrices (product, inverse, decomposition, etc.):

```{r}
A <- matrix(c(1, 2, 3, 4), 2, 2) # values, nrows, ncols
A 
A %*% A # matrix product
solve(A) # matrix inverse
A %*% solve(A) # this should return the identity matrix
B <- matrix(1, 3, 2) # you can fill the whole matrix with a single number (1)
B
B %*% t(B) # transpose
Z <- matrix(1:9, 3, 3) # by default, matrices are filled by column
Z
```

To determine the dimensions of a matrix, use `dim`:
```{r}
dim(B)
dim(B)[1]
nrow(B) 
dim(B)[2]
ncol(B)
```

Use indices to access a particular row/column of a matrix:
```{r}
Z
Z[1, ] # first row
Z[, 2] # second column
Z [1:2, 2:3] # submatrix with coefficients in first two rows, and second and third column
Z[c(1, 3), c(1, 3)] # indexing non-adjacent rows/columns
```

Some functions use all the elements of the matrix:
```{r}
sum(Z)
mean(Z)
```

Some functions apply the operation across a given dimension (e.g., columns) of the matrix:
```{r}
rowSums(Z) # returns a vector of the sums of the values in each row
colSums(Z) # does the same for columns
rowMeans(Z) # returns a vector of the means of the values in each row
colMeans(Z) # does the same for columns
```

#### Arrays

If you need tables with more than two dimensions, use arrays:
```{r}
M <- array(1:24, c(4, 3, 2))
M 
```

You can still determine the dimensions using:
```{r}
dim(M)
```

and access the elements as done for matrices. One thing you should be paying attention to: `R` drops dimensions that are not needed. So, if you access a "slice" of a 3-dimensional array:

```{r}
M[, , 1]
``` 

you obtain a matrix:
```{r}
dim(M[, , 1])
```

This can be problematic, for example, when your code expects an array and `R` turns your data into a matrix (or you expect a matrix but find a vector). To avoid this behavior, add `drop = FALSE` when subsetting:
```{r}
dim(M[, , 1, drop = FALSE])
```

#### Lists
Vectors are good if each element is of the same type (e.g., numbers, strings). Lists are used when we want to store elements of different types, or more complex objects (e.g., vectors, matrices, even lists of lists). Each element of the list can be referenced either by its index, or by a label:

```{r}
mylist <- list(Names = c("a", "b", "c", "d"), Values = c(1, 2, 3))
mylist
mylist[[1]] # access first element using index
mylist[[2]] # access second element by index
mylist$Names # access second element by label
mylist[["Names"]] # another way to access by label
mylist[["Values"]][3]  # access third element in second vector
```

#### Data frames

Data frames contain data organized like in a spreadsheet. The columns (typically representing different measurements) can be of different types (e.g., a column could be the date of measurement, another the weight of the individual, or the volume of the cell, or the treatment of the sample), while the rows typically represent different samples.

When you read a spreadsheet file in `R`, it is automatically stored as a data frame. The difference between a matrix and a data frame is that in a matrix all the values are of the same type (e.g., all numeric), while in a data frame each column can be of a different type.

Because typing a data frame by hand would be tedious, let's use a data set that is already available in `R`:
```{r}
data(trees) # girth, height and volume of cherry trees
str(trees) # structure of data frame
ncol(trees)
nrow(trees)
head(trees) # print the first few rows
summary(trees) # Quickly get an overview of the data frame.
trees$Girth # select column by name
trees$Height[1:5] # select column by name; return first five elements
trees[1:3, ] #select rows 1 through 3
trees[1:3, ]$Volume # select rows 1 through 3; return column Volume
trees <- rbind(trees, c(13.25, 76, 30.17)) # add a row
trees_double <- cbind(trees, trees) # combine columns
colnames(trees) <- c("Circumference", "Height", "Volume") # change column names
```

> **Exercise:** 
> 
> - What is the average height of the cherry trees?
> - What is the average girth of those that are more than 75 ft tall?
> - What is the maximum height of trees with a volume between 15 and 35 ft$^3$?

## Reading and writing data

In most cases, you will not generate your data in `R`, but import it from a file. By far, the best option is to have your data in a comma separated value text file or in a tab separated file. Then, you can use the function `read.csv` (or `read.table`) to import your data. The syntax of the functions is as follows:

```{r, eval = FALSE}
read.csv("MyFile.csv") # read the file MyFile.csv
read.csv("MyFile.csv", header = TRUE) # the file has a header
read.csv("MyFile.csv", sep = ';') # specify the column separator
read.csv("MyFile.csv", skip = 5) # skip the first 5 lines
```

Note that columns containing strings are typically converted to *factors* (categorical values, useful when performing regressions). To avoid this behavior, you can specify `stringsAsFactors = FALSE` when calling the function.

Similarly, you can save your data frames using `write.table` or `write.csv`. Suppose you want to save the data frame `MyDF`:

```{r, eval = FALSE}
write.csv(MyDF, "MyFile.csv") 
write.csv(MyDF, "MyFile.csv", append = TRUE) # append to the end of the file 
write.csv(MyDF, "MyFile.csv", row.names = TRUE) # include the row names
write.csv(MyDF, "MyFile.csv", col.names = FALSE) # do not include column names
```

Let's look at an example: Read a file containing data on the 6th chromosome for a number of Europeans (Data adapted from [Stanford HGDP SNP Genotyping Data](hagsc.org/hgdp/) by John   Novembre). This example shows that you can read data directly from the internet!
```{r}
# The actual URL is
# https://github.com/StefanoAllesina/BSD-QBio4/raw/master/tutorials/basic_computing_1/data/H938_Euro_chr6.geno
ch6 <- read.table("https://tinyurl.com/y7vctq3v", 
                  header = TRUE, stringsAsFactors = FALSE)
```

where `header = TRUE` means that we want to take the first line to be a header containing the column names. How big is this table?
```{r}
dim(ch6)
```

we have 7 columns, but more than 40k rows! Let's see the first few:
```{r}
head(ch6)
```

and the last few:
```{r}
tail(ch6)
```

The data contains the number of homozygotes (`nA1A1`, `nA2A2`) and heterozygotes (`nA1A2`), for 43,141 single nucleotide polymorphisms (SNPs) obtained by sequencing European individuals:

- `CHR` The chromosome (6 in this case)
- `SNP` The identifier of the Single Nucleotide Polymorphism
- `A1` One of the alleles
- `A2` The other allele
- `nA1A1` The number of individuals with the particular combination of alleles.

> **Exercise:** 
> 
> - How many individuals were sampled? Find the maximum of the sum `nA1A1 + nA1A2 + nA2A2`. Note: you can access the columns by index (e.g., `ch6[,5]`), or by name (e.g., `ch6$nA1A1`, or also `ch6[,"nA1A1"]`).
> - Try using the function `rowSums` to obtain the same result.
> - For how many SNPs do we have that all sampled individuals are homozygotes (i.e., all `A1A1` or all `A2A2`)?
> - For how many SNPs, are more than 99% of the sampled individuals homozygous?

## Conditional branching

Now we turn to writing actual programs in the **Source code** panel. To start a new `R` program, press `Ctrl + Shift + N`. This will open an `Untitled` script. Save the script by pressing `Ctrl + S`: save it as `conditional.R` in the directory `programming_skills/sandbox/`. To make sure you're working in the directory where the script is contained, on the menu on the top choose `Session -> Set Working Directory -> To Source File Location`.

Now type the following script:
```{r, eval = FALSE}
print("Hello world!")
x <- 4
print(x)
```
and execute the script by pressing `Ctrl + Shift + S`. You should see `Hello World!` and `4` printed in your console.

As you saw in this simple example, when `R` executes the program, it starts from the top and proceeds toward the end of the file. Every time it encounters a command (for example, `print(x)`, printing the value of `x` into the console), it executes it.

When we want a certain block of code to be executed only when a certain condition is met, we can write a conditional branching point. The syntax is as follows:
```{r, eval = FALSE}
if (condition is met){
  # execute this block of code
} else {
  # execute this other block of code
}
```

For example, add these lines to the script `conditional.R`, and run it again:
```{r, eval = FALSE}
print("Hello world!")
x <- 4
print(x)
if (x %% 2 == 0){
  my_message <- paste(x, "is even")
} else {
  my_message <- paste(x, "is odd")
}
print(my_message)
```

We have created a conditional branching point, so that the value of `my_message` changes depending on whether `x` is even (and thus the remainder of the integer division by 2 is 0), or odd. Change the line `x <- 4` to `x <- 131` and run it again.

> **Exercise:** What does this do?
> ```{r}
x <- 36
if (x > 20){
  x <- sqrt(x)
} else {
  x <- x ^ 2
}
if (x > 7) {
  print(x)
} else if (x %% 2 == 1){
  print(x + 1)
}
```

## Looping

Another way to change the flow of the program is to write a loop.  A loop is simply a series of commands that are repeated a number of times. For example, you want to run the same analysis on different data sets that you collected; you want to plot the results contained in a set of files; you want to test your simulation over a number of parameter sets; etc.

`R` provides you with two ways to loop over blocks of commands: the `for` loop, and the `while` loop. Let's start with the `for` loop, which is used to iterate over a vector (or a list): for each value of the vector, a series of commands will be run, as shown by the following example, which you can type in a new script called `forloop.R`.

```{r, eval=FALSE}
myvec <- 1:10 # vector with numbers from 1 to 10

for (i in myvec) {
  a <- i ^ 2
  print(a)
}
```

In the code above, the variable `i` takes the value of each element of `myvec` in sequence. Inside the block defined by the `for` loop, you can use the variable `i` to perform operations.

The anatomy of the `for` statement:

```{r, eval=FALSE}
for (variable in list_or_vector) {
  execute these commands
} # automatically moves to the next value
```

For loops are used when you know that you want to perform the analysis using a given set of values (e.g., run over all files of a directory, all samples in your data, all sequences of a fasta file, etc.).

The `while` loop is used when the commands need to be repeated while a certain condition is true, as shown by the following example, which you can type in a script called `whileloop.R`:

```{r, eval=FALSE}
i <- 1

while (i <= 10) {
  a <- i ^ 2
  print(a)
  i <- i + 1 
}
```

The script performs exactly the same operations we wrote for the `for` loop above. Note that you need to update the value of `i`, (using `i <- i + 1`), otherwise the loop will run forever (infinite loop---to terminate click on the stop button in the top-right corner of the console). The anatomy of the `while` statement:

```{r, eval=FALSE}
while (condition is met) {
  execute these commands
} # beware of infinite loops: remember to update the condition!
```

You can break a loop using the command `break`. For example:

```{r, eval=FALSE}
i <- 1

while (i <= 10) {
  if (i > 5) {
    break
  }
  a <- i ^ 2
  print(a)
  i <- i + 1
}
```

> **Exercise**: What does this do? Try to guess what each loop does, and then create
and run a script to confirm your intuition.
> ```{r, eval=FALSE}
z <- seq(1, 1000, by = 3)
for (k in z) {
  if (k %% 4 == 0) {
     print(k)
  }
}
```
> ```{r, eval=FALSE}
z <- readline(prompt = "Enter a number: ")
z <- as.numeric(z)
isthisspecial <- TRUE
i <- 2
while (i < z) {
  if (z %% i == 0) {
     isthisspecial <- FALSE
     break
  }
  i <- i + 1
}
if (isthisspecial == TRUE) {
  print(z)
}
```

## Useful Functions
Here's a short list of useful functions that will help you write your programs:

- `range(x)`: minimum and maximum of a vector `x`
- `sort(x)`: sort a vector `x`
- `unique(x)`: remove duplicate entries from vector `x`
- `which(x == a)`: returns a vector of the indices of `x` having value `a`
- `list.files("path_to_directory")`: list the files in a directory (current directory if not specified)
- `table(x)` build a table of frequencies

> **Exercises:** What does this code do? For each snippet of code, first try to guess what will happen. Then, write a script and run it to confirm your intuition.
> ```{r, eval = FALSE}
v <- c(1, 3, 5, 5, 3, 1, 2, 4, 6, 4, 2)
v <- sort(unique(v))
for (i in v){
  if (i > 2){
    print(i)
  }
  if (i > 4){
    break
  }
}
```
> ```{r, eval = FALSE}
x <- 1:100
x <- x[which(x %% 7 == 0)]
```
> ```{r, eval = FALSE}
my_amount <- 10
while (my_amount > 0){
  my_color <- NA
  while(is.na(my_color)){
    tmp <- readline(prompt="Do you want to bet on black or red? ")
    tmp <- tolower(tmp)
    if (tmp == "black") my_color <- "black"
    if (tmp == "red") my_color <- "red"
    if (is.na(my_color)) print("Please enter either red or black")
  }
  my_bet <- NA
  while(is.na(my_bet)){
    tmp <- readline(prompt="How much do you want to bet? ")
    tmp <- as.numeric(tmp)
    if (is.numeric(tmp) == FALSE){
      print("Please enter a number")
    } else {
      if (tmp > my_amount){
        print("You don't have enough money!")
      } else {
        my_bet <- tmp
        my_amount <- my_amount - tmp
      }
    }
  }
  lady_luck <- sample(c("red", "black"), 1)
  if (lady_luck == my_color){
    my_amount <- my_amount + 2 * my_bet
    print(paste("You won!! Now you have", my_amount, "gold doubloons"))
  } else {
    print(paste("You lost!! Now you have", my_amount, "gold doubloons"))
  }
}
```

## Packages
`R` is the most popular statistical computing software among biologists due to its highly specialized packages, often written by biologists for biologists. You can contribute a package too! The `RStudio` support [(`goo.gl/harVqF`)](http://goo.gl/harVqF) provides guidance on how to start developing `R` packages  and Hadley Wickham's free online book [(`r-pkgs.had.co.nz`)](http://r-pkgs.had.co.nz) will make you a pro.

You can find highly specialized packages to address your research questions. Here are some suggestions for finding an appropriate package. The Comprehensive R Archive Network (CRAN) offers several ways to find specific packages for your task. You can either browse packages [(`goo.gl/7oVyKC`)](http://goo.gl/7oVyKC) and their short description or select a scientific field of interest [(`goo.gl/0WdIcu`)](http://goo.gl/0WdIcu) to browse through a compilation of packages related to each discipline.

From within your `R` terminal or `RStudio` you can also call the function `RSiteSearch("KEYWORD")`, which submits a search query to the website [`search.r-project.org`](http://search.r-project.org). The website [`rseek.org`](http://rseek.org) casts an even wider net, as it not only includes package names and their documentation but also blogs and mailing lists related to `R`. If your research interests relate to high-throughput genomic data, you should have a look the packages provided by  Bioconductor [(`goo.gl/7dwQlq`)](http://goo.gl/7dwQlq).

### Installing a package
To install a package type

```{r, eval = FALSE}
install.packages("name_of_package")
```

in the **Console**, or choose the panel **Packages** and then click on *Install* in `RStudio`.

### Loading a package

To load a package type
```{r, eval=FALSE}
library(name_of_package)
```

or call the command into your script. If you want your script to automatically install a package in case it's missing, use this boilerplate:
```{r, eval = FALSE}
if (!require(needed_package, character.only = TRUE, quietly = TRUE)) {
    install.packages(needed_package)
    library(needed_package, character.only = TRUE)
}
```

### Example

For example, say we want to access the dataset `bacteria`, which reports the incidence of *H. influenzae* in Australian children. The dataset is contained in the package `MASS`.

First, we need to load the package:

```{r}
library(MASS)
```

Now we can load the data:
```{r}
data(bacteria)
bacteria[1:3,]
```

## Random numbers

To perform randomization, or any simulation, we typically need to draw random numbers. `R` has functions to sample random numbers from very many different statistical distributions. For example:

```{r}
runif(5) # sample 5 numbers from the uniform distribution between 0 and 1
runif(5, min = 1, max = 9) # set the limits of the uniform distribution
rnorm(3) # three values from standard normal
rnorm(3, mean = 5, sd = 4) # specify mean and standard deviation
```

To sample from a set of values, use `sample`:
```{r}
v <- c("a", "b", "c", "d")
sample(v, 2) # without replacement
sample(v, 6, replace = TRUE) # with replacement
sample(v) # simply shuffle the elements
```

## Writing functions

The `R` community provides about 7,000 packages. Still, sometimes there isn't an already made function capable of doing what you need. In these cases, you can write your own functions. In fact, it is generally a good idea to always divide your analysis into functions, and then write a small "master" program that calls the functions and performs the analysis. In this way, the code will be much more legible, and you will be able to recycle the functions for your other projects.

A function in `R` has this form:
```{r, eval = FALSE}
my_function_name <- function(optional, arguments, separated, by_commas){
  # Body of the function
  # ...
  # 
  return(return_value) # this is optional
}
```

A few examples:
```{r}
sum_two_numbers <- function(a, b){
  apb <- a + b  
  return(apb)
}
sum_two_numbers(5, 7.2)
```

You can set a default value for some of the arguments: if not specified by the user, the function will use these defaults:
```{r}
sum_two_numbers <- function(a = 1, b = 2){
  apb <- a + b  
  return(apb)
}
sum_two_numbers()
sum_two_numbers(3)
sum_two_numbers(b = 9)
```

The return value is optional:
```{r}
my_factorial <- function(a = 6){
  if (as.integer(a) != a) {
    print("Please enter an integer!")
  } else {
    tmp <- 1
    for (i in 2:a){
      tmp <- tmp * i
    }
    print(paste(a, "! = ", tmp, sep = ""))
  }
}
my_factorial()
my_factorial(10)
```

You can return **only one** object. If you need to return multiple values, organize them into a vector/matrix/list and return that.
```{r}
order_two_numbers <- function(a, b){
  if (a > b) return(c(a, b)) #nothing after the first return is executed
  return(c(b,a))
}

order_two_numbers(runif(1), runif(1))
```

## Organizing and running code

During the class, we will write a lot of code, of increasing complexity. Here is what you should do to ensure that your programs are well-organized, easy to understand, and easy to debug.

1. Take the problem, and divide it into its basic building blocks. Each block should be its own function.
2. Write the code for each building block separately, and test it thoroughly.
3. Extensively document the code, so that you can understand what you did, how you did it, and why.
4. Combine the building blocks into a master program.

For example, let's write code that takes the data on Chromosome 6 we have seen above, and tries to identify which SNPs deviate the most from Hardy-Weinberg equilibrium. Remember that in an infinite population, where mating is random, there is no selection and no mutations, the proportion of people carrying the alleles $A1A1$ should be approximately $p_{11} = p^2$ (where $p$ is the frequency of the first allele in the population $p = p_{11} + \frac{1}{2} p_{12}$), those carrying $A1A2$ should be $p_{12} = 2 p q$ (where $q = 1-p$) and finally those carrying $A2A2$ should be $p_{22} = q^2$. This is called the Hardy-Weinberg equilibrium.

We want to test this on a number of different SNPs. First, we write a function that takes as input the data and a given SNP, and computes the probability $p$ of carrying the first allele.

```{r}
compute_probabilities_HW <- function(my_data, my_SNP = "rs1535053"){
  # Take a SNP and compute the probabilities
  # p = frequency of first allele
  # q = frequency of second allele (1 - p)
  # p11 = proportion homozygous first allele
  # p12 = proportion heterozygous
  # p22 = proportion homozygous second allele
  my_SNP_data <- my_data[my_data$"SNP" == my_SNP,]
  AA <- my_SNP_data$nA1A1
  AB <- my_SNP_data$nA1A2
  BB <- my_SNP_data$nA2A2
  tot_observations <- AA + AB + BB
  p11 <- AA / tot_observations
  p12 <- AB / tot_observations
  p22 <- BB / tot_observations
  p <- p11 + p12 / 2
  q <- 1 - p
  return(list(SNP = my_SNP,
              p11 = p11,
              p12 = p12,
              p22 = p22,
              p = p,
              q = q,
              tot = tot_observations,
              AA = AA,
              AB = AB,
              BB = BB))
}
```

Now we can test our function:
```{r}
compute_probabilities_HW(ch6)
```

If the allele conformed to Hardy-Weinberg, we should find approximately $p^2 \cdot n$ people with $A1A1$, where $n$ is the number of people sampled. Let's see whether these assumptions are met by the data:

```{r}
observed_vs_expected_HW <- function(SNP_data){
  # compute expectations under Hardy-Weinberg equilibrium
  # organize expected and observed in a table
  observed <- c("AA" = SNP_data$AA, "AB" = SNP_data$AB, "BB" = SNP_data$BB)
  expected <- c("AA" = SNP_data$p^2 * SNP_data$tot, 
                "AB" = 2 * SNP_data$p * SNP_data$q * SNP_data$tot, 
                "BB" = SNP_data$q^2 * SNP_data$tot)
  return(rbind(observed, expected))
}
```

And test it:
```{r}
my_SNP_data <- compute_probabilities_HW(ch6)
observed_vs_expected_HW(my_SNP_data)
```

Pretty good! This SNP seems very close to the theoretical expectation.

Let's try another one
```{r}
observed_vs_expected_HW(compute_probabilities_HW(ch6, "rs1316662"))
```

Because we have so many SNPs, we will surely find some that do not comply with the expectation. For example:
```{r}
my_SNP_data <- compute_probabilities_HW(ch6, "rs6596835")
observed_vs_expected_HW(my_SNP_data)
```

To find those with the largest deviations, we can compute for the statistic:

$$
\sum_i \frac{(e_i - o_i)^2}{e_i}
$$
In genetics, this is called $\chi^2$ statistics, because if the data were to follow the assumptions, these quantities would follow the $\chi^2$ distribution.
```{r}
compute_chi_sq_stat <- function(my_obs_vs_expected){
  observed <- my_obs_vs_expected["observed",]
  expected <- my_obs_vs_expected["expected",]
  return(sum((expected - observed)^2 / expected))
}
```

Now let's compute the statistic for each SNPs:
```{r}
# because this might take a while, we're going to only analyze the first 1000 SNPs
all_SNPs <- ch6$SNP[1:1000]
results <- data.frame(SNP = all_SNPs, ChiSq = 0)
for (i in 1:nrow(results)){
  results[i, 2] <- compute_chi_sq_stat(observed_vs_expected_HW(compute_probabilities_HW(ch6, results[i, 1])))
}
```

To find the ones with the largest discrepancy, run
```{r}
results <- results[order(results$ChiSq, decreasing = TRUE),]
head(results)
```

This example showed how a seemingly difficuly problem can be decomposed in smaller problems that are easier to solve. 

## Documenting the code using `knitr`

 > *Let us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to humans what we want the computer to do. *

> Donald E. Knuth, Literate Programming, 1984

When doing experiments, we typically keep track of everything we do in a laboratory notebook, so that when writing the manuscript, or responding to queries, we can go back to our documentation to find exactly what we did, how we did it, and possibly why we did it. The same should be true for computational work.

`RStudio` makes it very easy to build a computational laboratory notebook. First, create a new `R Markdown`  file (choose `File` -> `New File` -> `R Markdown` from the menu). 

The gist of it is that you write a text file (`.Rmd`). The file is then read by an iterpreter that transforms it into an `.html` or `.pdf` file, or even into a Word document. You can use special syntax to render the text in different ways. For example, type

```
***********

*Test* **Test2**

# Very large header

## Large header

### Smaller header

## Unordered lists

* First
* Second
    + Second 1
    + Second 2

1. This is
2. A numbered list

You can insert `inline code`

-----------
```

The most important feature of `R Markdown`, however, is that you can include blocks of code, and they will be interpreted and executed by `R`. You can therefore combine effectively the code itself with the description of what you are doing. 

For example, including

    ```{r, eval=FALSE}  
    print("hello world!")  
    ```

will become 

```{r}  
print("hello world!")  
```

If you don't want to run the `R` code, but just display it, use `{r, eval = FALSE}`; if you want to show the output but not the code, use `{r, echo = FALSE}`.

You can include plots, tables, and even render equations using LaTeX. In summary, when exploring your data or writing the methods of your paper, give `R Markdown` a try!

You can find inspiration in the notes for this class: all are written in `R Markdown`.

## Resources
There are very many excellent books and tutorials you can read to become a proficient programmer in `R`. For example:

* [Intro to R](https://cran.r-project.org/doc/manuals/r-release/R-intro.html)
* [Advanced R](http://adv-r.had.co.nz/)
* [DataCamp](https://www.datacamp.com/courses/free-introduction-to-r)
* [ComputerWorld](https://www.computerworld.com/article/2497143/business-intelligence/business-intelligence-beginner-s-guide-to-r-introduction.html)
* [R Style guide](http://adv-r.had.co.nz/Style.html)
* [R for Data Science](https://hackr.io/tutorial/r-for-data-science)
* [RStudio Cheat Sheet](https://github.com/rstudio/cheatsheets/raw/master/rstudio-ide.pdf)
* [Base R Cheat Sheet](http://github.com/rstudio/cheatsheets/raw/master/base-r.pdf)
* [Advanced R Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2016/02/advancedR.pdf)
* [X in Y minutes](https://learnxinyminutes.com/docs/r/)
* [Intro to Data Wrangling](https://cengel.github.io/R-data-wrangling/index.html)
* [R Boot Camp](https://r-bootcamp.netlify.app/)


<!--chapter:end:00-refresher.Rmd-->

# Visualizing data using `ggplot2`

## Goal
Introduce the package `ggplot2`, which is part of the `tidyverse` bundle. Learn how to use `ggplot2` to produce publication-quality figures. Discuss the philosophical underpinnings of the "Grammar of Graphics", showcase the `ggplot2` syntax, produce examples of the different types of graphs. Learn how to change colors, legends, scales. Visualize histograms, barplots, scatterplots, etc.

## Introduction to the Grammar of Graphics
The most salient feature of scientific graphs should be clarity. Each figure should make crystal-clear a) what is being plotted; b) what are the axes; c) what do colors, shapes, and sizes represent; d) the message the figure wants to convey. Each figure is accompanied by a (sometimes long) caption, where the details can be explained further, but the main message should be clear from glancing at the figure (often, figures are the first thing editors and referees look at).

Many scientific publications contain very poor graphics: labels are missing, scales are unintelligible, there is no explanation of some graphical elements. Moreover, some color graphs are impossible to understand if printed in black and white, or difficult to discern for color-blind people.

Given the effort that you put into your science, you want to ensure that it is well presented and accessible. The investment to master some plotting software will be rewarded by pleasing graphics that convey a clear message.

In this section, we introduce `ggplot2`, a plotting package for `R` This package was developed by Hadley Wickham who contributed many important packages to `R` (all included in the `tidyverse` bundle we're going to use for the reminder of the class). Unlike many other plotting systems, `ggplot2` is deeply rooted in a  "philosophical" vision. The goal is to conceive a grammar for all graphical representation of data.  Leland Wilkinson and collaborators proposed The Grammar of Graphics. It follows the idea of a well-formed sentence that is composed of a subject, a predicate, and an object. The Grammar of Graphics likewise aims at describing a well-formed graph by a grammar that captures a very wide range of statistical and scientific graphics. This might be more clear with an example -- Take a simple two-dimensional scatterplot. How can we describe it? We have:

- **Data** The data we want to plot.

- **Mapping** What part of the data is associated with a particular visual feature? For example: Which column is associated with the x-axis? Which with the y-axis? Which column corresponds to the shape or the color of the points? In `ggplot2` lingo, these are called *aesthetic mappings* (`aes`).

- **Geometry** Do we want to draw points? Lines? In `ggplot2` we speak of *geometries* (`geom`).
  
- **Scale** Do we want the sizes and shapes of the points to scale according to some value? Linearly? Logarithmically? Which palette
    of colors do we want to use?

- **Coordinate** We need to choose a coordinate system (e.g., Cartesian, polar).

- **Faceting** Do we want to produce different panels, partitioning the data according to one (or more) of the variables?

This basic grammar can be extended by adding statistical transformations of the data (e.g., regression, smoothing), multiple layers, adjustment of position (e.g., stack bars instead of plotting them side-by-side), annotations, and so on.

Exactly like in the grammar of a natural language, we can easily change the meaning of a "sentence" by adding or removing parts. Also, it is very easy to completely change the type of geometry if we are moving from say a histogram to a boxplot or a violin plot, as these types of plots are meant to describe one-dimensional distributions. Similarly, we can go from points to lines, changing one "word" in our code. Finally, the look and feel of the graphs is controlled by a theming system, separating the content from the presentation.

## Basic `ggplot2`

`ggplot2` ships with a simplified graphing function, called `qplot`. In this introduction we are not going to use it, and we concentrate instead on the function `ggplot`, which gives you complete control over your plotting. First, we need to load the package:
```{r, message=FALSE, warning=FALSE}
library(tidyverse)
```

To explore the features of `ggplot2`, we are going to use a data set detailing the number of daily COVID cases and deaths in US counties. The data are provided by the [New York Times](https://github.com/nytimes/covid-19-data/blob/master/live/us-counties.csv).

```{r}
# read the data
# original URL https://github.com/nytimes/covid-19-data/raw/master/live/us-counties.csv
dt <- read_csv("https://rb.gy/zr65gg")
head(dt)
```

we are going to work with `date`, `county`, `state`, `cases` and `deaths`.

Let's select Illnois, and take only the counties with more than 250 cases (to have a less crowded graph):
```{r}
dti <- dt[(dt$state == "Illinois") & (dt$cases > 250), ]
```

A particularity of `ggplot2` is that it accepts exclusively data organized in tables (a `data.frame` or a `tibble` object---more on tibbles later). Thus, all of your data needs to be converted into a data frame format for plotting. 

## Building a well-formed graph
For our first plot, we're going to produce a barplot detailing how many cases have been reported in each County:

```{r}
ggplot(data = dti)
```

As you can see, nothing is drawn: we need to specify what we would like to associate to the *x* axis, and what to the *y* axis, etc. (i.e., we want to set as the *aesthetic mappings*). A barplot typically has classes on the *x* axis, while the *y* axis reports the counts in each class.

```{r}
ggplot(data = dti) + aes(x = county, y = cases)
```

Note that we concatenate pieces of our "sentence" using the `+` sign! We've got the aestethic mappings figured out, but still no graph... we need to specify a geometry, i.e., the type of graph we want to produce. In this case, a barplot where the height of the bars is specified by the `y` value:

```{r}
ggplot(data = dti) + aes(x = county, y = cases) + geom_col()
```

Because it is very difficult to see the labels, let's swap the axes:

```{r}
ggplot(data = dti) + 
  aes(x = county, y = cases) + 
  geom_col() + 
  coord_flip()
```

The graph shows that, naturally, the vast majority of cases was reported in Cook county. We have written a "well-formed sentence", composed of **data** + **mapping** + **geometry**, and this is sufficient to produce a graph. We can add "adjectives" and "adverbs" to our graph, to make it clearer:

```{r}
ggplot(data = dti) + 
  aes(x = reorder(county, cases), y = cases) + # order labels according to cases
  geom_col() +
  xlab("Number of COVID cases reported") + # x label
  ylab("Illinois County") + # y label
  scale_y_log10() + # transform the counts to logs
  coord_flip()+
  ggtitle(dti$date[1]) # main title (use current date)
```

## Scatterplots
Using `ggplot2`, one can produce very many types of graphs. The package works very well for 2D graphs (or 3D rendered in two dimensions), while it lack capabilities to draw proper 3D graphs, or networks.

The main feature of `ggplot2` is that you can tinker with your graph fairly easily, and with a common grammar. You don't have to settle on a certain presentation of the data until you're ready, and it is very easy to switch from one type of graph to another. 

For example, let's plot the number of cases vs. number of deaths:
```{r}
# you can store the graph in a variable
pl <- ggplot(data = dti)
pl <- pl + aes(x = cases, y = deaths) # for a scatter plot, we need two aes mappings!
pl <- pl + geom_point() # draw points in a scatterplot
pl <- pl + scale_x_sqrt() + scale_y_sqrt() # transform axes
pl # or show(pl)
```

Showing that number of daily cases and number of daily deaths are highly correlated (but it would be a stronger correlation if we were to plot past cases vs. current deaths).

## Histograms, density and boxplots
It would be nice to see the distribution of the ratio deaths/cases. To do so, we can produce a histogram:

```{r}
pl <- ggplot(data = dti)
pl <- pl + aes(x = deaths / cases)  
pl + geom_histogram() 
```

We can control the width of the bins by specifying:
```{r}
pl + geom_histogram(bins = 30) # specify the number of bins
pl + geom_histogram(binwidth = 0.0025) # specify the bin width
```

Let's see whether the histograms differ between Illinois and Indiana:
```{r}
ggplot(data = dt[dt$state %in% c("Illinois", "Indiana"),]) + 
  aes(x = deaths / cases, fill = state) + # fill the bar colors by state
  geom_histogram()
```

To plot the histogram side by side, use
```{r}
ggplot(data = dt[dt$state %in% c("Illinois", "Indiana"),]) + 
  aes(x = deaths / cases, fill = state) + # fill the bar colors by state
  geom_histogram(position = "dodge")
```

Similarly, we can approximate the histogram using a density plot, which interpolates the bin height to create a smooth distribution:
```{r}
ggplot(data = dt[dt$state %in% c("Illinois", "Indiana"),]) + 
  aes(x = deaths / cases, fill = state) + # fill by state
  geom_density()
```

To see the graph better, let's make the coloring semi-transparent:
```{r}
ggplot(data = dt[dt$state %in% c("Illinois", "Indiana"),]) + 
  aes(x = deaths / cases, fill = state) + # fill by state
  geom_density(alpha = 0.5)
```

Again showing that Indiana tends to have a higher mortality for the same number of cases.

For this type of comparison, the ideal graph to show is maybe a box-plot or a violin plot:
```{r}
ggplot(data = dt[dt$state %in% c("Illinois", "Indiana"),]) + 
  aes(x = state, y = deaths / cases, fill = state) + # we need both x and y
  geom_boxplot()
```

A boxplot shows the median (horizontal bar) as well as the inter-quartile range (box size goes from 25th to 75th percentile), as well as the typical range of the data (whiskers). The dots represent "outliers". To show the full distribution, you can use a violin plot:
```{r}
ggplot(data = dt[dt$state %in% c("Illinois", "Indiana"),]) + 
  aes(x = state, y = deaths / cases, fill = state) + # we need both x and y
  geom_violin(draw_quantiles = 0.5)
```

Note that when we're producing "similar" plots (e.g., histogram vs. density, box vs. violin, or any other plot sharing the same aesthetic mappings) changing a single word, we have changed the structure of the graph considerably!

## Scales
We can use scales to determine how the aesthetic mappings are displayed. For example, we could set the *x* axis to be in logarithmic scale, or we can choose how the colors, shapes and sizes are used. `ggplot2` uses two types of scales: `continuous` scales are used for continuos variables (e.g., real numbers); `discrete` scales for variables that can only take a certain number of values (e.g., colors, shapes, sizes).

For example, let's plot deaths vs. cases in our `dti` data set:
```{r}
pl <- ggplot(data = dti) + 
  aes(x = cases, y = deaths, colour = log(deaths)) +
    geom_point() 
pl
```

We can change the scale of the *x* axis by calling:
```{r}
pl + scale_x_log10() # log of number of cases
pl + scale_x_sqrt() # sqrt of number of cases
pl + scale_x_reverse() # from large to small
```

Similarly, we can change the use of colors, points, etc.

## List of aesthetic mappings
We've seen some of the aesthetic mappings. Here's a list of the main `aes`:

* `x` what to use for *x* axis
* `y` what to use for *y* axis
* `color` the color of points and lines
* `fill` the color of shapes (e.g., boxes, bars, etc.)
* `size` the size of points, lines, etc.
* `shape` the shape of points
* `alpha` the level of transparency of the object
* `linetype` the type of line (e.g., solid, dashed, etc.)

```{r}
# a more complex example
ggplot(data = dt) + 
  aes(x = cases, y = deaths, 
          color = state) +
  geom_point() + 
  scale_x_log10() + # note that the points with 0 cases or deaths will not work
  scale_y_log10() +
  theme(legend.position = "bottom")
```

## List of geometries
There are very many geometries; here are a few of the most useful ones:

* Lines: `geom_abline` (line given slope and intercept); `geom_hline`, `geom_vline` (horizontal, vertical line); `geom_line` (connect observation in scatterplot).
* Bars: `geom_bar` (bar height is the count/sum); `geom_col` (bar heigts are provided by the data).
* Boxes: `geom_boxplot`.
* Distributions: `geom_violin` (like boxplots, but showing the density of the distribution); `geom_density` (density of 1D distribution), `geom_density2d` (density of bivariate distribution); `geom_histogram`, `geom_bin2d` (histograms).
* Text: `geom_text`.
* Smoothing function: `geom_smooth` (interpolates the points of a scatterplot).
* Error bars: `geom_errorbar`.
* Maps: `geom_map` (polygons from a reference map).

## List of scales
There are also very many scales. Here are a few:

* `xlab`, `ylab`, `xlim`, `ylim` control labels and ranges of the axes.
* `scale_alpha` transparency of the points/shapes.
* `scale_color` (many options) colors of points and lines.
* `scale_fill` (many options) colors of boxes, bars and shapes.
* `scale_shape` shape of the points.
* `scale_linetype` type of lines.
* `scale_size` size of points and lines.
* `scale_x`, `scale_y` (many options) transformations of the axes.

## Themes
Themes allow you to manipulate the look and feel of a graph with just one command. The package `ggthemes` extends the themes collection of `ggplot2` considerably. For example:

```{r, eval=FALSE}
# to install, type install.packages("ggthemes") in the console
library(ggthemes)
pl <- ggplot(data = dti) + aes(x = cases, y = deaths) +
    geom_point() + scale_x_log10() + scale_y_log10()
pl + theme_bw() # white background
pl + theme_economist() # like in the magazine "The Economist"
pl + theme_wsj() # like "The Wall Street Journal"
```

## Faceting
In many cases, we would like to produce a multi-panel graph, in which each panel shows the data for a certain combination of parameters. In `ggplot2` this is called *faceting*: the command `facet_grid` is used when you want to produce a grid of panels, in which all the panels in the same row (or column) have axes-ranges in common; `facet_wrap` is used when the different panels do not necessarily have axes-ranges in common.

For example:
```{r}
pl <- ggplot(data = dt[dt$state %in% c("Illinois", "Missouri", "Wisconsin", "Indiana"), ]) + 
  aes(x = cases, y = deaths, colour = state) + geom_point() + scale_x_log10() + scale_y_log10()
pl <- pl + facet_wrap(~state)
pl
```

Let's add a line separating showing the best-fit line:
```{r}
pl <- pl + geom_smooth()
pl
```

Make ranges on *x* and *y* axes equal, and add the 1:1 line:
```{r}
pl <- pl + coord_equal() + geom_abline(slope = 1, intercept = 0)
pl
```

## Setting features
Often, you want to simply set a feature (e.g., the color of the points, or their shape), rather than using it to display information (i.e., mapping some aestethic). In such cases, simply declare the feature outside the `aes`:

```{r}
pl <- ggplot(data = dt) + 
  aes(x = cases, y = deaths) + 
  scale_x_log10() + 
  scale_y_log10()
pl + geom_point()
pl + geom_point(colour = "red")
pl + geom_point(shape = 3)
pl + geom_point(alpha = 0.5)
```

## Saving graphs
You can either save graphs as done normally in `R`:

```{r, eval = FALSE}
# save to pdf format
pdf("my_output.pdf", width = 6, height = 4)
print(my_plot)
dev.off()
# save to svg format
svg("my_output.svg", width = 6, height = 4)
print(my_plot)
dev.off()
```

or use the function `ggsave`

```{r, eval=FALSE}
# save current graph
ggsave("my_output.pdf")
# save a graph stored in ggplot object
ggsave(plot = my_plot, filename = "my_output.svg")
```

## Multiple layers
You can overlay different plots. To do so, however, they must share some of the aesthetic mappings. 
The simplest case is that in which you have only one dataset:

```{r}
ggplot(data = dt) + 
  geom_point(aes(y = state, x = cases), color = "black") + 
  geom_point(aes(y = state, x = deaths), color = "red") +
  scale_x_log10() + 
  xlab("cases (black), deaths (red)")
```

## Try on your own data!
Now that you're familiar with `ggplot2`, try producing some meaningful plots for your own data.

## Resources

* [R for Data Science](https://hackr.io/tutorial/r-for-data-science)
* [Tidyverse reference website](https://www.tidyverse.org/)
* [Data Visualization Cheat  Sheet](https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf)

<!--chapter:end:01-dataviz.Rmd-->

# Fundamentals of probability

## Sample spaces and random variables

No observation or measurement in our world is perfectly reproducible, no matter how carefully planned and executed. The level of uncertainly varies, but randomness always finds a way to creep into a data set. Where does the "random" factor come from? From the classical physics perspective, as articulated by Laplace, most natural phenomena are theoretically deterministic for an omniscient being with an unlimited computational power. Quantum mechanical phenomena are (theoretically) truly random, but the randomness is not observable on the scales of biology or social science. The lack of predictability in the data we work with is usually due either to its intrinsic complexity (e.g., biomolecular systems, prediction of animal behavior), which essentially makes it impossible to know every detail of the system, or to some external source of noise (e.g., measurement error, weather affecting food availability) that is outside of our control.

In probability terminology, a *random experiment* produces *outcomes* and the collection of all outcomes of an experiment is called its *sample space*. 

**Example:** The specifics of the experiment can affect the degree of uncertainty in the outcome; the same measurement may be random or not, depending on context. For example, measuring the height of a person should be deterministic, if one measures the height of the same person within a short amount of time. So unless you're interested in studying the error in [stadiometer](https://www.quickmedical.com/measure/stadiometer.html) results, you probably won't consider this a random experiment. However, measuring the heights of different people is a random experiment, where the source of randomness is primarily due to the selection of people for your study, called *sampling error*, rather than due to the measurement noise of any one person.

The measurement of interest from a random experiment is called a *random variable*. Sometimes the measurement is simply the outcome, but usually it reports some aspect of the outcome and so several outcomes can have the same value of the random variable. The random variable can then be seen as condensing the sample space into a smaller range of values. Random variables can be *numeric* or *categorical*, with the difference that categorical variables cannot be assigned meaningful numbers. For instance, one may report an individual by phenotype (e.g., white or purple flowers), or having a nucleotide A, T, G, C in a particular position, and although one could assign numbers to these categories (e.g., 1, 2, 3, 4) they could not be used in sensical way---one can compare and do arithmetic with numbers, but A is not less than T and A + T does not equal G. Thus there are different tools for describing and working with numeric and categorical random variables. 

**Example:** In a DNA sequence a codon triplet represents a specific amino acid, but there is redundancy (several triplets may code for the same amino acid). One may think of a coding DNA sequence as an outcome, but the amino acid (sequence or single one) as a random variable. Extending this framework, one may think of genotype as an outcome, but a phenotype (e.g., eye color) as a random variable---although this is not correct for any phenotype that is not strictly determined by the genotype, because then there are other factors (e.g., environmental or epigenetic) that influence the value of the random variable besides the outcome (genotype).

**Exercise:** The package `palmerpenguins` contains multiple variables measured in populations of three different species of penguins over three years on three different islands. Identify numeric and categorical variables, and specify whether numeric variables are discrete and continuous. 

```{r}
library(tidyverse)
library(palmerpenguins)
str(penguins)
```

## Probability axioms
An outcome in sample space can be assigned a *probability* depending on its frequency of occurrence out of many trials, each is a number between 0 and 1. Combinations of outcomes (*events*) can be assigned probabilities by building them out of individual outcomes.  These probabilities have a few rules, called the *axioms of probability*, expressed using set theory notation.

1. The total probability of all outcomes in sample space is 1. $P(\Omega) = 1$

2. The probability of nothing (empty set) is 0. $P(\emptyset) = 0$

3. The probability of an event made up of the union of two events is the sum of the two probabilities minus the probability of the overlap (intersection.) $P(A \cup B) = P(A) + P(B) - P(A \cap B)$

**Example:** Let's assign a probability to every possible three-letter codon. There are $4^3 = 64$ codons, so if one assumes that each one has equal probability, then they they all equal $1/64$ (by axiom 1.) The probability of a codon having A as the first letter is 1/4, and so is the probability of A as the second letter. Axiom 3 allows us to calculate the probability of A in either the first or the second letter: 

$$ P(AXX \cup \ XAX ) =  P(AXX) + P(XAX) - P(AAX) = 1/4 + 1/4 - 1/16 = 7/16$$

## Probability distributions
The probability of each value of a random variable can be calculated from the probability of the event that corresponds to each value of the random variable. The collection of the probabilities of all of the values of the random variable is called the *probability distribution function* of the random variable, more formally the *mass function* for a discrete random variable or the *density function* for a continuous random variable. 

For a discrete random variable (let's call it $X$) with a probability mass function $f$, the probability of $X$ taking the value of $a$ can be written either as $f(X=a)$ or $f(a)$, as long as it's clear that $f$ is the probability distribution function of $X$. The one ironclad rule of probability is that all values of the mass function have to add up to 1. To state this mathematically, if all the possible values of $X$ can be written as $a_1, a_2, ...$ (there may be finitely or infinitely many of them, as long as it's a countable infinity), this sum has to be equal to 1:
$$ \sum_i f(a_i) = 1 $$

A continuous random variable (let's call it $Y$) with a probability density function $g$ is a bit more complicated. The continous part means that the random variable has uncountably many values, even if the range is finite (for example, there are uncountably many real numbers between 0 and 1). Thus, the probability of any single value must be vanishingly small (zero), otherwise it would be impossible to add up (integrate) all of the values and get a finite result (let alone 1). We can only measure the probability of a range of values of $Y$ and it is defined by the integral of the density function overal that range:

$$ P( a< Y < b) = \int_a ^b g(y) dy $$

The total probability over the entire range of $Y$ has to be 1, but it's similarly calculated by integration instead of summation ($R$ represents the range of values of $Y$):

$$ \int_R g(y) dy = 1$$

**Example:**  As codons (DNA triplets) code for amino acids, we can consider the genetic code a random variable on the sample space. Assuming all codons have equal probabilities, the probability of each amino acid is the number of triplets that code for it divided by 64. For example, the probabilities of leucine and arginine are $6/64 = 3/32$, the probability of threonine is $4/64 = 1/16$ and the probabilities of methionine and tryptophan are $1/64$. This defines a probability distribution function of the random variable of the genetic code. Note that the sum of all the probabilites of amino acids has to be 1. Of course there is no inherent reason why each triplet should be equally probable, so a different probability structure on the sample space would result in a different probability distribution (mass) function.

## Measures of center: medians and means
The standard measures described here are applicable only numeric random variables. Some measures of center and spread for categorical variables exist as well.

The *median* of a random variable is the value which is in the middle of the distribution, specifically, that the probability of the random variable being no greater than that value is 0.5.

The *mean* or *expectation* of a random variable is the center of mass of the probability distribution. Specifically, it is defined for a mass function to be:

$$ E(X) = \sum_i a_i\, f(a_i)$$

And for a density function it is defined using the integral:
$$ E(Y) =  \int_R y\, g(y) dy $$

**Example:** Let us examine the factors (categorical variables) in the penguins data set. They cannot be described using means and medians, but can be plotted by counts in each category as you learned in the introduction to `ggplot2`:

```{r}
ggplot(data = penguins) +
  aes(x = species, fill = sex) + 
  geom_bar(position = "fill")


ggplot(data = penguins) +
  aes(x = year, fill = species) + 
  geom_bar(position = "fill")
```

One can plot the distributions of numeric variables like body mass for different penguin species using box plots:

```{r}
ggplot(data = penguins) + aes(x = as.factor(species), y=body_mass_g) + geom_boxplot()
```

The following code chunk uses `dplyr` functions that we will learn in the next chapter to calculate the mean and median values of these variables aggregated by species:

```{r}
penguins %>% drop_na() %>% group_by(species) %>% summarise(mean = mean(body_mass_g))
penguins %>% drop_na()  %>% group_by(species) %>% summarise(median = median(body_mass_g))
```

Comment on how the descriptive statistics correspond to the box plots. 

## Measures of spread: quartiles and variances

All random variables have spread in their values. The simplest way to describe it is by stating its range (the interval between the minimum and maximum values) and the quartiles (the medians of the two halves of the distribution).

A more standard measure of the spread of a distribution is the variance, defined as the expected value of the squared differences from the mean:

$$\text{Var}(X) = E [X - E(X)]^2 = \sum_i (a_i- E(X))^2 f(a_i)$$

And for a density function it is defined using the integral:
$$\text{Var}(Y) =  E[ Y - E(Y)]^2 = \int_R (y-E(Y))^2 g(y) dy $$

Variances have squared units so they are not directly comparable to the values of the random variable. Taking the square root of the variance converts it into the same units and is called the standard deviation of the distribution:
$$ \sigma_X = \sqrt{\text{Var}(X)}$$
**Example:** Let's go back to the penguins data set and calculate the measures of spread for the variable body mass for different penguin species

```{r}
ggplot(data = penguins) + aes(x = as.factor(species), y=body_mass_g) + geom_boxplot()
penguins %>% drop_na() %>% group_by(species) %>% summarise(var = var(body_mass_g))
penguins %>% drop_na() %>% group_by(species) %>% summarise(first_quart = quantile(body_mass_g,0.25))
penguins %>% drop_na() %>% group_by(species) %>% summarise(third_quart = quantile(body_mass_g,0.75))
```

Which species has a wider spread in its body mass? How do the descriptive stats and the box plots correspond?

## Data as samples from distributions: statistics

In scientific practice, we collect data from one or more random variables, called a *sample*, and then try to make sense of it. One of the basic goals is statistical inference: using the data set to describe the *population* distribution from which the sample was drawn. Data sets can be plotted as *histograms* and the frequency/fraction of each value should be an approximation of the underlying probability distribution. In addition, descriptive statistics of the sample data (means, variances, medians, etc.) can be used to estimate the true parameters such as the mean and the variance of the population distribution.

Some of the fundamental questions about the population include:

1. What type of distribution is it?

2. Estimate the parameters of that distribution.

3. Test a hypothesis, e.g., whether two samples were drawn from the same distribution.

4. Describe and test a relationship between two or more variables.

### Law of large numbers

First, the sample has to be *unbiased*, that is, no outcomes should be systematically over- or under-represented. But even an unbiased sample will differ from the population due to the inherent randomness of selection (sampling error). The **law of large numbers** states that as the *sample size* increases, the mean of the sample converges to the true mean of the population. Formally, for a set of $n$ indepdenent, identically distributed random variables (the sample) $\{X_i\}$ the sample mean $\overline{X}_n$ converges to the mean of the distribution $\mu$:

$$ 
\lim _{n \to \infty} \frac{\sum_{i=1}^n {X_i}}{n} = \lim _{n \to \infty} \overline{X}_n = \mu
$$

### Central Limit Theorem

That is nice to know, but doesn't say exactly how large a sample is needed to estimate, for example, the mean of the population to a given precision. For that, we have the **Central Limit Theorem**, which states that the distribution of sample means (from samples of independent, identically distributed random variables) as sample size increases, approaches the normal (Gaussian) distribution with mean equal to the population mean and standard deviation equal to the standard deviation of the population divided by the square root of the sample size. Formally, it states that for a set of $n$ indepdenent, identically distributed random variables (the sample) $\{X_i\}$ with distribution mean $\mu$ and variance $\sigma^2$, the probability density function of the sample mean $\overline{X}_n$ converges for large sample size $n$ to the normal distribution:

$$ 
P(\overline{X}_n) \to N(\mu, \sigma^2/n)
$$

where $N(\mu, \sigma^2/n$) stands for the normal distribution with mean $\mu$ and variance $\sigma^2/n$.  One extremely useful consequence of this theorem is that the variance of the sample mean is reciprocally related to the sample size $n$. More precicely, it allows the calculation of *confidence intervals* by using the normal distribution to generate an interval around the observed sample mean in which the  true mean $\mu$ lies with a given likelihood.

This is an amazing result because it applies to any distribution, so it allows for the estimation of means for any situation, as long as the condition of independent, identically disributed variables in the sample is satisfied (the identical distributed condition can actually be relaxed). There are other central limit theorems that apply to other situations, including cases where the random variables in the sample are not independent (e.g., Markov models). The bottom line is that an unbiased sample contains a reflection of the true population, but it is always distorted by uncertainty. Larger sample sizes decrease the uncertainty but are more difficult and expensive to obtain.

**Discussion:** Suggest examples of biological data sets which are not made up of independent identically distributed random variables.


## Exploration: misleading means

Means are the most common type of descriptive statistic and are sometimes the only numeric quantity used to compare two data sets, e.g. "the average GPA at school A is 3.5 vs 3.8 at school B". However, means can be misleading measures in multiple ways. 

First, means are highly sensitive to outliers, or points that are very different from other values. They can skew the mean value, even pulling it completely away from the bulk of the values, in which case the mean ceases to be a measure of a "typical" value.

Second, there can be funny business with combining means of different *subsets* of data. Normally, you might expect if you have group A and group B, and each group has two subgroups divided by another variable (e.g. we are comparing the GPAs of students in school A and school B, and we split up the students in each school by gender), then if the means of each subgroup of A and larger than the means of the same subgroup of B (e.g. the GPA of girls and boys in school A are higher than those of their counterparts in school B), then the same relationship should be true for the combined mean of group A and group B (that is, the overall GPA in school A is higher than school B). That is not necessarily true!

This apparent contradiction is called Simpson's paradox. It can be illustrated in the data set of all the passengers and crew on the doomed ocean liner Titanic. The data set is found in the library `stablelearner` and is loaded by the chunk below:

```{r}
library(stablelearner)
data(titanic)
str(titanic)
```
The chunk below calculated the survival probability of passengers of all classes compared to the crew (of all types:

```{r}
titanic %>% group_by(Passenger = class %in% c('1st', '2nd', '3rd'), survived) %>% summarise(num = n()) %>% mutate(fraction = num/sum(num)) 
```
You can see that about 24% of the crew survived and almost 38% of the passengers survived. In this week's assignment you will calculate and explain what happens when you divide the people in each group by gender.

## References

* [Laplace's views on probability and determinism](https://www.bayesianspectacles.org/laplaces-demon/)
* [Central Limit Theorem in R](https://medium.com/@ODSC/exploring-the-central-limit-theorem-in-r-e2a2f7091606)
* [Exploration of the Central Limit Theorem](https://genomicsclass.github.io/book/pages/clt_in_practice.html)
* [Simpson's paradox](https://medium.com/@nikhilborkar/the-simpsons-paradox-and-where-to-find-them-cfcec6c2d8b3)

<!--chapter:end:02-probdist.Rmd-->

# Data wrangling

## Goal
Learn how to manipulate large data sets by writing efficient, consistent, and compact code. Introduce the use of `dplyr`, `tidyr`, and the "pipeline" operator `%>%`. Effortlessly produce statistics for grouped data. Massage data into "tidy" form.

## What is data wrangling?

As biologists living in the XXI century, we are often faced with tons of data, possibly replicated over several organisms, treatments, or locations. We would like to streamline and automate our analysis as much as possible, writing scripts that are easy to read, fast to run, and easy to debug. Base `R` can get the job done, but often the code contains complicated operations, and a lot of `$` signs and brackets.

We're going to learn about the packages `dplyr` and `tidyr`, which are part of `tidyverse` and can be used to manipulate large data frames in a simple and straightforward way. These tools are also much faster than the corresponding base `R` commands, are very compact, and can be concatenated into "pipelines". 

To start, we need to import the libraries:

```{r, warning=FALSE, message=FALSE}
library(tidyverse) # this loads both dplyr and tidyr, along with other packages
library(palmerpenguins) # a nice data set to play with
```

We are going to use the data set `penguins` from the package `palmerpenguins`, which we have already seen last week.

## A new data type, `tibble`

This is now a :
```{r}
class(penguins)
```

`dplyr` ships with a new data type, called a `tibble`. To convert a `data.frame` into a tibble, use `as_tibble`:

```{r, eval=FALSE}
# load a data frame
data("trees")
class(trees)
trees <- as_tibble(trees)
class(trees)
```

The nice feature of `tbl` objects is that they will print only what fits on the screen, and also give you useful information on the size of the data, as well as the type of data in each column. Other than that, a `tbl` object behaves very much like a `data.frame`. In some rare cases, you want to transform the `tbl` back into a `data.frame`. For this, use the function `as.data.frame(tbl_object)`.

We can take a look at the data using one of several functions:

* `head(dt)` shows the first few rows
* `tail(dt)` shows the last few rows
* `glimpse(dt)` a summary of the data (similar to `str` in base R)
* `View(dt)` open in spreadsheet-like window

## Selecting rows and columns

There are many ways to subset the data, either by row (subsetting the *observations*), or by column (subsetting the *variables*). For example, let's select only the rows with observations from the island `Torgersen`:

```{r}
filter(penguins, island == "Torgersen")
```

We have 52 observations. We have used the command `filter(tbl, conditions)` to select certain observations. We can combine several conditions, by listing them side by side, possibly using logical operators.

> **Exercise:** what does this do?
>``
filter(penguins, 
       bill_length_mm > 40, 
       bill_depth_mm > 20,
       sex == male)
``

We can also select particular variables (columns) using the function `select(tbl, cols to select)`. For example, select `species` and `island`:

```{r}
select(penguins, species, island)
```

How many `species` are represented in the data set? We can use the function `distinct(tbl, cols to select)` to retain only the rows that differ from each other:

```{r}
distinct(select(penguins, species))
```

Showing that there are three species, once we removed the duplicates. There are many other ways to subset observations:

- `sample_n(tbl, howmany, replace = TRUE)` sample `howmany` rows at random (with replacement)
- `sample_frac(tbl, proportion, replace = FALSE)` sample a certain proportion (e.g. `0.2` for 20%) of rows at random without replacement
- `slice(tbl, 5:20)` extract the rows `5` to `20`
- ``top_n(penguins, 10, body_mass_g)`` extract the first `10` rows, once ordered by `body_mass_g`

More ways to select columns:

- `select(penguins, contains("mm"))` select all columns containing the string `mm`
- ``select(penguins, -year, -body_mass_g)`` exclude the columns `year` and `body_mass_g`
- `select(penguins, matches("length|bill"))` select all columns whose names match a regular expression

## Creating pipelines using `%>%`

We've been calling nested functions, such as `distinct(select(penguins, species))`. If you have to add another layer or two, the code would become unreadable. `dplyr` allows you to "un-nest" these functions and create a "pipeline" in which you concatenate commands separated by a special operator, `%>%`. For example:

```{r}
penguins %>% # take a data table
  select(species) %>% # select a column
  distinct() # remove duplicates
```

does exactly the same operations as the command above, but is much more readable. By concatenating many commands, you can create incredibly complex pipelines while retaining readability. It is also quite easy to add another piece of the pipeline in between commands, or to comment some of the pipeline out.

Another advantage of pipelines is that they help with name completion. In fact, `RStudio` is running in the background your pipeline while you type it. Try typing `dt %>% filter(` and then start typing `bill` and press `Tab`: you will see the options to complete the column name; choose it with your arrows and hit `Return`. The back tickmarks will be added automatically if needed (e.g., column names containing spaces, or starting with a digit).

## Producing summaries

Sometimes we need to calculate statistics on certain columns. For example, calculate the average number of eggs shedded by the infected mice. We can do this using `summarise` (you can use British or American spelling):

```{r}
penguins %>% 
  summarise(avg = mean(body_mass_g, na.rm = TRUE))
# alternatively, drop_na(body_mass_g) removes all the observations for which
# body_mass_g is NA
penguins %>% 
  drop_na(body_mass_g) %>% 
  summarise(avg = mean(body_mass_g, na.rm = TRUE))
```

where we used `na.rm = TRUE` to ignore missing values. This command returns a `tbl` object with just the average egg count. You can combine multiple statistics (use `first`, `last`, `min`, `max`, `n` [count the number of rows], `n_distinct` [count the number of distinct rows], `mean`, `median`, `var`, `sd`, etc.):

```{r}
penguins %>% 
  summarise(avg = mean(body_mass_g, na.rm = TRUE), 
            sd = sd(body_mass_g, na.rm = TRUE), 
            median = median(body_mass_g, na.rm = TRUE))
```

## Summaries by group

One of the most useful features of `dplyr` is the ability to produce statistics for the data once subsetted by *groups*. For example, we would like to compute the average body mass by species and sex:

```{r}
penguins %>% 
  drop_na() %>% 
  group_by(sex, species) %>% 
  summarise(mean = mean(body_mass_g, na.rm = TRUE))
```

showing that male penguins are heavier for the three species considered.

> **Exercise:** find the average `bill_depth_mm` and `bill_length_mm` by `species` and `sex`. Filter the data to consider only observations for the year 2008.

## Ordering the data 

To order the data according to one or more variables, use `arrange()`:

```{r}
penguins %>% 
  arrange(body_mass_g) # ascending
dt %>% 
  arrange(desc(body_mass_g)) # descending
```

## Renaming columns

To rename one or more columns, use `rename()`:
```{r}
penguins %>% 
  rename(bm = body_mass_g)
```

## Adding new variables using mutate

If you want to add one or more new columns, with the content being a function of other columns, use the function `mutate`. For example, we are going to add a new column showing the z-score for the body mass of each individual:

```{r}
penguins %>% 
  mutate(zscore_bm = scale(body_mass_g)) %>% 
  select(species, sex, body_mass_g, zscore_bm)
```

We can pipe the results to `ggplot` for plotting!
```{r}
penguins %>% 
  mutate(zscore_bm = scale(body_mass_g)) %>% 
  select(species, sex, body_mass_g, zscore_bm) %>% 
  ggplot() + aes(x = species, y = zscore_bm, colour = sex) + 
    geom_jitter()
```

You can use the function `transmute()` to create a new column and drop the original columns. 

Most importantly, you can use `mutate` and `transmute` on grouped data. For example, let's recompute the z-score of the `body_mass_g` once the data is grouped by species and sex:

```{r}
penguins %>% 
  drop_na() %>% 
  select(species, sex, body_mass_g) %>% 
  group_by(species, sex) %>% 
  mutate(zscore_bm = scale(body_mass_g)) %>% 
  arrange(body_mass_g)
```

# Data wrangling

Data is rarely in a format that is good for computing, and much effort goes into reading the data and wrestling with it to make it into a good format. As the name implies, `tidyverse` strongly advocates for the use of data in *tidy* form. What does this mean?

- Each variable forms a column
- Each observation forms a row
- Each type of observational unit forms a table

This is often called *narrow table* format. Any other form of data (e.g., *wide table* format) is considered *messy*. However, often data are not organized in tidy form, or we want to produce tables for human consumption rather than computer consumption. The package `tidyr` allows to accomplish just that. It contains only a few, very powerful functions. To explore this issue, we build a data set containing the average body mass by species and sex:

```{r}
penguin_bm <- penguins %>% 
  drop_na() %>% 
  group_by(sex, species) %>% 
  summarise(body_mass = mean(body_mass_g)) %>% 
  ungroup() # remove group information

penguin_bm
```

## From narrow to wide

Our data is in tidy form. For a paper, we want to show the difference between males and females in a table:

```{r}
penguin_bm %>% 
  pivot_wider(names_from = sex, values_from = body_mass)
```

where we have created new column names using the values found in `sex` (hence, `names_from`), and filled each cell with the corresponding value found in `body_mass` (hence, `values_from`). Similarly, if we want to show the data with species as column names, and sex as rows, we can use:

```{r}
penguin_bm %>% 
  pivot_wider(names_from = species, values_from = body_mass)
```

## From wide to narrow

For a real-world example, we will make data from:

> *Tree-ring analysis for sustainable harvest of Millettia stuhlmannii in Mozambique*, I.A.D.Remane M.D.Therrell, **South African Journal of Botany** Volume 125, September 2019, Pages 120-125

You can read a tab-separated file from:

```{r, warning=FALSE}
dt <- read_tsv("https://rb.gy/upqoxh") %>% 
  select(Age, contains("CAT"))
# selecting only age and samples
```

Each column besides `Age` represents a single tree, and each cell contains the diameter (in cm) of the tree when it was at a given age. To make this in tidy form, we first create the columns `tree` and `diameter`:

```{r}
dt <- dt %>% 
  pivot_longer(-Age, names_to = "tree", values_to = "diameter")
```

and then remove the NAs:

```{r}
dt <- dt %>% filter(!is.na(diameter))
```

Now it is easy to plot the growth trajectory of each tree (as in Fig. 3 of the original paper):

```{r}
dt %>% 
  ggplot() + 
  aes(x = Age, y = diameter) + 
  geom_line(aes(group = tree)) + # note---this makes a line for each tree
  geom_smooth(method = "loess") # while the smoothing function considers all trees
```


## Separate: split a column into two or more

```{r}
test <- tibble(name = c("Allesina, Stefano", "Kondrashov, Dmitry", "Smith, Julia"))
test
```

```{r}
test %>% separate(name, into = c("last_name", "first_name"), sep = ", ")
```

The complement of `separate` is called `unite`.

## Separate rows: from one row to many

```{r}
test <- tibble(id = c(1, 2, 3, 4), records = c("a;b;c", "c;d", "a;e", "f"))
test
```

To make it into tidy form, only one record per row:

```{r}
test %>% separate_rows(records, sep = ";")
```

## Example: brown bear, brown bear, what do you see?

This  exercise uses a dataset from [GBIF](https://www.gbif.org/en/), the Global Biodiversity Information Facility. You can download the latest version yourself by doing the following (but just skip ahead if you want to use the data provided by us).

1. Go to [GBIF](https://www.gbif.org/en/) and click on Occurrences. 
2. Under Scientific Name type in *Ursus arctos* (brown bear), and hit enter.
3. To download the data, create an account on GBIF
4. Then click on Download, and select Simple (which should have a tab-delimited .csv file)
5. Save to the data folder in your working folder.

If you don't want to go through all this, you can use the downloaded file called `Ursus_GBIF.csv` that should be in the data folder for this week. The following command loads and displays the contents of the tibble:

```{r warning=F, message=F}
# you will need ggmap!
library(ggmap)
Ursus_data <- read_tsv("data/Ursus_GBIF.csv")
glimpse(Ursus_data)
```

You see there are 50 variables in the data set, so it may be useful to remove the ones we don't need. For this exercise, our objective is to plot the occurrences of this species on the world map, so we need two variables for certain: `decimalLatitude` and `decimalLongitude`, as well as the `BasisofRecord` for additional information. Use your `tidyverse` skills to create a new tibble with only those variables. In addition, remove duplicate records from the tibble.

```{r}
# your code goes here!
```

Now we can plot this data set on the world map, using the useful package maps. To plot, use the `ggplot()` syntax with the following addition:

```{r}
mapWorld <- borders("world", colour="gray50", fill="gray50") # create a layer of borders
# now you can call 
# ggplot() + mapWorld + ...
```

Note the warning message generated by `ggplot`. Then consider the map with the locations of the brown bear specimens. Do any of them seem strange to you? What may be the explanation behind these strange data point? Now filter out the points that you identified as suspicious and print out their BasisofRecord. Does this suggest an explanation for the strangeness?

```{r}
# your code goes here!
```

# Resources

* [R for Data Science](https://hackr.io/tutorial/r-for-data-science)
* A [cool class](https://cfss.uchicago.edu/syllabus.html) at U of C in Social Sciences 
* [Data transformation](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf) cheat sheet
* [Dealing with dates](https://github.com/rstudio/cheatsheets/raw/master/lubridate.pdf) cheat sheet
* [Data import](https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf) cheat sheet

<!--chapter:end:03-wrangling.Rmd-->

