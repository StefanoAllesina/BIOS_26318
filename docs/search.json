[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fundamentals of Biological Data Analysis",
    "section": "",
    "text": "Organization of the class",
    "crumbs": [
      "Organization of the class"
    ]
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Fundamentals of Biological Data Analysis",
    "section": "Learning goals",
    "text": "Learning goals\n\nR tools for visualizing and analyzing data\n\nexploration of tidyverse\ndplyr, tidyr and readr for data wrangling and organization\nggplot2 for visualization\nspecific packages and functions for statistical analysis\n\nTheory to perform statistical inference\n\nassumptions of different methods\nhypothesis testing\nestimation of parameters\nmodel building and selection\n\nAvoiding common errors\n\nwhen (not) to use a statistical method\nsneaky paradoxes\nphantom effects\n\nWork on your own data\n\nanalyze data\nproduce graphics\nwrite up a report\npresent to class",
    "crumbs": [
      "Organization of the class"
    ]
  },
  {
    "objectID": "index.html#approach",
    "href": "index.html#approach",
    "title": "Fundamentals of Biological Data Analysis",
    "section": "Approach",
    "text": "Approach\n\nMix of theory and practice\nApply what you’re learning to your own data",
    "crumbs": [
      "Organization of the class"
    ]
  },
  {
    "objectID": "index.html#materials",
    "href": "index.html#materials",
    "title": "Fundamentals of Biological Data Analysis",
    "section": "Materials",
    "text": "Materials\n\nWeek 0\n\nR refresher @ref(refresher)\n\n\n\nWeek 1\n\nUsing ggplot2 to produce publication-ready figures\nReview of probability\n\n\n\nWeek 2\n\nData wrangling in tidyverse\nProbability distributions\n\n\n\nWeek 3\n\nHypothesis testing\nLikelihood\n\n\n\nWeek 4\n\nLinear algebra primer\nLinear models\n\n\n\nWeek 5\n\nAnalysis of variance\nModel selection\n\n\n\nWeek 6\n\nPrincipal Component Analysis and SVD\nMultidimensional scaling and Clustering\n\n\n\nWeek 7\n\nGeneralized Linear Models\nMachine Learning and cross validation\n\n\n\nWeek 8\n\nMonte Carlo and boostrap\nModeling time-series data\n\n\n\nWeek 9\nThanksgiving break\n\n\nWeek 10\n\nStudent presentations 1\nStudent presentations 2",
    "crumbs": [
      "Organization of the class"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Fundamentals of Biological Data Analysis",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nZach Miller for TAing the first iteration of the class, and for contributing materials and comments; Julia Smith for TAing the second iteration; Cassie Manrique for TAing the third iteration; Amatullah Mir for this year. Development of the class was partially supported by the Burroughs Wellcome Fund through the program “Quantitative and statistical thinking in the life sciences” (Stefano Allesina, PI).",
    "crumbs": [
      "Organization of the class"
    ]
  },
  {
    "objectID": "00-refresher.html",
    "href": "00-refresher.html",
    "title": "1  Refresher",
    "section": "",
    "text": "1.1 Goal\nIntroduce the statistical software R, and show how it can be used to analyze biological data in an automated, replicable way. Showcase the RStudio development environment, illustrate the notion of assignment, present the main data structures available in R. Show how to read and write data, how to execute simple programs, and how to modify the stream of execution of a program through conditional branching and looping. Introduce the use of packages and user-defined functions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`R`efresher</span>"
    ]
  },
  {
    "objectID": "00-refresher.html#motivation",
    "href": "00-refresher.html#motivation",
    "title": "1  Refresher",
    "section": "1.2 Motivation",
    "text": "1.2 Motivation\nWhen it comes to analyzing data, there are two competing paradigms. First, one could use point-and-click software with a graphical user interface, such as Excel, to perform calculations and draw graphs; second, one could write programs that can be run to perform the analysis of the data, the generation of tables and statistics, and the production of figures automatically.\nThis latter approach is to be preferred, because it allows for the automation of analysis, it requires a good documentation of the procedures, and is completely replicable.\nA few motivating examples:\n\nYou have written code to analyze your data. You receive from your collaborators a new batch of data. With simple modifications of your code, you can update your results, tables and figures automatically.\nA new student joins the laboratory. The new student can read the code and understand the analysis without the need of a lab mate showing the procedure step-by-step.\nThe reviewers of your manuscript ask you to slightly alter the analysis. Rather than having to start over, you can modify a few lines of code and satisfy the reviewers.\n\nHere we introduce R, which can help you write simple programs to analyze your data, perform statistical analysis, and draw beautiful figures.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`R`efresher</span>"
    ]
  },
  {
    "objectID": "00-refresher.html#before-we-start",
    "href": "00-refresher.html#before-we-start",
    "title": "1  Refresher",
    "section": "1.3 Before we start",
    "text": "1.3 Before we start\nTo follow this tutorial, you will need to install R and RStudio\n\nInstall R: download and install R from this page. Choose the right architecture (Windows, Mac, Linux). If possible, install the latest release.\nInstall RStudio: go to this page and download the “RStudio Desktop Open Source License”.\nInstall R packages: launch RStudio. Click on “Packages” in the bottom-right panel. Click on “Install”: a dialog window will open. Type tidyverse in the field “Packages” and click on “Install”. This might take a few minutes, and ask you to download further packages.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`R`efresher</span>"
    ]
  },
  {
    "objectID": "00-refresher.html#what-is-r",
    "href": "00-refresher.html#what-is-r",
    "title": "1  Refresher",
    "section": "1.4 What is R?",
    "text": "1.4 What is R?\nR is a statistical software that is completely programmable. This means that one can write a program (script) containing a series of commands for the analysis of data, and execute them automatically. This approach is especially good as it makes the analysis of data well-documented, and completely replicable.\nR is free software: anyone can download its source code, modify it, and improve it. The R community of users is vast and very active. In particular, scientists have enthusiastically embraced the program, creating thousands of packages to perform specific types of analysis, and adding many new capabilities. You can find a list of official packages (which have been vetted by R core developers) here; many more are available on GitHub and other websites.\nThe main hurdle new users face when approaching R is that it is based on a command line interface: when you launch R, you simply open a console with the character &gt; signaling that R is ready to accept an input. When you write a command and press Enter, the command is interpreted by R, and the result is printed immediately after the command. For example,\n\n1 + 1\n\n[1] 2\n\n\nA little history: R was modeled after the commercial statistical software S by Robert Gentleman and Ross Ihaka. The project was started in 1992, first released in 1994, and the first stable version appeared in 2000. Today, R is managed by the R Core Team.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`R`efresher</span>"
    ]
  },
  {
    "objectID": "00-refresher.html#rstudio",
    "href": "00-refresher.html#rstudio",
    "title": "1  Refresher",
    "section": "1.5 RStudio",
    "text": "1.5 RStudio\nFor this introduction, we’re going to use RStudio, an Integrated Development Environment (IDE) for R. The main advantage is that the environment will look identical irrespective of your computer architecture (Linux, Windows, Mac). Also, RStudio makes writing code much easier by automatically completing commands and file names (simply type the beginning of the name and press Tab), and allowing you to easily inspect data and code.\nTypically, an RStudio window contains four panels:\n\nConsole This is a panel containing an instance of R. For this tutorial, we will work mainly in this panel.\nSource code In this panel, you can write a program, save it to a file pressing Ctrl + S and then execute it by pressing Ctrl + Shift + S.\nEnvironment This panel lists all the variables you created (more on this later); another tab shows you the history of the commands you typed.\nPlots This panel shows you all the plots you drew. Other tabs allow you to access the list of packages you have loaded, and the help page for commands (just type help(name_of_command) in the Console) and packages.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`R`efresher</span>"
    ]
  },
  {
    "objectID": "00-refresher.html#how-to-write-a-simple-program",
    "href": "00-refresher.html#how-to-write-a-simple-program",
    "title": "1  Refresher",
    "section": "1.6 How to write a simple program",
    "text": "1.6 How to write a simple program\nAn R program is simply a list of commands, which are executed one after the other. The commands are written in a text file (with extension .R). When R executes the program, it will start from the beginning of the file and proceed toward the end of the file. Every time R encounters a command, it will execute it. Special commands can modify this basic flow of the program by, for example, executing a series of commands only when a condition is met, or repeating the execution of a series of commands multiple times.\nNote that if you were to copy and paste (or type) the code into the Console you would obtain exactly the same result. Writing a program is advantageous, however, because the analysis can be automated, and the code shared with other researchers. Moreover, after a while you will have a large code base, so that you can recycle much of your code.\nWe start by working on the console, and then start writing simple scripts.\n\n1.6.1 The most basic operation: assignment\nThe most basic operation in any programming language is the assignment. In R, assignment is marked by the operator &lt;- (can be typed quickly using Alt -). When you type a command in R, it is executed, and the output is printed in the Console. For example:\n\nsqrt(9)\n\n[1] 3\n\n\nIf we want to save the result of this operation, we can assign it to a variable. For example:\n\nx &lt;- sqrt(9)\nx\n\n[1] 3\n\n\nWhat has happened? We wrote a command containing an assignment operator (&lt;-). R has evaluated the right-hand-side of the command (sqrt(9)), and has stored the result (3) in a newly created variable called x. Now we can use x in our commands: every time the command needs to be evaluated, the program will look up which value is associated with the variable x, and substitute it. For example:\n\nx * 2 \n\n[1] 6\n\n\n\n\n1.6.2 Data types\nR provides different types of data that can be used in your programs. For each variable x, calling class(x) prints the type of the variable. The basic data types are:\n\nlogical, taking only two possible values: TRUE and FALSE\n\n\nv &lt;- TRUE\nclass(v)\n\n[1] \"logical\"\n\n\n\nnumeric, storing real numbers (actually, their approximations, as computers have limited memory and thus cannot store numbers like π, or even 0.2)\n\n\nv &lt;- 3.77\nclass(v)\n\n[1] \"numeric\"\n\n\n\nReal numbers can also be specified using scientific notation:\n\n\nv &lt;- 6.022e23 # 6.022⋅10^23 (Avogadro's number)\nclass(v)\n\n[1] \"numeric\"\n\n\n\ninteger, storing whole numbers\n\n\nv &lt;- 23L # the L signals that this should be stored as integer\nclass(v)\n\n[1] \"integer\"\n\n\n\ncomplex, storing complex numbers (i.e., with a real and an imaginary part)\n\n\nv &lt;- 23 + 5i # the i marks the imaginary part\nclass(v)\n\n[1] \"complex\"\n\n\n\ncharacter, for strings, characters and text\n\n\nv &lt;- 'a string' # you can use single or double quotes\nclass(v)\n\n[1] \"character\"\n\n\nIn R, the value and type of a variable are evaluated at run-time. This means that you can recycle the names of variables. This is very handy, but can make your programs more difficult to read and to debug (i.e., find mistakes). For example:\n\nx &lt;- '2.3' # this is a string\nx\n\n[1] \"2.3\"\n\nx &lt;- 2.3 # this is numeric\nx\n\n[1] 2.3\n\n\n\n\n1.6.3 Operators and functions\nEach data type supports a certain number of operators and functions. For example, numeric variables can be combined with + (addition), - (subtraction), * (multiplication), / (division), and ^ (or **, exponentiation). A possibly unfamiliar operator is the modulo (%%), calculating the remainder of an integer division:\n\n5 %% 3\n\n[1] 2\n\n\nmeaning that 5 %/% 3 (5 integer divided by 3) is 1 with a remainder of 2\nThe modulo operator is useful to determine whether a number is divisible for another: if y is divisible by x, then y %% x is 0.\nR provides many built-in functions: each functions has a name, followed by round parentheses surrounding the (possibly optional) function arguments. For example, these functions operate on numeric variables:\n\nabs(x) absolute value\nsqrt(x) square root\nround(x, digits = 3) round x to three decimal digits\ncos(x) cosine (also supported are all the usual trigonometric functions)\nlog(x) natural logarithm (use log10 for base 10 logarithms)\nexp(x) calculating \\(e^x\\)\n\nSimilarly, character variables have their own set of functions, such as:\n\ntoupper(x) make uppercase\nnchar(x) count the number of characters in the string\npaste(x, y, sep = \"_\") concatenate strings, joining them using the separator _\nstrsplit(x, \"_\") separate the string using the separator _\n\nCalling a function meant for a certain data type on another will cause errors. If sensible, you can convert a type into another. For example:\n\nv &lt;- \"2.13\"\nclass(v)\n\n[1] \"character\"\n\n# if we call v * 2, we get an error.\n# to avoid it, we can convert v to numeric:\nas.numeric(v) * 2 \n\n[1] 4.26\n\n\nIf sensible, you can use the comparison operators &gt; (greater), &lt; (lower), == (equals), != (differs), &gt;= and &lt;=, returning a logical value:\n\n2 == sqrt(4)\n\n[1] TRUE\n\n2 &lt; sqrt(4)\n\n[1] FALSE\n\n2 &lt;= sqrt(4)\n\n[1] TRUE\n\n\n\nExercise:\nWhy are two equal signs (==) used to check that two values are equal? What happens if you use only one = sign?\n\nSimilarly, you can concatenate several comparison and logical variables using & (and), | (or), and ! (not):\n\n(2 &gt; 3) & (3 &gt; 1)\n\n[1] FALSE\n\n(2 &gt; 3) | (3 &gt; 1)\n\n[1] TRUE\n\n\n\n\n1.6.4 Getting help\nIf you want to know more about a function, type ?my_function_name in the console (e.g., ?abs). This will open the help page in one of the panels on the right. The same can be accomplished calling help(abs). For more complex questions, check out stackoverflow.\n\n\n1.6.5 Data structures\nBesides these simple types, R provides structured data types, meant to collect and organize multiple values.\n\n1.6.5.1 Vectors\nThe most basic data structure in R is the vector, which is an ordered collection of values of the same type. Vectors can be created by concatenating different values with the function c() (“combine”):\n\nx &lt;- c(2, 3, 5, 27, 31, 13, 17, 19) \nx\n\n[1]  2  3  5 27 31 13 17 19\n\n\nYou can access the elements of a vector by their index: the first element is indexed at 1, the second at 2, etc.\n\nx[3]\n\n[1] 5\n\nx[8]\n\n[1] 19\n\nx[9] # what if the element does not exist?\n\n[1] NA\n\n\nNA stands for “Not Available”. Other special values are NaN (Not a Number, e.g., 0/0), Inf (Infinity, e.g., 1/0), and NULL (variable undefined). You can test for special values using is.na(x), is.infinite(x), is.null(x), etc.\nNote that in R a single number (string, logical) is a vector of length 1 by default. That’s why if you type 3 in the console you see [1] 3 in the output.\nYou can extract several elements at once (i.e., create another vector), using the colon (:) command, or by concatenating the indices:\n\nx[1:3]\n\n[1] 2 3 5\n\nx[4:7]\n\n[1] 27 31 13 17\n\nx[c(1,3,5)]\n\n[1]  2  5 31\n\n\nYou can also use a vector of logical variables to extract values from vectors. For example, suppose we have two vectors:\n\nsex &lt;- c(\"M\", \"M\", \"F\", \"M\", \"F\") # sex of Drosophila\nweight &lt;- c(0.230, 0.281, 0.228, 0.260, 0.231) # weight in mg\n\nand that we want to extract only the weights for the males.\n\nsex == \"M\"\n\n[1]  TRUE  TRUE FALSE  TRUE FALSE\n\n\nreturns a vector of logical values, which we can use to subset the data:\n\nweight[sex == \"M\"]\n\n[1] 0.230 0.281 0.260\n\n\nGiven that R was born for statistics, there are many statistical functions you can perform on vectors:\n\nlength(x)\n\n[1] 8\n\nmin(x)\n\n[1] 2\n\nmax(x)\n\n[1] 31\n\nsum(x) # sum all elements\n\n[1] 117\n\nprod(x) # multiply all elements\n\n[1] 105436890\n\nmedian(x) # median value\n\n[1] 15\n\nmean(x) # arithmetic mean\n\n[1] 14.625\n\nvar(x) # unbiased sample variance\n\n[1] 119.4107\n\nmean(x ^ 2) - mean(x) ^ 2 # population variance\n\n[1] 104.4844\n\nsummary(x) # print a summary\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2.00    4.50   15.00   14.62   21.00   31.00 \n\n\nYou can generate vectors of sequential numbers using the colon command:\n\nx &lt;- 1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nFor more complex sequences, use seq:\n\nseq(from = 1, to = 5, by = 0.5)\n\n[1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\n\nTo repeat a value or a sequence several times, use rep:\n\nrep(\"abc\", 3)\n\n[1] \"abc\" \"abc\" \"abc\"\n\nrep(c(1, 2, 3), 3)\n\n[1] 1 2 3 1 2 3 1 2 3\n\n\n\nExercise:\n\nCreate a vector containing all the even numbers between 2 and 100 (inclusive) and store it in variable z.\nExtract all the elements of z that are divisible by 12. How many elements match this criterion?\nWhat is the sum of all the elements of z?\nIs it equal to \\(51 \\cdot 50\\)?\nWhat is the product of elements 5, 10 and 15 of z?\nDoes seq(2, 100, by = 2) produce the same vector as (1:50) * 2?\nWhat happens if you type z ^ 2?\n\n\n\n\n1.6.5.2 Matrices\nA matrix is a two-dimensional table of values. In case of numeric values, you can perform the usual operations on matrices (product, inverse, decomposition, etc.):\n\nA &lt;- matrix(c(1, 2, 3, 4), 2, 2) # values, nrows, ncols\nA \n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nA %*% A # matrix product\n\n     [,1] [,2]\n[1,]    7   15\n[2,]   10   22\n\nsolve(A) # matrix inverse\n\n     [,1] [,2]\n[1,]   -2  1.5\n[2,]    1 -0.5\n\nA %*% solve(A) # this should return the identity matrix\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\nB &lt;- matrix(1, 3, 2) # you can fill the whole matrix with a single number (1)\nB\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n[3,]    1    1\n\nB %*% t(B) # transpose\n\n     [,1] [,2] [,3]\n[1,]    2    2    2\n[2,]    2    2    2\n[3,]    2    2    2\n\nZ &lt;- matrix(1:9, 3, 3) # by default, matrices are filled by column\nZ\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nTo determine the dimensions of a matrix, use dim:\n\ndim(B)\n\n[1] 3 2\n\ndim(B)[1]\n\n[1] 3\n\nnrow(B) \n\n[1] 3\n\ndim(B)[2]\n\n[1] 2\n\nncol(B)\n\n[1] 2\n\nnrow(B)\n\n[1] 3\n\n\nUse indices to access a particular row/column of a matrix:\n\nZ\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\nZ[1, ] # first row\n\n[1] 1 4 7\n\nZ[, 2] # second column\n\n[1] 4 5 6\n\nZ [1:2, 2:3] # submatrix with coefficients in first two rows, and second and third column\n\n     [,1] [,2]\n[1,]    4    7\n[2,]    5    8\n\nZ[c(1, 3), c(1, 3)] # indexing non-adjacent rows/columns\n\n     [,1] [,2]\n[1,]    1    7\n[2,]    3    9\n\n\nSome functions use all the elements of the matrix:\n\nsum(Z)\n\n[1] 45\n\nmean(Z)\n\n[1] 5\n\n\nSome functions apply the operation across a given dimension (e.g., columns) of the matrix:\n\nrowSums(Z) # returns a vector of the sums of the values in each row\n\n[1] 12 15 18\n\ncolSums(Z) # does the same for columns\n\n[1]  6 15 24\n\nrowMeans(Z) # returns a vector of the means of the values in each row\n\n[1] 4 5 6\n\ncolMeans(Z) # does the same for columns\n\n[1] 2 5 8\n\n\n\n\n1.6.5.3 Arrays\nIf you need tables with more than two dimensions, use arrays:\n\nM &lt;- array(1:24, c(4, 3, 2))\nM \n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   13   17   21\n[2,]   14   18   22\n[3,]   15   19   23\n[4,]   16   20   24\n\n\nYou can still determine the dimensions using:\n\ndim(M)\n\n[1] 4 3 2\n\n\nand access the elements as done for matrices. One thing you should be paying attention to: R drops dimensions that are not needed. So, if you access a “slice” of a 3-dimensional array:\n\nM[, , 1]\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n\nyou obtain a matrix:\n\ndim(M[, , 1])\n\n[1] 4 3\n\n\nThis can be problematic, for example, when your code expects an array and R turns your data into a matrix (or you expect a matrix but find a vector). To avoid this behavior, add drop = FALSE when subsetting:\n\ndim(M[, , 1, drop = FALSE])\n\n[1] 4 3 1\n\n\n\n\n1.6.5.4 Lists\nVectors are good if each element is of the same type (e.g., numbers, strings). Lists are used when we want to store elements of different types, or more complex objects (e.g., vectors, matrices, even lists of lists). Each element of the list can be referenced either by its index, or by a label:\n\nmylist &lt;- list(Names = c(\"a\", \"b\", \"c\", \"d\"), Values = c(1, 2, 3))\nmylist\n\n$Names\n[1] \"a\" \"b\" \"c\" \"d\"\n\n$Values\n[1] 1 2 3\n\nmylist[[1]] # access first element using index\n\n[1] \"a\" \"b\" \"c\" \"d\"\n\nmylist[[2]] # access second element by index\n\n[1] 1 2 3\n\nmylist$Names # access second element by label\n\n[1] \"a\" \"b\" \"c\" \"d\"\n\nmylist[[\"Names\"]] # another way to access by label\n\n[1] \"a\" \"b\" \"c\" \"d\"\n\nmylist[[\"Values\"]][3]  # access third element in second vector\n\n[1] 3\n\n\n\n\n1.6.5.5 Data frames\nData frames contain data organized like in a spreadsheet. The columns (typically representing different measurements) can be of different types (e.g., a column could be the date of measurement, another the weight of the individual, or the volume of the cell, or the treatment of the sample), while the rows typically represent different samples.\nWhen you read a spreadsheet file in R, it is automatically stored as a data frame. The difference between a matrix and a data frame is that in a matrix all the values are of the same type (e.g., all numeric), while in a data frame each column can be of a different type.\nBecause typing a data frame by hand would be tedious, let’s use a data set that is already available in R:\n\ndata(trees) # girth, height and volume of cherry trees\nstr(trees) # structure of data frame\n\n'data.frame':   31 obs. of  3 variables:\n $ Girth : num  8.3 8.6 8.8 10.5 10.7 10.8 11 11 11.1 11.2 ...\n $ Height: num  70 65 63 72 81 83 66 75 80 75 ...\n $ Volume: num  10.3 10.3 10.2 16.4 18.8 19.7 15.6 18.2 22.6 19.9 ...\n\nncol(trees)\n\n[1] 3\n\nnrow(trees)\n\n[1] 31\n\nhead(trees) # print the first few rows\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\nsummary(trees) # Quickly get an overview of the data frame.\n\n     Girth           Height       Volume     \n Min.   : 8.30   Min.   :63   Min.   :10.20  \n 1st Qu.:11.05   1st Qu.:72   1st Qu.:19.40  \n Median :12.90   Median :76   Median :24.20  \n Mean   :13.25   Mean   :76   Mean   :30.17  \n 3rd Qu.:15.25   3rd Qu.:80   3rd Qu.:37.30  \n Max.   :20.60   Max.   :87   Max.   :77.00  \n\ntrees$Girth # select column by name\n\n [1]  8.3  8.6  8.8 10.5 10.7 10.8 11.0 11.0 11.1 11.2 11.3 11.4 11.4 11.7 12.0\n[16] 12.9 12.9 13.3 13.7 13.8 14.0 14.2 14.5 16.0 16.3 17.3 17.5 17.9 18.0 18.0\n[31] 20.6\n\ntrees$Height[1:5] # select column by name; return first five elements\n\n[1] 70 65 63 72 81\n\ntrees[1:3, ] #select rows 1 through 3\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n\ntrees[1:3, ]$Volume # select rows 1 through 3; return column Volume\n\n[1] 10.3 10.3 10.2\n\ntrees &lt;- rbind(trees, c(13.25, 76, 30.17)) # add a row\ntrees_double &lt;- cbind(trees, trees) # combine columns\ncolnames(trees) &lt;- c(\"Circumference\", \"Height\", \"Volume\") # change column names\n\n\nExercise:\n\nWhat is the average height of the cherry trees?\nWhat is the average girth of those that are more than 75 ft tall?\nWhat is the maximum height of trees with a volume between 15 and 35 ft\\(^3\\)?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`R`efresher</span>"
    ]
  },
  {
    "objectID": "00-refresher.html#reading-and-writing-data",
    "href": "00-refresher.html#reading-and-writing-data",
    "title": "1  Refresher",
    "section": "1.7 Reading and writing data",
    "text": "1.7 Reading and writing data\nIn most cases, you will not generate your data in R, but import it from a file. By far, the best option is to have your data in a comma separated value text file or in a tab separated file. Then, you can use the function read.csv (or read.table) to import your data. The syntax of the functions is as follows:\n\nread.csv(\"MyFile.csv\") # read the file MyFile.csv\nread.csv(\"MyFile.csv\", header = TRUE) # the file has a header\nread.csv(\"MyFile.csv\", sep = ';') # specify the column separator\nread.csv(\"MyFile.csv\", skip = 5) # skip the first 5 lines\n\nNote that columns containing strings are typically converted to factors (categorical values, useful when performing regressions). To avoid this behavior, you can specify stringsAsFactors = FALSE when calling the function.\nSimilarly, you can save your data frames using write.table or write.csv. Suppose you want to save the data frame MyDF:\n\nwrite.csv(MyDF, \"MyFile.csv\") \nwrite.csv(MyDF, \"MyFile.csv\", append = TRUE) # append to the end of the file \nwrite.csv(MyDF, \"MyFile.csv\", row.names = TRUE) # include the row names\nwrite.csv(MyDF, \"MyFile.csv\", col.names = FALSE) # do not include column names\n\nLet’s look at an example: Read a file containing data on the 6th chromosome for a number of Europeans (Data adapted from Stanford HGDP SNP Genotyping Data by John Novembre). This example shows that you can read data directly from the internet!\n\n# The actual URL is\n# https://github.com/StefanoAllesina/BSD-QBio4/raw/master/tutorials/basic_computing_1/data/H938_Euro_chr6.geno\nch6 &lt;- read.table(\"https://tinyurl.com/y7vctq3v\", \n                  header = TRUE, stringsAsFactors = FALSE)\n\nwhere header = TRUE means that we want to take the first line to be a header containing the column names. How big is this table?\n\ndim(ch6)\n\n[1] 43141     7\n\n\nwe have 7 columns, but more than 40k rows! Let’s see the first few:\n\nhead(ch6)\n\n  CHR        SNP A1 A2 nA1A1 nA1A2 nA2A2\n1   6  rs4959515  A  G     0    17   107\n2   6   rs719065  A  G     0    26    98\n3   6  rs6596790  C  T     0     4   119\n4   6  rs6596796  A  G     0    22   102\n5   6  rs1535053  G  A     5    39    80\n6   6 rs12660307  C  T     0     3   121\n\n\nand the last few:\n\ntail(ch6)\n\n      CHR        SNP A1 A2 nA1A1 nA1A2 nA2A2\n43136   6 rs10946282  C  T     0    16   108\n43137   6  rs3734763  C  T    19    56    48\n43138   6   rs960744  T  C    32    60    32\n43139   6  rs4428484  A  G     1    11   112\n43140   6  rs7775031  T  C    26    56    42\n43141   6 rs12213906  C  T     1    11   112\n\n\nThe data contains the number of homozygotes (nA1A1, nA2A2) and heterozygotes (nA1A2), for 43,141 single nucleotide polymorphisms (SNPs) obtained by sequencing European individuals:\n\nCHR The chromosome (6 in this case)\nSNP The identifier of the Single Nucleotide Polymorphism\nA1 One of the alleles\nA2 The other allele\nnA1A1 The number of individuals with the particular combination of alleles.\n\n\nExercise:\n\nHow many individuals were sampled? Find the maximum of the sum nA1A1 + nA1A2 + nA2A2. Note: you can access the columns by index (e.g., ch6[,5]), or by name (e.g., ch6$nA1A1, or also ch6[,\"nA1A1\"]).\nTry using the function rowSums to obtain the same result.\nFor how many SNPs do we have that all sampled individuals are homozygotes (i.e., all A1A1 or all A2A2)?\nFor how many SNPs, are more than 99% of the sampled individuals homozygous?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`R`efresher</span>"
    ]
  },
  {
    "objectID": "00-refresher.html#conditional-branching",
    "href": "00-refresher.html#conditional-branching",
    "title": "1  Refresher",
    "section": "1.8 Conditional branching",
    "text": "1.8 Conditional branching\nNow we turn to writing actual programs in the Source code panel. To start a new R program, press Ctrl + Shift + N. This will open an Untitled script. Save the script by pressing Ctrl + S: save it as conditional.R in the directory programming_skills/sandbox/. To make sure you’re working in the directory where the script is contained, on the menu on the top choose Session -&gt; Set Working Directory -&gt; To Source File Location.\nNow type the following script:\n\nprint(\"Hello world!\")\nx &lt;- 4\nprint(x)\n\nand execute the script by pressing Ctrl + Shift + S. You should see Hello World! and 4 printed in your console.\nAs you saw in this simple example, when R executes the program, it starts from the top and proceeds toward the end of the file. Every time it encounters a command (for example, print(x), printing the value of x into the console), it executes it.\nWhen we want a certain block of code to be executed only when a certain condition is met, we can write a conditional branching point. The syntax is as follows:\n\nif (condition is met){\n  # execute this block of code\n} else {\n  # execute this other block of code\n}\n\nFor example, add these lines to the script conditional.R, and run it again:\n\nprint(\"Hello world!\")\nx &lt;- 4\nprint(x)\nif (x %% 2 == 0){\n  my_message &lt;- paste(x, \"is even\")\n} else {\n  my_message &lt;- paste(x, \"is odd\")\n}\nprint(my_message)\n\nWe have created a conditional branching point, so that the value of my_message changes depending on whether x is even (and thus the remainder of the integer division by 2 is 0), or odd. Change the line x &lt;- 4 to x &lt;- 131 and run it again.\n\nExercise: What does this do?\n\n\n\nx &lt;- 36\nif (x &gt; 20){\n  x &lt;- sqrt(x)\n} else {\n  x &lt;- x ^ 2\n}\nif (x &gt; 7) {\n  print(x)\n} else if (x %% 2 == 1){\n  print(x + 1)\n}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`R`efresher</span>"
    ]
  },
  {
    "objectID": "00-refresher.html#looping",
    "href": "00-refresher.html#looping",
    "title": "1  Refresher",
    "section": "1.9 Looping",
    "text": "1.9 Looping\nAnother way to change the flow of the program is to write a loop. A loop is simply a series of commands that are repeated a number of times. For example, you want to run the same analysis on different data sets that you collected; you want to plot the results contained in a set of files; you want to test your simulation over a number of parameter sets; etc.\nR provides you with two ways to loop over blocks of commands: the for loop, and the while loop. Let’s start with the for loop, which is used to iterate over a vector (or a list): for each value of the vector, a series of commands will be run, as shown by the following example, which you can type in a new script called forloop.R.\n\nmyvec &lt;- 1:10 # vector with numbers from 1 to 10\n\nfor (i in myvec) {\n  a &lt;- i ^ 2\n  print(a)\n}\n\nIn the code above, the variable i takes the value of each element of myvec in sequence. Inside the block defined by the for loop, you can use the variable i to perform operations.\nThe anatomy of the for statement:\n\nfor (variable in list_or_vector) {\n  execute these commands\n} # automatically moves to the next value\n\nFor loops are used when you know that you want to perform the analysis using a given set of values (e.g., run over all files of a directory, all samples in your data, all sequences of a fasta file, etc.).\nThe while loop is used when the commands need to be repeated while a certain condition is true, as shown by the following example, which you can type in a script called whileloop.R:\n\ni &lt;- 1\n\nwhile (i &lt;= 10) {\n  a &lt;- i ^ 2\n  print(a)\n  i &lt;- i + 1 \n}\n\nThe script performs exactly the same operations we wrote for the for loop above. Note that you need to update the value of i, (using i &lt;- i + 1), otherwise the loop will run forever (infinite loop—to terminate click on the stop button in the top-right corner of the console). The anatomy of the while statement:\n\nwhile (condition is met) {\n  execute these commands\n} # beware of infinite loops: remember to update the condition!\n\nYou can break a loop using the command break. For example:\n\ni &lt;- 1\n\nwhile (i &lt;= 10) {\n  if (i &gt; 5) {\n    break\n  }\n  a &lt;- i ^ 2\n  print(a)\n  i &lt;- i + 1\n}\n\n\nExercise: What does this do? Try to guess what each loop does, and then create and run a script to confirm your intuition.\n\n\n\nz &lt;- seq(1, 1000, by = 3)\nfor (k in z) {\n  if (k %% 4 == 0) {\n     print(k)\n  }\n}\n\n\n\n\n\n\n\nz &lt;- readline(prompt = \"Enter a number: \")\nz &lt;- as.numeric(z)\nisthisspecial &lt;- TRUE\ni &lt;- 2\nwhile (i &lt; z) {\n  if (z %% i == 0) {\n     isthisspecial &lt;- FALSE\n     break\n  }\n  i &lt;- i + 1\n}\nif (isthisspecial == TRUE) {\n  print(z)\n}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`R`efresher</span>"
    ]
  },
  {
    "objectID": "00-refresher.html#useful-functions",
    "href": "00-refresher.html#useful-functions",
    "title": "1  Refresher",
    "section": "1.10 Useful Functions",
    "text": "1.10 Useful Functions\nHere’s a short list of useful functions that will help you write your programs:\n\nrange(x): minimum and maximum of a vector x\nsort(x): sort a vector x\nunique(x): remove duplicate entries from vector x\nwhich(x == a): returns a vector of the indices of x having value a\nlist.files(\"path_to_directory\"): list the files in a directory (current directory if not specified)\ntable(x) build a table of frequencies\n\n\nExercises: What does this code do? For each snippet of code, first try to guess what will happen. Then, write a script and run it to confirm your intuition.\n\n\n\nv &lt;- c(1, 3, 5, 5, 3, 1, 2, 4, 6, 4, 2)\nv &lt;- sort(unique(v))\nfor (i in v){\n  if (i &gt; 2){\n    print(i)\n  }\n  if (i &gt; 4){\n    break\n  }\n}\n\n\n\n\n\n\n\nx &lt;- 1:100\nx &lt;- x[which(x %% 7 == 0)]\n\n\n\n\n\n\n\nmy_amount &lt;- 10\nwhile (my_amount &gt; 0){\n  my_color &lt;- NA\n  while(is.na(my_color)){\n    tmp &lt;- readline(prompt=\"Do you want to bet on black or red? \")\n    tmp &lt;- tolower(tmp)\n    if (tmp == \"black\") my_color &lt;- \"black\"\n    if (tmp == \"red\") my_color &lt;- \"red\"\n    if (is.na(my_color)) print(\"Please enter either red or black\")\n  }\n  my_bet &lt;- NA\n  while(is.na(my_bet)){\n    tmp &lt;- readline(prompt=\"How much do you want to bet? \")\n    tmp &lt;- as.numeric(tmp)\n    if (is.numeric(tmp) == FALSE){\n      print(\"Please enter a number\")\n    } else {\n      if (tmp &gt; my_amount){\n        print(\"You don't have enough money!\")\n      } else {\n        my_bet &lt;- tmp\n        my_amount &lt;- my_amount - tmp\n      }\n    }\n  }\n  lady_luck &lt;- sample(c(\"red\", \"black\"), 1)\n  if (lady_luck == my_color){\n    my_amount &lt;- my_amount + 2 * my_bet\n    print(paste(\"You won!! Now you have\", my_amount, \"gold doubloons\"))\n  } else {\n    print(paste(\"You lost!! Now you have\", my_amount, \"gold doubloons\"))\n  }\n}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`R`efresher</span>"
    ]
  },
  {
    "objectID": "00-refresher.html#packages",
    "href": "00-refresher.html#packages",
    "title": "1  Refresher",
    "section": "1.11 Packages",
    "text": "1.11 Packages\nR is the most popular statistical computing software among biologists due to its highly specialized packages, often written by biologists for biologists. You can contribute a package too! The RStudio support (goo.gl/harVqF) provides guidance on how to start developing R packages and Hadley Wickham’s free online book (r-pkgs.had.co.nz) will make you a pro.\nYou can find highly specialized packages to address your research questions. Here are some suggestions for finding an appropriate package. The Comprehensive R Archive Network (CRAN) offers several ways to find specific packages for your task. You can either browse packages (goo.gl/7oVyKC) and their short description or select a scientific field of interest (goo.gl/0WdIcu) to browse through a compilation of packages related to each discipline.\nFrom within your R terminal or RStudio you can also call the function RSiteSearch(\"KEYWORD\"), which submits a search query to the website search.r-project.org. The website rseek.org casts an even wider net, as it not only includes package names and their documentation but also blogs and mailing lists related to R. If your research interests relate to high-throughput genomic data, you should have a look the packages provided by Bioconductor (goo.gl/7dwQlq).\n\n1.11.1 Installing a package\nTo install a package type\n\ninstall.packages(\"name_of_package\")\n\nin the Console, or choose the panel Packages and then click on Install in RStudio.\n\n\n1.11.2 Loading a package\nTo load a package type\n\nlibrary(name_of_package)\n\nor call the command into your script. If you want your script to automatically install a package in case it’s missing, use this boilerplate:\n\nif (!require(needed_package, character.only = TRUE, quietly = TRUE)) {\n    install.packages(needed_package)\n    library(needed_package, character.only = TRUE)\n}\n\n\n\n1.11.3 Example\nFor example, say we want to access the dataset bacteria, which reports the incidence of H. influenzae in Australian children. The dataset is contained in the package MASS.\nFirst, we need to load the package:\n\nlibrary(MASS)\n\nNow we can load the data:\n\ndata(bacteria)\nbacteria[1:3,]\n\n  y ap hilo week  ID     trt\n1 y  p   hi    0 X01 placebo\n2 y  p   hi    2 X01 placebo\n3 y  p   hi    4 X01 placebo",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`R`efresher</span>"
    ]
  },
  {
    "objectID": "00-refresher.html#random-numbers",
    "href": "00-refresher.html#random-numbers",
    "title": "1  Refresher",
    "section": "1.12 Random numbers",
    "text": "1.12 Random numbers\nTo perform randomization, or any simulation, we typically need to draw random numbers. R has functions to sample random numbers from very many different statistical distributions. For example:\n\nrunif(5) # sample 5 numbers from the uniform distribution between 0 and 1\n\n[1] 0.2088868 0.1502735 0.7005580 0.4700187 0.9240479\n\nrunif(5, min = 1, max = 9) # set the limits of the uniform distribution\n\n[1] 1.310876 1.441832 3.964806 3.315722 5.632026\n\nrnorm(3) # three values from standard normal\n\n[1]  0.05741184 -0.74206297 -0.18606703\n\nrnorm(3, mean = 5, sd = 4) # specify mean and standard deviation\n\n[1] 4.533529 4.527350 3.130423\n\n\nTo sample from a set of values, use sample:\n\nv &lt;- c(\"a\", \"b\", \"c\", \"d\")\nsample(v, 2) # without replacement\n\n[1] \"d\" \"b\"\n\nsample(v, 6, replace = TRUE) # with replacement\n\n[1] \"d\" \"a\" \"a\" \"c\" \"d\" \"a\"\n\nsample(v) # simply shuffle the elements\n\n[1] \"a\" \"c\" \"b\" \"d\"",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`R`efresher</span>"
    ]
  },
  {
    "objectID": "00-refresher.html#writing-functions",
    "href": "00-refresher.html#writing-functions",
    "title": "1  Refresher",
    "section": "1.13 Writing functions",
    "text": "1.13 Writing functions\nThe R community provides about 7,000 packages. Still, sometimes there isn’t an already made function capable of doing what you need. In these cases, you can write your own functions. In fact, it is generally a good idea to always divide your analysis into functions, and then write a small “master” program that calls the functions and performs the analysis. In this way, the code will be much more legible, and you will be able to recycle the functions for your other projects.\nA function in R has this form:\n\nmy_function_name &lt;- function(optional, arguments, separated, by_commas){\n  # Body of the function\n  # ...\n  # \n  return(return_value) # this is optional\n}\n\nA few examples:\n\nsum_two_numbers &lt;- function(a, b){\n  apb &lt;- a + b  \n  return(apb)\n}\nsum_two_numbers(5, 7.2)\n\n[1] 12.2\n\n\nYou can set a default value for some of the arguments: if not specified by the user, the function will use these defaults:\n\nsum_two_numbers &lt;- function(a = 1, b = 2){\n  apb &lt;- a + b  \n  return(apb)\n}\nsum_two_numbers()\n\n[1] 3\n\nsum_two_numbers(3)\n\n[1] 5\n\nsum_two_numbers(b = 9)\n\n[1] 10\n\n\nThe return value is optional:\n\nmy_factorial &lt;- function(a = 6){\n  if (as.integer(a) != a) {\n    print(\"Please enter an integer!\")\n  } else {\n    tmp &lt;- 1\n    for (i in 2:a){\n      tmp &lt;- tmp * i\n    }\n    print(paste(a, \"! = \", tmp, sep = \"\"))\n  }\n}\nmy_factorial()\n\n[1] \"6! = 720\"\n\nmy_factorial(10)\n\n[1] \"10! = 3628800\"\n\n\nYou can return only one object. If you need to return multiple values, organize them into a vector/matrix/list and return that.\n\norder_two_numbers &lt;- function(a, b){\n  if (a &gt; b) return(c(a, b)) #nothing after the first return is executed\n  return(c(b,a))\n}\n\norder_two_numbers(runif(1), runif(1))\n\n[1] 0.9291461 0.1098093",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`R`efresher</span>"
    ]
  },
  {
    "objectID": "00-refresher.html#organizing-and-running-code",
    "href": "00-refresher.html#organizing-and-running-code",
    "title": "1  Refresher",
    "section": "1.14 Organizing and running code",
    "text": "1.14 Organizing and running code\nDuring the class, we will write a lot of code, of increasing complexity. Here is what you should do to ensure that your programs are well-organized, easy to understand, and easy to debug.\n\nTake the problem, and divide it into its basic building blocks. Each block should be its own function.\nWrite the code for each building block separately, and test it thoroughly.\nExtensively document the code, so that you can understand what you did, how you did it, and why.\nCombine the building blocks into a master program.\n\nFor example, let’s write code that takes the data on Chromosome 6 we have seen above, and tries to identify which SNPs deviate the most from Hardy-Weinberg equilibrium. Remember that in an infinite population, where mating is random, there is no selection and no mutations, the proportion of people carrying the alleles \\(A1A1\\) should be approximately \\(p_{11} = p^2\\) (where \\(p\\) is the frequency of the first allele in the population \\(p = p_{11} + \\frac{1}{2} p_{12}\\)), those carrying \\(A1A2\\) should be \\(p_{12} = 2 p q\\) (where \\(q = 1-p\\)) and finally those carrying \\(A2A2\\) should be \\(p_{22} = q^2\\). This is called the Hardy-Weinberg equilibrium.\nWe want to test this on a number of different SNPs. First, we write a function that takes as input the data and a given SNP, and computes the probability \\(p\\) of carrying the first allele.\n\ncompute_probabilities_HW &lt;- function(my_data, my_SNP = \"rs1535053\"){\n  # Take a SNP and compute the probabilities\n  # p = frequency of first allele\n  # q = frequency of second allele (1 - p)\n  # p11 = proportion homozygous first allele\n  # p12 = proportion heterozygous\n  # p22 = proportion homozygous second allele\n  my_SNP_data &lt;- my_data[my_data$\"SNP\" == my_SNP,]\n  AA &lt;- my_SNP_data$nA1A1\n  AB &lt;- my_SNP_data$nA1A2\n  BB &lt;- my_SNP_data$nA2A2\n  tot_observations &lt;- AA + AB + BB\n  p11 &lt;- AA / tot_observations\n  p12 &lt;- AB / tot_observations\n  p22 &lt;- BB / tot_observations\n  p &lt;- p11 + p12 / 2\n  q &lt;- 1 - p\n  return(list(SNP = my_SNP,\n              p11 = p11,\n              p12 = p12,\n              p22 = p22,\n              p = p,\n              q = q,\n              tot = tot_observations,\n              AA = AA,\n              AB = AB,\n              BB = BB))\n}\n\nNow we can test our function:\n\ncompute_probabilities_HW(ch6)\n\n$SNP\n[1] \"rs1535053\"\n\n$p11\n[1] 0.04032258\n\n$p12\n[1] 0.3145161\n\n$p22\n[1] 0.6451613\n\n$p\n[1] 0.1975806\n\n$q\n[1] 0.8024194\n\n$tot\n[1] 124\n\n$AA\n[1] 5\n\n$AB\n[1] 39\n\n$BB\n[1] 80\n\n\nIf the allele conformed to Hardy-Weinberg, we should find approximately \\(p^2 \\cdot n\\) people with \\(A1A1\\), where \\(n\\) is the number of people sampled. Let’s see whether these assumptions are met by the data:\n\nobserved_vs_expected_HW &lt;- function(SNP_data){\n  # compute expectations under Hardy-Weinberg equilibrium\n  # organize expected and observed in a table\n  observed &lt;- c(\"AA\" = SNP_data$AA, \"AB\" = SNP_data$AB, \"BB\" = SNP_data$BB)\n  expected &lt;- c(\"AA\" = SNP_data$p^2 * SNP_data$tot, \n                \"AB\" = 2 * SNP_data$p * SNP_data$q * SNP_data$tot, \n                \"BB\" = SNP_data$q^2 * SNP_data$tot)\n  return(rbind(observed, expected))\n}\n\nAnd test it:\n\nmy_SNP_data &lt;- compute_probabilities_HW(ch6)\nobserved_vs_expected_HW(my_SNP_data)\n\n               AA       AB       BB\nobserved 5.000000 39.00000 80.00000\nexpected 4.840726 39.31855 79.84073\n\n\nPretty good! This SNP seems very close to the theoretical expectation.\nLet’s try another one\n\nobserved_vs_expected_HW(compute_probabilities_HW(ch6, \"rs1316662\"))\n\n               AA       AB       BB\nobserved 26.00000 62.00000 36.00000\nexpected 26.20161 61.59677 36.20161\n\n\nBecause we have so many SNPs, we will surely find some that do not comply with the expectation. For example:\n\nmy_SNP_data &lt;- compute_probabilities_HW(ch6, \"rs6596835\")\nobserved_vs_expected_HW(my_SNP_data)\n\n                AA      AB      BB\nobserved 17.000000 24.0000 82.0000\nexpected  6.837398 44.3252 71.8374\n\n\nTo find those with the largest deviations, we can compute for the statistic:\n\\[\n\\sum_i \\frac{(e_i - o_i)^2}{e_i}\n\\] In genetics, this is called \\(\\chi^2\\) statistics, because if the data were to follow the assumptions, these quantities would follow the \\(\\chi^2\\) distribution.\n\ncompute_chi_sq_stat &lt;- function(my_obs_vs_expected){\n  observed &lt;- my_obs_vs_expected[\"observed\",]\n  expected &lt;- my_obs_vs_expected[\"expected\",]\n  return(sum((expected - observed)^2 / expected))\n}\n\nNow let’s compute the statistic for each SNPs:\n\n# because this might take a while, we're going to only analyze the first 1000 SNPs\nall_SNPs &lt;- ch6$SNP[1:1000]\nresults &lt;- data.frame(SNP = all_SNPs, ChiSq = 0)\nfor (i in 1:nrow(results)){\n  results[i, 2] &lt;- compute_chi_sq_stat(observed_vs_expected_HW(compute_probabilities_HW(ch6, results[i, 1])))\n}\n\nTo find the ones with the largest discrepancy, run\n\nresults &lt;- results[order(results$ChiSq, decreasing = TRUE),]\nhead(results)\n\n          SNP     ChiSq\n10  rs2281351 53.993853\n221 rs1933650 27.724832\n36  rs6596835 25.862675\n681  rs689035  9.802277\n178 rs6930805  9.491511\n179 rs1737539  9.491511\n\n\nThis example showed how a seemingly difficult problem can be decomposed in smaller problems that are easier to solve.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`R`efresher</span>"
    ]
  },
  {
    "objectID": "00-refresher.html#documenting-the-code-using-knitr",
    "href": "00-refresher.html#documenting-the-code-using-knitr",
    "title": "1  Refresher",
    "section": "1.15 Documenting the code using knitr",
    "text": "1.15 Documenting the code using knitr\n\nLet us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to humans what we want the computer to do.\n\n\nDonald E. Knuth, Literate Programming, 1984\n\nWhen doing experiments, we typically keep track of everything we do in a laboratory notebook, so that when writing the manuscript, or responding to queries, we can go back to our documentation to find exactly what we did, how we did it, and possibly why we did it. The same should be true for computational work.\nRStudio makes it very easy to build a computational laboratory notebook. First, create a new R Markdown file (choose File -&gt; New File -&gt; R Markdown from the menu).\nThe gist of it is that you write a text file (.Rmd). The file is then read by an interpreter that transforms it into an .html or .pdf file, or even into a Word document. You can use special syntax to render the text in different ways. For example, type\n***********\n\n*Test* **Test2**\n\n# Very large header\n\n## Large header\n\n### Smaller header\n\n## Unordered lists\n\n* First\n* Second\n    + Second 1\n    + Second 2\n\n1. This is\n2. A numbered list\n\nYou can insert `inline code`\n\n-----------\nThe most important feature of R Markdown, however, is that you can include blocks of code, and they will be interpreted and executed by R. You can therefore combine effectively the code itself with the description of what you are doing.\nFor example, including\n{r, eval=FALSE}   print(\"hello world!\")\nwill become\n\nprint(\"hello world!\")  \n\n[1] \"hello world!\"\n\n\nIf you don’t want to run the R code, but just display it, use {r, eval = FALSE}; if you want to show the output but not the code, use {r, echo = FALSE}.\nYou can include plots, tables, and even render equations using LaTeX. In summary, when exploring your data or writing the methods of your paper, give R Markdown a try!\nYou can find inspiration in the notes for this class: all are written in R Markdown.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`R`efresher</span>"
    ]
  },
  {
    "objectID": "00-refresher.html#resources",
    "href": "00-refresher.html#resources",
    "title": "1  Refresher",
    "section": "1.16 Resources",
    "text": "1.16 Resources\nThere are very many excellent books and tutorials you can read to become a proficient programmer in R. For example:\n\nIntro to R\nAdvanced R\nDataCamp\nComputerWorld\nR Style guide\nR for Data Science\nRStudio Cheat Sheet\nBase R Cheat Sheet\nAdvanced R Cheat Sheet\nX in Y minutes\nIntro to Data Wrangling\nR Boot Camp",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>`R`efresher</span>"
    ]
  },
  {
    "objectID": "01-dataviz.html",
    "href": "01-dataviz.html",
    "title": "2  Visualizing data using ggplot2",
    "section": "",
    "text": "2.1 Goal\nIntroduce the package ggplot2, which is part of the tidyverse bundle. Learn how to use ggplot2 to produce publication-quality figures. Discuss the philosophical underpinnings of the “Grammar of Graphics”, showcase the ggplot2 syntax, produce examples of the different types of graphs. Learn how to change colors, legends, scales. Visualize histograms, barplots, scatterplots, etc.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing data using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "01-dataviz.html#introduction-to-the-grammar-of-graphics",
    "href": "01-dataviz.html#introduction-to-the-grammar-of-graphics",
    "title": "2  Visualizing data using ggplot2",
    "section": "2.2 Introduction to the Grammar of Graphics",
    "text": "2.2 Introduction to the Grammar of Graphics\nThe most salient feature of scientific graphs should be clarity. Each figure should make crystal-clear a) what is being plotted; b) what are the axes; c) what do colors, shapes, and sizes represent; d) the message the figure wants to convey. Each figure is accompanied by a (sometimes long) caption, where the details can be explained further, but the main message should be clear from glancing at the figure (often, figures are the first thing editors and referees look at).\nMany scientific publications contain very poor graphics: labels are missing, scales are unintelligible, there is no explanation of some graphical elements. Moreover, some color graphs are impossible to understand if printed in black and white, or difficult to discern for color-blind people.\nGiven the effort that you put into your science, you want to ensure that it is well presented and accessible. The investment to master some plotting software will be rewarded by pleasing graphics that convey a clear message.\nIn this section, we introduce ggplot2, a plotting package for R This package was developed by Hadley Wickham who contributed many important packages to R (all included in the tidyverse bundle we’re going to use for the reminder of the class). Unlike many other plotting systems, ggplot2 is deeply rooted in a “philosophical” vision. The goal is to conceive a grammar for all graphical representation of data. Leland Wilkinson and collaborators proposed The Grammar of Graphics. It follows the idea of a well-formed sentence that is composed of a subject, a predicate, and an object. The Grammar of Graphics likewise aims at describing a well-formed graph by a grammar that captures a very wide range of statistical and scientific graphics. This might be more clear with an example – Take a simple two-dimensional scatterplot. How can we describe it? We have:\n\nData The data we want to plot.\nMapping What part of the data is associated with a particular visual feature? For example: Which column is associated with the x-axis? Which with the y-axis? Which column corresponds to the shape or the color of the points? In ggplot2 lingo, these are called aesthetic mappings (aes).\nGeometry Do we want to draw points? Lines? In ggplot2 we speak of geometries (geom).\nScale Do we want the sizes and shapes of the points to scale according to some value? Linearly? Logarithmically? Which palette of colors do we want to use?\nCoordinate We need to choose a coordinate system (e.g., Cartesian, polar).\nFaceting Do we want to produce different panels, partitioning the data according to one (or more) of the variables?\n\nThis basic grammar can be extended by adding statistical transformations of the data (e.g., regression, smoothing), multiple layers, adjustment of position (e.g., stack bars instead of plotting them side-by-side), annotations, and so on.\nExactly like in the grammar of a natural language, we can easily change the meaning of a “sentence” by adding or removing parts. Also, it is very easy to completely change the type of geometry if we are moving from say a histogram to a boxplot or a violin plot, as these types of plots are meant to describe one-dimensional distributions. Similarly, we can go from points to lines, changing one “word” in our code. Finally, the look and feel of the graphs is controlled by a theming system, separating the content from the presentation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing data using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "01-dataviz.html#basic-ggplot2",
    "href": "01-dataviz.html#basic-ggplot2",
    "title": "2  Visualizing data using ggplot2",
    "section": "2.3 Basic ggplot2",
    "text": "2.3 Basic ggplot2\nggplot2 ships with a simplified graphing function, called qplot. In this introduction we are not going to use it, and we concentrate instead on the function ggplot, which gives you complete control over your plotting. We will need to use the package tidyverse, which is already loaded if you are using the Web version of the book. If you want to copy this code into an R script to execute on your local machine, start with this line:\n\nlibrary(tidyverse)\n\nTo explore the features of ggplot2, we are going to use a data set detailing the total number of COVID cases and deaths in US counties. The data are provided by the New York Times.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nwe are going to work with date, county, state, cases and deaths.\nLet’s select Illnois, and take only the counties with more than 10k cases (to have a less crowded graph):\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nA particularity of ggplot2 is that it accepts exclusively data organized in tables (a data.frame or a tibble object—more on tibbles later). Thus, all of your data needs to be converted into a data frame format for plotting.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing data using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "01-dataviz.html#building-a-well-formed-graph",
    "href": "01-dataviz.html#building-a-well-formed-graph",
    "title": "2  Visualizing data using ggplot2",
    "section": "2.4 Building a well-formed graph",
    "text": "2.4 Building a well-formed graph\nFor our first plot, we’re going to produce a barplot detailing how many cases have been reported in each County:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAs you can see, nothing is drawn: we need to specify what we would like to associate to the x axis, and what to the y axis, etc. (i.e., we want to set as the aesthetic mappings). A barplot typically has classes on the x axis, while the y axis reports the counts in each class.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNote that we concatenate pieces of our “sentence” using the + sign! We’ve got the aestethic mappings figured out, but still no graph… we need to specify a geometry, i.e., the type of graph we want to produce. In this case, a barplot where the height of the bars is specified by the y value:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBecause it is very difficult to see the labels, let’s swap the axes:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe graph shows that, naturally, the vast majority of cases was reported in Cook county. We have written a “well-formed sentence”, composed of data + mapping + geometry, and this is sufficient to produce a graph. We can add “adjectives” and “adverbs” to our graph, to make it clearer:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing data using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "01-dataviz.html#scatterplots",
    "href": "01-dataviz.html#scatterplots",
    "title": "2  Visualizing data using ggplot2",
    "section": "2.5 Scatterplots",
    "text": "2.5 Scatterplots\nUsing ggplot2, one can produce very many types of graphs. The package works very well for 2D graphs (or 3D rendered in two dimensions), while it lack capabilities to draw proper 3D graphs, or networks.\nThe main feature of ggplot2 is that you can tinker with your graph fairly easily, and with a common grammar. You don’t have to settle on a certain presentation of the data until you’re ready, and it is very easy to switch from one type of graph to another.\nFor example, let’s plot the number of cases vs. number of deaths:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nShowing that number of daily cases and number of daily deaths are highly correlated (but it would be a stronger correlation if we were to plot past cases vs. current deaths).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing data using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "01-dataviz.html#histograms-density-and-boxplots",
    "href": "01-dataviz.html#histograms-density-and-boxplots",
    "title": "2  Visualizing data using ggplot2",
    "section": "2.6 Histograms, density and boxplots",
    "text": "2.6 Histograms, density and boxplots\nIt would be nice to see the distribution of the ratio deaths/cases. To do so, we can produce a histogram:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can control the width of the bins by specifying:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nLet’s see whether the histograms differ between Illinois and Indiana:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTo plot the histogram side by side, use\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSimilarly, we can approximate the histogram using a density plot, which interpolates the bin height to create a smooth distribution:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTo see the graph better, let’s make the coloring semi-transparent:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nShowing a similar distribution for the death rate in the two states. For this type of comparison, the ideal graph to show is maybe a box-plot or a violin plot:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nA boxplot shows the median (horizontal bar) as well as the inter-quartile range (box size goes from 25th to 75th percentile), as well as the typical range of the data (whiskers). The dots represent “outliers”. To show the full distribution, you can use a violin plot:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNote that when we’re producing “similar” plots (e.g., histogram vs. density, box vs. violin, or any other plot sharing the same aesthetic mappings) changing a single word, we have changed the structure of the graph considerably!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing data using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "01-dataviz.html#scales",
    "href": "01-dataviz.html#scales",
    "title": "2  Visualizing data using ggplot2",
    "section": "2.7 Scales",
    "text": "2.7 Scales\nWe can use scales to determine how the aesthetic mappings are displayed. For example, we could set the x axis to be in logarithmic scale, or we can choose how the colors, shapes and sizes are used. ggplot2 uses two types of scales: continuous scales are used for continuos variables (e.g., real numbers); discrete scales for variables that can only take a certain number of values (e.g., colors, shapes, sizes).\nFor example, let’s plot deaths vs. cases in our dti data set:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can change the scale of the x axis by calling:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSimilarly, we can change the use of colors, points, etc.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing data using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "01-dataviz.html#list-of-aesthetic-mappings",
    "href": "01-dataviz.html#list-of-aesthetic-mappings",
    "title": "2  Visualizing data using ggplot2",
    "section": "2.8 List of aesthetic mappings",
    "text": "2.8 List of aesthetic mappings\nWe’ve seen some of the aesthetic mappings. Here’s a list of the main aes:\n\nx what to use for x axis\ny what to use for y axis\ncolor the color of points and lines\nfill the color of shapes (e.g., boxes, bars, etc.)\nsize the size of points, lines, etc.\nshape the shape of points\nalpha the level of transparency of the object\nlinetype the type of line (e.g., solid, dashed, etc.)\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing data using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "01-dataviz.html#list-of-geometries",
    "href": "01-dataviz.html#list-of-geometries",
    "title": "2  Visualizing data using ggplot2",
    "section": "2.9 List of geometries",
    "text": "2.9 List of geometries\nThere are very many geometries; here are a few of the most useful ones:\n\nLines: geom_abline (line given slope and intercept); geom_hline, geom_vline (horizontal, vertical line); geom_line (connect observation in scatterplot).\nBars: geom_bar (bar height is the count/sum); geom_col (bar heigts are provided by the data).\nBoxes: geom_boxplot.\nDistributions: geom_violin (like boxplots, but showing the density of the distribution); geom_density (density of 1D distribution), geom_density2d (density of bivariate distribution); geom_histogram, geom_bin2d (histograms).\nText: geom_text.\nSmoothing function: geom_smooth (interpolates the points of a scatterplot).\nError bars: geom_errorbar.\nMaps: geom_map (polygons from a reference map).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing data using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "01-dataviz.html#list-of-scales",
    "href": "01-dataviz.html#list-of-scales",
    "title": "2  Visualizing data using ggplot2",
    "section": "2.10 List of scales",
    "text": "2.10 List of scales\nThere are also very many scales. Here are a few:\n\nxlab, ylab, xlim, ylim control labels and ranges of the axes.\nscale_alpha transparency of the points/shapes.\nscale_color (many options) colors of points and lines.\nscale_fill (many options) colors of boxes, bars and shapes.\nscale_shape shape of the points.\nscale_linetype type of lines.\nscale_size size of points and lines.\nscale_x, scale_y (many options) transformations of the axes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing data using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "01-dataviz.html#themes",
    "href": "01-dataviz.html#themes",
    "title": "2  Visualizing data using ggplot2",
    "section": "2.11 Themes",
    "text": "2.11 Themes\nThemes allow you to manipulate the look and feel of a graph with just one command. The package ggthemes extends the themes collection of ggplot2 considerably. For example:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing data using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "01-dataviz.html#faceting",
    "href": "01-dataviz.html#faceting",
    "title": "2  Visualizing data using ggplot2",
    "section": "2.12 Faceting",
    "text": "2.12 Faceting\nIn many cases, we would like to produce a multi-panel graph, in which each panel shows the data for a certain combination of parameters. In ggplot2 this is called faceting: the command facet_grid is used when you want to produce a grid of panels, in which all the panels in the same row (or column) have axes-ranges in common; facet_wrap is used when the different panels do not necessarily have axes-ranges in common.\nFor example:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nLet’s add a line separating showing the best-fit line:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nMake ranges on x and y axes equal, and add the 1:1 line:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing data using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "01-dataviz.html#setting-features",
    "href": "01-dataviz.html#setting-features",
    "title": "2  Visualizing data using ggplot2",
    "section": "2.13 Setting features",
    "text": "2.13 Setting features\nOften, you want to simply set a feature (e.g., the color of the points, or their shape), rather than using it to display information (i.e., mapping some aestethic). In such cases, simply declare the feature outside the aes:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing data using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "01-dataviz.html#saving-graphs",
    "href": "01-dataviz.html#saving-graphs",
    "title": "2  Visualizing data using ggplot2",
    "section": "2.14 Saving graphs",
    "text": "2.14 Saving graphs\nYou can either save graphs as done normally in R:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nor use the function ggsave\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing data using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "01-dataviz.html#multiple-layers",
    "href": "01-dataviz.html#multiple-layers",
    "title": "2  Visualizing data using ggplot2",
    "section": "2.15 Multiple layers",
    "text": "2.15 Multiple layers\nYou can overlay different plots. To do so, however, they must share some of the aesthetic mappings. The simplest case is that in which you have only one dataset:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing data using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "01-dataviz.html#try-on-your-own-data",
    "href": "01-dataviz.html#try-on-your-own-data",
    "title": "2  Visualizing data using ggplot2",
    "section": "2.16 Try on your own data!",
    "text": "2.16 Try on your own data!\nNow that you’re familiar with ggplot2, try producing some meaningful plots for your own data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing data using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "01-dataviz.html#resources",
    "href": "01-dataviz.html#resources",
    "title": "2  Visualizing data using ggplot2",
    "section": "2.17 Resources",
    "text": "2.17 Resources\n\nR for Data Science\nTidyverse reference website\nData Visualization Cheat Sheet",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualizing data using `ggplot2`</span>"
    ]
  },
  {
    "objectID": "02-probdist.html",
    "href": "02-probdist.html",
    "title": "3  Fundamentals of probability",
    "section": "",
    "text": "3.1 Sample spaces and random variables\nNo observation or measurement in our world is perfectly reproducible, no matter how carefully planned and executed. The level of uncertainly varies, but randomness always finds a way to creep into a data set. Where does the “random” factor come from? From the classical physics perspective, as articulated by Laplace, most natural phenomena are theoretically deterministic for an omniscient being with an unlimited computational power. Quantum mechanical phenomena are (theoretically) truly random, but the randomness is not observable on the scales of biology or social science. The lack of predictability in the data we work with is usually due either to its intrinsic complexity (e.g., bio-molecular systems, prediction of animal behavior), which essentially makes it impossible to know every detail of the system, or to some external source of noise (e.g., measurement error, weather affecting food availability) that is outside of our control.\nIn probability terminology, a random experiment produces outcomes and the collection of all outcomes of an experiment is called its sample space.\nExample: The specifics of the experiment can affect the degree of uncertainty in the outcome; the same measurement may be random or not, depending on context. For example, measuring the height of a person should be deterministic, if one measures the height of the same person within a short amount of time. So unless you’re interested in studying the error in stadiometer results, you probably won’t consider this a random experiment. However, measuring the heights of different people is a random experiment, where the source of randomness is primarily due to the selection of people for your study, called sampling error, rather than due to the measurement noise of any one person.\nThe measurement of interest from a random experiment is called a random variable. Sometimes the measurement is simply the outcome, but usually it reports some aspect of the outcome and so several outcomes can have the same value of the random variable. The random variable can then be seen as condensing the sample space into a smaller range of values. Random variables can be numeric or categorical, with the difference that categorical variables cannot be assigned meaningful numbers. For instance, one may report an individual by phenotype (e.g., white or purple flowers), or having a nucleotide A, T, G, C in a particular position, and although one could assign numbers to these categories (e.g., 1, 2, 3, 4) they could not be used in a sensible way—one can compare and do arithmetic with numbers, but A is not less than T and A + T does not equal G. Thus there are different tools for describing and working with numeric and categorical random variables.\nExample: In a DNA sequence a codon triplet represents a specific amino acid, but there is redundancy (several triplets may code for the same amino acid). One may think of a coding DNA sequence as an outcome, but the amino acid (sequence or single one) as a random variable. Extending this framework, one may think of genotype as an outcome, but a phenotype (e.g., eye color) as a random variable—although this is not correct for any phenotype that is not strictly determined by the genotype, because then there are other factors (e.g., environmental or epigenetic) that influence the value of the random variable besides the outcome (genotype).\nExercise: The package palmerpenguins contains multiple variables measured in populations of three different species of penguins over three years on three different islands. Identify numeric and categorical variables, and specify whether numeric variables are discrete and continuous.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Fundamentals of probability</span>"
    ]
  },
  {
    "objectID": "02-probdist.html#probability-axioms",
    "href": "02-probdist.html#probability-axioms",
    "title": "3  Fundamentals of probability",
    "section": "3.2 Probability axioms",
    "text": "3.2 Probability axioms\nAn outcome in sample space can be assigned a probability depending on its frequency of occurrence out of many trials, each is a number between 0 and 1. Combinations of outcomes (events) can be assigned probabilities by building them out of individual outcomes. These probabilities have a few rules, called the axioms of probability, expressed using set theory notation.\n\nThe total probability of all outcomes in sample space is 1. \\(P(\\Omega) = 1\\)\nThe probability of nothing (empty set) is 0. \\(P(\\emptyset) = 0\\)\nThe probability of an event made up of the union of two events is the sum of the two probabilities minus the probability of the overlap (intersection.) \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\nExample: Let’s assign a probability to every possible three-letter codon. There are \\(4^3 = 64\\) codons, so if one assumes that each one has equal probability, then they they all equal \\(1/64\\) (by axiom 1.) The probability of a codon having A as the first letter is 1/4, and so is the probability of A as the second letter. Axiom 3 allows us to calculate the probability of A in either the first or the second letter:\n\\[ P(AXX \\cup \\ XAX ) =  P(AXX) + P(XAX) - P(AAX) = 1/4 + 1/4 - 1/16 = 7/16\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Fundamentals of probability</span>"
    ]
  },
  {
    "objectID": "02-probdist.html#probability-distributions",
    "href": "02-probdist.html#probability-distributions",
    "title": "3  Fundamentals of probability",
    "section": "3.3 Probability distributions",
    "text": "3.3 Probability distributions\nThe probability of each value of a random variable can be calculated from the probability of the event that corresponds to each value of the random variable. The collection of the probabilities of all of the values of the random variable is called the probability distribution function of the random variable, more formally the mass function for a discrete random variable or the density function for a continuous random variable.\nFor a discrete random variable (let’s call it \\(X\\)) with a probability mass function \\(f\\), the probability of \\(X\\) taking the value of \\(a\\) can be written either as \\(f(X=a)\\) or \\(f(a)\\), as long as it’s clear that \\(f\\) is the probability distribution function of \\(X\\). The one ironclad rule of probability is that all values of the mass function have to add up to 1. To state this mathematically, if all the possible values of \\(X\\) can be written as \\(a_1, a_2, ...\\) (there may be finitely or infinitely many of them, as long as it’s a countable infinity), this sum has to be equal to 1: \\[ \\sum_i f(a_i) = 1 \\]\nA continuous random variable (let’s call it \\(Y\\)) with a probability density function \\(g\\) is a bit more complicated. The continuous part means that the random variable has uncountably many values, even if the range is finite (for example, there are uncountably many real numbers between 0 and 1). Thus, the probability of any single value must be vanishingly small (zero), otherwise it would be impossible to add up (integrate) all of the values and get a finite result (let alone 1). We can only measure the probability of a range of values of \\(Y\\) and it is defined by the integral of the density function overall that range:\n\\[ P( a&lt; Y &lt; b) = \\int_a ^b g(y) dy \\]\nThe total probability over the entire range of \\(Y\\) has to be 1, but it’s similarly calculated by integration instead of summation (\\(R\\) represents the range of values of \\(Y\\)):\n\\[ \\int_R g(y) dy = 1\\]\nExample: As codons (DNA triplets) code for amino acids, we can consider the genetic code a random variable on the sample space. Assuming all codons have equal probabilities, the probability of each amino acid is the number of triplets that code for it divided by 64. For example, the probabilities of leucine and arginine are \\(6/64 = 3/32\\), the probability of threonine is \\(4/64 = 1/16\\) and the probabilities of methionine and tryptophan are \\(1/64\\). This defines a probability distribution function of the random variable of the genetic code. Note that the sum of all the probabilities of amino acids has to be 1. Of course there is no inherent reason why each triplet should be equally probable, so a different probability structure on the sample space would result in a different probability distribution (mass) function.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Fundamentals of probability</span>"
    ]
  },
  {
    "objectID": "02-probdist.html#measures-of-center-medians-and-means",
    "href": "02-probdist.html#measures-of-center-medians-and-means",
    "title": "3  Fundamentals of probability",
    "section": "3.4 Measures of center: medians and means",
    "text": "3.4 Measures of center: medians and means\nThe standard measures described here are applicable only numeric random variables. Some measures of center and spread for categorical variables exist as well.\nThe median of a random variable is the value which is in the middle of the distribution, specifically, that the probability of the random variable being no greater than that value is 0.5.\nThe mean or expectation of a random variable is the center of mass of the probability distribution. Specifically, it is defined for a mass function to be:\n\\[ E(X) = \\sum_i a_i\\, f(a_i)\\]\nAnd for a density function it is defined using the integral: \\[ E(Y) =  \\int_R y\\, g(y) dy \\]\nExample: Let us examine the factors (categorical variables) in the penguins data set. They cannot be described using means and medians, but can be plotted by counts in each category as you learned in the introduction to ggplot2:\n\nggplot(data = penguins) +\n  aes(x = species, fill = sex) + \n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nggplot(data = penguins) +\n  aes(x = year, fill = species) + \n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nOne can plot the distributions of numeric variables like body mass for different penguin species using box plots:\n\nggplot(data = penguins) + aes(x = as.factor(species), y=body_mass_g) + geom_boxplot()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nThe following code chunk uses dplyr functions that we will learn in the next chapter to calculate the mean and median values of these variables aggregated by species:\n\npenguins %&gt;% drop_na() %&gt;% group_by(species) %&gt;% summarise(mean = mean(body_mass_g))\n\n# A tibble: 3 × 2\n  species    mean\n  &lt;fct&gt;     &lt;dbl&gt;\n1 Adelie    3706.\n2 Chinstrap 3733.\n3 Gentoo    5092.\n\npenguins %&gt;% drop_na()  %&gt;% group_by(species) %&gt;% summarise(median = median(body_mass_g))\n\n# A tibble: 3 × 2\n  species   median\n  &lt;fct&gt;      &lt;dbl&gt;\n1 Adelie      3700\n2 Chinstrap   3700\n3 Gentoo      5050\n\n\nComment on how the descriptive statistics correspond to the box plots.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Fundamentals of probability</span>"
    ]
  },
  {
    "objectID": "02-probdist.html#measures-of-spread-quartiles-and-variances",
    "href": "02-probdist.html#measures-of-spread-quartiles-and-variances",
    "title": "3  Fundamentals of probability",
    "section": "3.5 Measures of spread: quartiles and variances",
    "text": "3.5 Measures of spread: quartiles and variances\nAll random variables have spread in their values. The simplest way to describe it is by stating its range (the interval between the minimum and maximum values) and the quartiles (the medians of the two halves of the distribution).\nA more standard measure of the spread of a distribution is the variance, defined as the expected value of the squared differences from the mean:\n\\[\\text{Var}(X) = E [X - E(X)]^2 = \\sum_i (a_i- E(X))^2 f(a_i)\\]\nAnd for a density function it is defined using the integral: \\[\\text{Var}(Y) =  E[ Y - E(Y)]^2 = \\int_R (y-E(Y))^2 g(y) dy \\]\nVariances have squared units so they are not directly comparable to the values of the random variable. Taking the square root of the variance converts it into the same units and is called the standard deviation of the distribution: \\[ \\sigma_X = \\sqrt{\\text{Var}(X)}\\] Example: Let’s go back to the penguins data set and calculate the measures of spread for the variable body mass for different penguin species\n\nggplot(data = penguins) + aes(x = as.factor(species), y=body_mass_g) + geom_boxplot()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\npenguins %&gt;% drop_na() %&gt;% group_by(species) %&gt;% summarise(var = var(body_mass_g))\n\n# A tibble: 3 × 2\n  species       var\n  &lt;fct&gt;       &lt;dbl&gt;\n1 Adelie    210332.\n2 Chinstrap 147713.\n3 Gentoo    251478.\n\npenguins %&gt;% drop_na() %&gt;% group_by(species) %&gt;% summarise(first_quart = quantile(body_mass_g,0.25))\n\n# A tibble: 3 × 2\n  species   first_quart\n  &lt;fct&gt;           &lt;dbl&gt;\n1 Adelie          3362.\n2 Chinstrap       3488.\n3 Gentoo          4700 \n\npenguins %&gt;% drop_na() %&gt;% group_by(species) %&gt;% summarise(third_quart = quantile(body_mass_g,0.75))\n\n# A tibble: 3 × 2\n  species   third_quart\n  &lt;fct&gt;           &lt;dbl&gt;\n1 Adelie           4000\n2 Chinstrap        3950\n3 Gentoo           5500\n\n\nWhich species has a wider spread in its body mass? How do the descriptive stats and the box plots correspond?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Fundamentals of probability</span>"
    ]
  },
  {
    "objectID": "02-probdist.html#data-as-samples-from-distributions-statistics",
    "href": "02-probdist.html#data-as-samples-from-distributions-statistics",
    "title": "3  Fundamentals of probability",
    "section": "3.6 Data as samples from distributions: statistics",
    "text": "3.6 Data as samples from distributions: statistics\nIn scientific practice, we collect data from one or more random variables, called a sample, and then try to make sense of it. One of the basic goals is statistical inference: using the data set to describe the population distribution from which the sample was drawn. Data sets can be plotted as histograms and the frequency/fraction of each value should be an approximation of the underlying probability distribution. In addition, descriptive statistics of the sample data (means, variances, medians, etc.) can be used to estimate the true parameters such as the mean and the variance of the population distribution.\nSome of the fundamental questions about the population include:\n\nWhat type of distribution is it?\nEstimate the parameters of that distribution.\nTest a hypothesis, e.g., whether two samples were drawn from the same distribution.\nDescribe and test a relationship between two or more variables.\n\n\n3.6.1 Law of large numbers\nFirst, the sample has to be unbiased, that is, no outcomes should be systematically over- or under-represented. But even an unbiased sample will differ from the population due to the inherent randomness of selection (sampling error). The law of large numbers states that as the sample size increases, the mean of the sample converges to the true mean of the population. Formally, for a set of \\(n\\) independent, identically distributed random variables (the sample) \\(\\{X_i\\}\\) the sample mean \\(\\overline{X}_n\\) converges to the mean of the distribution \\(\\mu\\):\n\\[\n\\lim _{n \\to \\infty} \\frac{\\sum_{i=1}^n {X_i}}{n} = \\lim _{n \\to \\infty} \\overline{X}_n = \\mu\n\\]\n\n\n3.6.2 Central Limit Theorem\nThat is nice to know, but doesn’t say exactly how large a sample is needed to estimate, for example, the mean of the population to a given precision. For that, we have the Central Limit Theorem, which states that the distribution of sample means (from samples of independent, identically distributed random variables) as sample size increases, approaches the normal (Gaussian) distribution with mean equal to the population mean and standard deviation equal to the standard deviation of the population divided by the square root of the sample size. Formally, it states that for a set of \\(n\\) independent, identically distributed random variables (the sample) \\(\\{X_i\\}\\) with distribution mean \\(\\mu\\) and variance \\(\\sigma^2\\), the probability density function of the sample mean \\(\\overline{X}_n\\) converges for large sample size \\(n\\) to the normal distribution:\n\\[\nP(\\overline{X}_n) \\to N(\\mu, \\sigma^2/n)\n\\]\nwhere \\(N(\\mu, \\sigma^2/n\\)) stands for the normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\). One extremely useful consequence of this theorem is that the variance of the sample mean is reciprocally related to the sample size \\(n\\). More precisely, it allows the calculation of confidence intervals by using the normal distribution to generate an interval around the observed sample mean in which the true mean \\(\\mu\\) lies with a given likelihood.\nThis is an amazing result because it applies to any distribution, so it allows for the estimation of means for any situation, as long as the condition of independent, identically distributed variables in the sample is satisfied (the identical distributed condition can actually be relaxed). There are other central limit theorems that apply to other situations, including cases where the random variables in the sample are not independent (e.g., Markov models). The bottom line is that an unbiased sample contains a reflection of the true population, but it is always distorted by uncertainty. Larger sample sizes decrease the uncertainty but are more difficult and expensive to obtain.\nDiscussion: Suggest examples of biological data sets which are not made up of independent identically distributed random variables.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Fundamentals of probability</span>"
    ]
  },
  {
    "objectID": "02-probdist.html#exploration-misleading-means",
    "href": "02-probdist.html#exploration-misleading-means",
    "title": "3  Fundamentals of probability",
    "section": "3.7 Exploration: misleading means",
    "text": "3.7 Exploration: misleading means\nMeans are the most common type of descriptive statistic and are sometimes the only numeric quantity used to compare two data sets, e.g. “the average GPA at school A is 3.5 vs 3.8 at school B”. However, means can be misleading measures in multiple ways.\nFirst, means are highly sensitive to outliers, or points that are very different from other values. They can skew the mean value, even pulling it completely away from the bulk of the values, in which case the mean ceases to be a measure of a “typical” value.\nSecond, there can be funny business with combining means of different subsets of data. Normally, you might expect if you have group A and group B, and each group has two subgroups divided by another variable (e.g. we are comparing the GPAs of students in school A and school B, and we split up the students in each school by gender), then if the means of each subgroup of A and larger than the means of the same subgroup of B (e.g. the GPA of girls and boys in school A are higher than those of their counterparts in school B), then the same relationship should be true for the combined mean of group A and group B (that is, the overall GPA in school A is higher than school B). That is not necessarily true!\nThis apparent contradiction is called Simpson’s paradox. It can be illustrated in the data set of all the passengers and crew on the doomed ocean liner Titanic. The data set is found in the library stablelearner and is loaded by the chunk below:\n\nlibrary(stablelearner)\ndata(titanic)\nstr(titanic)\n\n'data.frame':   2207 obs. of  11 variables:\n $ name    : chr  \"Abbing, Mr. Anthony\" \"Abbott, Mr. Eugene Joseph\" \"Abbott, Mr. Rossmore Edward\" \"Abbott, Mrs. Rhoda Mary 'Rosa'\" ...\n $ gender  : Factor w/ 2 levels \"female\",\"male\": 2 2 2 1 1 2 2 1 2 2 ...\n $ age     : num  42 13 16 39 16 25 30 28 27 20 ...\n $ class   : Factor w/ 7 levels \"1st\",\"2nd\",\"3rd\",..: 3 3 3 3 3 3 2 2 3 3 ...\n $ embarked: Factor w/ 4 levels \"B\",\"C\",\"Q\",\"S\": 4 4 4 4 4 4 2 2 2 4 ...\n $ country : Factor w/ 48 levels \"Argentina\",\"Australia\",..: 44 44 44 15 30 44 17 17 26 16 ...\n $ ticketno: int  5547 2673 2673 2673 348125 348122 3381 3381 2699 3101284 ...\n $ fare    : num  7.11 20.05 20.05 20.05 7.13 ...\n $ sibsp   : Ord.factor w/ 9 levels \"0\"&lt;\"1\"&lt;\"2\"&lt;\"3\"&lt;..: 1 1 2 2 1 1 2 2 1 1 ...\n $ parch   : Ord.factor w/ 10 levels \"0\"&lt;\"1\"&lt;\"2\"&lt;\"3\"&lt;..: 1 3 2 2 1 1 1 1 1 1 ...\n $ survived: Factor w/ 2 levels \"no\",\"yes\": 1 1 1 2 2 2 1 2 2 2 ...\n\n\nThe chunk below calculated the survival probability of passengers of all classes compared to the crew (of all types:\n\ntitanic %&gt;% group_by(Passenger = class %in% c('1st', '2nd', '3rd'), survived) %&gt;% summarise(num = n()) %&gt;% mutate(fraction = num/sum(num)) \n\n`summarise()` has grouped output by 'Passenger'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4 × 4\n# Groups:   Passenger [2]\n  Passenger survived   num fraction\n  &lt;lgl&gt;     &lt;fct&gt;    &lt;int&gt;    &lt;dbl&gt;\n1 FALSE     no         679    0.763\n2 FALSE     yes        211    0.237\n3 TRUE      no         817    0.620\n4 TRUE      yes        500    0.380\n\n\nYou can see that about 24% of the crew survived and almost 38% of the passengers survived. In this week’s assignment you will calculate and explain what happens when you divide the people in each group by gender.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Fundamentals of probability</span>"
    ]
  },
  {
    "objectID": "02-probdist.html#references",
    "href": "02-probdist.html#references",
    "title": "3  Fundamentals of probability",
    "section": "3.8 References",
    "text": "3.8 References\n\nLaplace’s views on probability and determinism\nCentral Limit Theorem in R\nExploration of the Central Limit Theorem\nSimpson’s paradox",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Fundamentals of probability</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html",
    "href": "03-wrangling.html",
    "title": "4  Data wrangling",
    "section": "",
    "text": "4.1 Goal\nLearn how to manipulate large data sets by writing efficient, consistent, and compact code. Introduce the use of dplyr, tidyr, and the “pipe” operator %&gt;%. Effortlessly produce statistics for grouped data. Massage data into “tidy” form.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#what-is-data-wrangling",
    "href": "03-wrangling.html#what-is-data-wrangling",
    "title": "4  Data wrangling",
    "section": "4.2 What is data wrangling?",
    "text": "4.2 What is data wrangling?\nAs biologists living in the XXI century, we are often faced with tons of data, possibly replicated over several organisms, treatments, or locations. We would like to streamline and automate our analysis as much as possible, writing scripts that are easy to read, fast to run, and easy to debug. Base R can get the job done, but often the code contains complicated operations, and a lot of $ signs and brackets.\nWe’re going to learn about the packages dplyr and tidyr, which are part of tidyverse and can be used to manipulate large data frames in a simple and straightforward way. These tools are also much faster than the corresponding base R commands, are very compact, and can be concatenated into “pipelines”.\nTo start, we need to import the libraries:\n\nlibrary(tidyverse) # this loads both dplyr and tidyr, along with other packages\nlibrary(palmerpenguins) # a nice data set to play with\n\n\n# make sure function select is the right one...\nselect &lt;- dplyr::select\n\nWe are going to use the data set penguins from the package palmerpenguins, which we have already seen last week.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#a-new-data-type-tibble",
    "href": "03-wrangling.html#a-new-data-type-tibble",
    "title": "4  Data wrangling",
    "section": "4.3 A new data type, tibble",
    "text": "4.3 A new data type, tibble\nThe data is stored in a “tibble”:\n\nclass(penguins)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nIn fact, dplyr ships with a new data type, called a tibble. To convert a data.frame into a tibble, use as_tibble:\n\n# load a data frame\ndata(\"trees\")\nclass(trees)\ntrees &lt;- as_tibble(trees)\nclass(trees)\n\nThe nice feature of tbl objects is that they will print only what fits on the screen, and also give you useful information on the size of the data, as well as the type of data in each column. Other than that, a tbl object behaves very much like a data.frame. In some rare cases, you want to transform the tbl back into a data.frame. For this, use the function as.data.frame(tbl_object).\nWe can take a look at the data using one of several functions:\n\nhead(dt) shows the first few rows\ntail(dt) shows the last few rows\nglimpse(dt) a summary of the data (similar to str in base R)\nView(dt) open in spreadsheet-like window",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#selecting-rows-and-columns",
    "href": "03-wrangling.html#selecting-rows-and-columns",
    "title": "4  Data wrangling",
    "section": "4.4 Selecting rows and columns",
    "text": "4.4 Selecting rows and columns\nThere are many ways to subset the data, either by row (subsetting the observations), or by column (subsetting the variables). For example, let’s select only the rows with observations from the island Torgersen:\n\nfilter(penguins, island == \"Torgersen\")\n\n# A tibble: 52 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 42 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nWe have 52 observations. We have used the command filter(tbl, conditions) to select certain observations. We can combine several conditions, by listing them side by side, possibly using logical operators.\n\nExercise: what does this do? filter(penguins,  bill_length_mm &gt; 40,  bill_depth_mm &gt; 20, sex == male)\n\nWe can also select particular variables (columns) using the function select(tbl, cols to select). For example, select species and island:\n\nselect(penguins, species, island)\n\n# A tibble: 344 × 2\n   species island   \n   &lt;fct&gt;   &lt;fct&gt;    \n 1 Adelie  Torgersen\n 2 Adelie  Torgersen\n 3 Adelie  Torgersen\n 4 Adelie  Torgersen\n 5 Adelie  Torgersen\n 6 Adelie  Torgersen\n 7 Adelie  Torgersen\n 8 Adelie  Torgersen\n 9 Adelie  Torgersen\n10 Adelie  Torgersen\n# ℹ 334 more rows\n\n\nHow many species are represented in the data set? We can use the function distinct(tbl, cols to select) to retain only the rows that differ from each other:\n\ndistinct(select(penguins, species))\n\n# A tibble: 3 × 1\n  species  \n  &lt;fct&gt;    \n1 Adelie   \n2 Gentoo   \n3 Chinstrap\n\n\nShowing that there are three species, once we removed the duplicates. There are many other ways to subset observations:\n\nslice_sample(tbl, howmany, replace = TRUE) sample howmany rows at random (with replacement)\nsample_sample(tbl, proportion, replace = FALSE) sample a certain proportion (e.g. 0.2 for 20%) of rows at random without replacement\nslice(tbl, 5:20) extract the rows 5 to 20\nslice_max(penguins, 10, body_mass_g) extract the first 10 rows, once ordered by body_mass_g\n\nMore ways to select columns:\n\nselect(penguins, contains(\"mm\")) select all columns containing the string mm\nselect(penguins, -year, -body_mass_g) exclude the columns year and body_mass_g\nselect(penguins, matches(\"length|bill\")) select all columns whose names match a regular expression",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#creating-pipelines-using",
    "href": "03-wrangling.html#creating-pipelines-using",
    "title": "4  Data wrangling",
    "section": "4.5 Creating pipelines using %>%",
    "text": "4.5 Creating pipelines using %&gt;%\nWe’ve been calling nested functions, such as distinct(select(penguins, species)). If you have to add another layer or two, the code would become unreadable. dplyr allows you to “un-nest” these functions and create a “pipeline” in which you concatenate commands separated by a special operator, %&gt;%. For example:\n\npenguins %&gt;% # take a data table\n  select(species) %&gt;% # select a column\n  distinct() # remove duplicates\n\n# A tibble: 3 × 1\n  species  \n  &lt;fct&gt;    \n1 Adelie   \n2 Gentoo   \n3 Chinstrap\n\n\ndoes exactly the same operations as the command above, but is much more readable. By concatenating many commands, you can create incredibly complex pipelines while retaining readability. It is also quite easy to add another piece of the pipeline in between commands, or to comment some of the pipeline out.\nAnother advantage of pipelines is that they help with name completion. In fact, RStudio is running in the background your pipeline while you type it. Try typing dt %&gt;% filter( and then start typing bill and press Tab: you will see the options to complete the column name; choose it with your arrows and hit Return. The back tick-marks will be added automatically if needed (e.g., column names containing spaces, or starting with a digit).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#producing-summaries",
    "href": "03-wrangling.html#producing-summaries",
    "title": "4  Data wrangling",
    "section": "4.6 Producing summaries",
    "text": "4.6 Producing summaries\nSometimes we need to calculate statistics on certain columns. For example, calculate the average body mass of the penguins. We can do this using summarise (you can use British or American spelling):\n\npenguins %&gt;% \n  summarise(avg = mean(body_mass_g, na.rm = TRUE))\n\n# A tibble: 1 × 1\n    avg\n  &lt;dbl&gt;\n1 4202.\n\n# alternatively, drop_na(body_mass_g) removes all the observations for which\n# body_mass_g is NA\npenguins %&gt;% \n  drop_na(body_mass_g) %&gt;% \n  summarise(avg = mean(body_mass_g, na.rm = TRUE))\n\n# A tibble: 1 × 1\n    avg\n  &lt;dbl&gt;\n1 4202.\n\n\nwhere we used na.rm = TRUE to ignore missing values. This command returns a tbl object with just the average body mass. You can combine multiple statistics (use first, last, min, max, n [count the number of rows], n_distinct [count the number of distinct rows], mean, median, var, sd, etc.):\n\npenguins %&gt;% \n  summarise(avg = mean(body_mass_g, na.rm = TRUE), \n            sd = sd(body_mass_g, na.rm = TRUE), \n            median = median(body_mass_g, na.rm = TRUE))\n\n# A tibble: 1 × 3\n    avg    sd median\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 4202.  802.   4050",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#summaries-by-group",
    "href": "03-wrangling.html#summaries-by-group",
    "title": "4  Data wrangling",
    "section": "4.7 Summaries by group",
    "text": "4.7 Summaries by group\nOne of the most useful features of dplyr is the ability to produce statistics for the data once subsetted by groups. For example, we would like to compute the average body mass by species and sex:\n\npenguins %&gt;% \n  drop_na() %&gt;% \n  group_by(sex, species) %&gt;% \n  summarise(mean = mean(body_mass_g, na.rm = TRUE))\n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 6 × 3\n# Groups:   sex [2]\n  sex    species    mean\n  &lt;fct&gt;  &lt;fct&gt;     &lt;dbl&gt;\n1 female Adelie    3369.\n2 female Chinstrap 3527.\n3 female Gentoo    4680.\n4 male   Adelie    4043.\n5 male   Chinstrap 3939.\n6 male   Gentoo    5485.\n\n\nshowing that male penguins are heavier for the three species considered.\n\nExercise: find the average bill_depth_mm and bill_length_mm by species and sex. Filter the data to consider only observations for the year 2008.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#ordering-the-data",
    "href": "03-wrangling.html#ordering-the-data",
    "title": "4  Data wrangling",
    "section": "4.8 Ordering the data",
    "text": "4.8 Ordering the data\nTo order the data according to one or more variables, use arrange():\n\npenguins %&gt;% \n  arrange(body_mass_g) # ascending\n\n# A tibble: 344 × 8\n   species   island   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;     &lt;fct&gt;             &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Chinstrap Dream              46.9          16.6               192        2700\n 2 Adelie    Biscoe             36.5          16.6               181        2850\n 3 Adelie    Biscoe             36.4          17.1               184        2850\n 4 Adelie    Biscoe             34.5          18.1               187        2900\n 5 Adelie    Dream              33.1          16.1               178        2900\n 6 Adelie    Torgers…           38.6          17                 188        2900\n 7 Chinstrap Dream              43.2          16.6               187        2900\n 8 Adelie    Biscoe             37.9          18.6               193        2925\n 9 Adelie    Dream              37.5          18.9               179        2975\n10 Adelie    Dream              37            16.9               185        3000\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\npenguins %&gt;% \n  arrange(desc(body_mass_g)) # descending\n\n# A tibble: 344 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Gentoo  Biscoe           49.2          15.2               221        6300\n 2 Gentoo  Biscoe           59.6          17                 230        6050\n 3 Gentoo  Biscoe           51.1          16.3               220        6000\n 4 Gentoo  Biscoe           48.8          16.2               222        6000\n 5 Gentoo  Biscoe           45.2          16.4               223        5950\n 6 Gentoo  Biscoe           49.8          15.9               229        5950\n 7 Gentoo  Biscoe           48.4          14.6               213        5850\n 8 Gentoo  Biscoe           49.3          15.7               217        5850\n 9 Gentoo  Biscoe           55.1          16                 230        5850\n10 Gentoo  Biscoe           49.5          16.2               229        5800\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#renaming-columns",
    "href": "03-wrangling.html#renaming-columns",
    "title": "4  Data wrangling",
    "section": "4.9 Renaming columns",
    "text": "4.9 Renaming columns\nTo rename one or more columns, use rename():\n\npenguins %&gt;% \n  rename(bm = body_mass_g)\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm    bm sex   \n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt; &lt;int&gt; &lt;fct&gt; \n 1 Adelie  Torgersen           39.1          18.7               181  3750 male  \n 2 Adelie  Torgersen           39.5          17.4               186  3800 female\n 3 Adelie  Torgersen           40.3          18                 195  3250 female\n 4 Adelie  Torgersen           NA            NA                  NA    NA &lt;NA&gt;  \n 5 Adelie  Torgersen           36.7          19.3               193  3450 female\n 6 Adelie  Torgersen           39.3          20.6               190  3650 male  \n 7 Adelie  Torgersen           38.9          17.8               181  3625 female\n 8 Adelie  Torgersen           39.2          19.6               195  4675 male  \n 9 Adelie  Torgersen           34.1          18.1               193  3475 &lt;NA&gt;  \n10 Adelie  Torgersen           42            20.2               190  4250 &lt;NA&gt;  \n# ℹ 334 more rows\n# ℹ 1 more variable: year &lt;int&gt;",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#adding-new-variables-using-mutate",
    "href": "03-wrangling.html#adding-new-variables-using-mutate",
    "title": "4  Data wrangling",
    "section": "4.10 Adding new variables using mutate",
    "text": "4.10 Adding new variables using mutate\nIf you want to add one or more new columns, with the content being a function of other columns, use the function mutate. For example, we are going to add a new column showing the z-score for the body mass of each individual:\n\npenguins %&gt;% \n  mutate(zscore_bm = scale(body_mass_g)) %&gt;% \n  select(species, sex, body_mass_g, zscore_bm)\n\n# A tibble: 344 × 4\n   species sex    body_mass_g zscore_bm[,1]\n   &lt;fct&gt;   &lt;fct&gt;        &lt;int&gt;         &lt;dbl&gt;\n 1 Adelie  male          3750       -0.563 \n 2 Adelie  female        3800       -0.501 \n 3 Adelie  female        3250       -1.19  \n 4 Adelie  &lt;NA&gt;            NA       NA     \n 5 Adelie  female        3450       -0.937 \n 6 Adelie  male          3650       -0.688 \n 7 Adelie  female        3625       -0.719 \n 8 Adelie  male          4675        0.590 \n 9 Adelie  &lt;NA&gt;          3475       -0.906 \n10 Adelie  &lt;NA&gt;          4250        0.0602\n# ℹ 334 more rows\n\n\nWe can pipe the results to ggplot for plotting!\n\npenguins %&gt;% \n  mutate(zscore_bm = scale(body_mass_g)) %&gt;% \n  select(species, sex, body_mass_g, zscore_bm) %&gt;% \n  ggplot() + aes(x = species, y = zscore_bm, colour = sex) + \n    geom_jitter()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nYou can use the function transmute() to create a new column and drop the original columns.\nMost importantly, you can use mutate and transmute on grouped data. For example, let’s recompute the z-score of the body_mass_g once the data is grouped by species and sex:\n\npenguins %&gt;% \n  drop_na() %&gt;% \n  select(species, sex, body_mass_g) %&gt;% \n  group_by(species, sex) %&gt;% \n  mutate(zscore_bm = scale(body_mass_g)) %&gt;% \n  arrange(body_mass_g)\n\n# A tibble: 333 × 4\n# Groups:   species, sex [6]\n   species   sex    body_mass_g zscore_bm[,1]\n   &lt;fct&gt;     &lt;fct&gt;        &lt;int&gt;         &lt;dbl&gt;\n 1 Chinstrap female        2700         -2.90\n 2 Adelie    female        2850         -1.93\n 3 Adelie    female        2850         -1.93\n 4 Adelie    female        2900         -1.74\n 5 Adelie    female        2900         -1.74\n 6 Adelie    female        2900         -1.74\n 7 Chinstrap female        2900         -2.20\n 8 Adelie    female        2925         -1.65\n 9 Adelie    female        3000         -1.37\n10 Adelie    female        3000         -1.37\n# ℹ 323 more rows",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#data-wrangling",
    "href": "03-wrangling.html#data-wrangling",
    "title": "4  Data wrangling",
    "section": "4.11 Data wrangling",
    "text": "4.11 Data wrangling\nData is rarely in a format that is good for computing, and much effort goes into reading the data and wrestling with it to make it into a good format. As the name implies, tidyverse strongly advocates for the use of data in tidy form. What does this mean?\n\nEach variable forms a column\nEach observation forms a row\nEach type of observational unit forms a table\n\nThis is often called narrow table format. Any other form of data (e.g., wide table format) is considered messy. However, often data are not organized in tidy form, or we want to produce tables for human consumption rather than computer consumption. The package tidyr allows to accomplish just that. It contains only a few, very powerful functions. To explore this issue, we build a data set containing the average body mass by species and sex:\n\npenguin_bm &lt;- penguins %&gt;% \n  drop_na() %&gt;% \n  group_by(sex, species) %&gt;% \n  summarise(body_mass = mean(body_mass_g), .groups = \"drop\") # remove groups after calculation\n\npenguin_bm\n\n# A tibble: 6 × 3\n  sex    species   body_mass\n  &lt;fct&gt;  &lt;fct&gt;         &lt;dbl&gt;\n1 female Adelie        3369.\n2 female Chinstrap     3527.\n3 female Gentoo        4680.\n4 male   Adelie        4043.\n5 male   Chinstrap     3939.\n6 male   Gentoo        5485.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#from-narrow-to-wide",
    "href": "03-wrangling.html#from-narrow-to-wide",
    "title": "4  Data wrangling",
    "section": "4.12 From narrow to wide",
    "text": "4.12 From narrow to wide\nOur data is in tidy form. For a paper, we want to show the difference between males and females in a table:\n\npenguin_bm %&gt;% \n  pivot_wider(names_from = sex, values_from = body_mass)\n\n# A tibble: 3 × 3\n  species   female  male\n  &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie     3369. 4043.\n2 Chinstrap  3527. 3939.\n3 Gentoo     4680. 5485.\n\n\nwhere we have created new column names using the values found in sex (hence, names_from), and filled each cell with the corresponding value found in body_mass (hence, values_from). Similarly, if we want to show the data with species as column names, and sex as rows, we can use:\n\npenguin_bm %&gt;% \n  pivot_wider(names_from = species, values_from = body_mass)\n\n# A tibble: 2 × 4\n  sex    Adelie Chinstrap Gentoo\n  &lt;fct&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 female  3369.     3527.  4680.\n2 male    4043.     3939.  5485.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#from-wide-to-narrow",
    "href": "03-wrangling.html#from-wide-to-narrow",
    "title": "4  Data wrangling",
    "section": "4.13 From wide to narrow",
    "text": "4.13 From wide to narrow\nFor a real-world example, we will make data from:\n\nTree-ring analysis for sustainable harvest of Millettia stuhlmannii in Mozambique, I.A.D.Remane M.D.Therrell, South African Journal of Botany Volume 125, September 2019, Pages 120-125\n\nYou can read a tab-separated file from:\n\ndt &lt;- read_tsv(\"https://raw.githubusercontent.com/StefanoAllesina/BIOS_26318/master/data/annual_increment.txt\") %&gt;% \n  select(Age, contains(\"CAT\"))\n\nNew names:\nRows: 172 Columns: 55\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \"\\t\" dbl\n(37): Age, CAT01, CAT03, CAT04A, CAT05B, CAT06, CAT07, CAT08A, CAT09C, C... lgl\n(18): ...38, ...39, ...40, ...41, ...42, ...43, ...44, ...45, ...46, ......\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `Mean` -&gt; `Mean...32`\n• `Mean` -&gt; `Mean...35`\n• `` -&gt; `...37`\n• `` -&gt; `...38`\n• `` -&gt; `...39`\n• `` -&gt; `...40`\n• `` -&gt; `...41`\n• `` -&gt; `...42`\n• `` -&gt; `...43`\n• `` -&gt; `...44`\n• `` -&gt; `...45`\n• `` -&gt; `...46`\n• `` -&gt; `...47`\n• `` -&gt; `...48`\n• `` -&gt; `...49`\n• `` -&gt; `...50`\n• `` -&gt; `...51`\n• `` -&gt; `...52`\n• `` -&gt; `...53`\n• `` -&gt; `...54`\n• `` -&gt; `...55`\n\n# selecting only age and samples\n\nEach column besides YEAR represents a single tree, and each cell contains the diameter (in cm) of the tree when it was at a given age. To make this in tidy form, we first create the columns tree and diameter:\n\ndt &lt;- dt %&gt;% \n  pivot_longer(-Age, names_to = \"tree\", values_to = \"diameter\")\n\nand then remove the NAs:\n\ndt &lt;- dt %&gt;% filter(!is.na(diameter))\n\nNow it is easy to plot the growth trajectory of each tree (as in Fig. 3 of the original paper):\n\ndt %&gt;% \n  ggplot() + \n  aes(x = Age, y = diameter) + \n  geom_line(aes(group = tree)) + # note---this makes a line for each tree\n  geom_smooth(method = \"loess\") # while the smoothing function considers all trees\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#separate-split-a-column-into-two-or-more",
    "href": "03-wrangling.html#separate-split-a-column-into-two-or-more",
    "title": "4  Data wrangling",
    "section": "4.14 Separate: split a column into two or more",
    "text": "4.14 Separate: split a column into two or more\n\ntest &lt;- tibble(name = c(\"Allesina, Stefano\", \"Kondrashov, Dmitry\", \"Mir, Amatullah\"))\ntest\n\n# A tibble: 3 × 1\n  name              \n  &lt;chr&gt;             \n1 Allesina, Stefano \n2 Kondrashov, Dmitry\n3 Mir, Amatullah    \n\n\n\ntest %&gt;% separate(name, into = c(\"last_name\", \"first_name\"), sep = \", \")\n\n# A tibble: 3 × 2\n  last_name  first_name\n  &lt;chr&gt;      &lt;chr&gt;     \n1 Allesina   Stefano   \n2 Kondrashov Dmitry    \n3 Mir        Amatullah \n\n\nThe complement of separate is called unite.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#separate-rows-from-one-row-to-many",
    "href": "03-wrangling.html#separate-rows-from-one-row-to-many",
    "title": "4  Data wrangling",
    "section": "4.15 Separate rows: from one row to many",
    "text": "4.15 Separate rows: from one row to many\n\ntest &lt;- tibble(id = c(1, 2, 3, 4), records = c(\"a;b;c\", \"c;d\", \"a;e\", \"f\"))\ntest\n\n# A tibble: 4 × 2\n     id records\n  &lt;dbl&gt; &lt;chr&gt;  \n1     1 a;b;c  \n2     2 c;d    \n3     3 a;e    \n4     4 f      \n\n\nTo make it into tidy form, only one record per row:\n\ntest %&gt;% separate_rows(records, sep = \";\")\n\n# A tibble: 8 × 2\n     id records\n  &lt;dbl&gt; &lt;chr&gt;  \n1     1 a      \n2     1 b      \n3     1 c      \n4     2 c      \n5     2 d      \n6     3 a      \n7     3 e      \n8     4 f",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#example-brown-bear-brown-bear-what-do-you-see",
    "href": "03-wrangling.html#example-brown-bear-brown-bear-what-do-you-see",
    "title": "4  Data wrangling",
    "section": "4.16 Example: brown bear, brown bear, what do you see?",
    "text": "4.16 Example: brown bear, brown bear, what do you see?\nThis exercise uses a dataset from GBIF, the Global Biodiversity Information Facility. You can download the latest version yourself by doing the following (but just skip ahead if you want to use the data provided by us).\n\nGo to GBIF and click on Occurrences.\nUnder Scientific Name type in Ursus arctos (brown bear), and hit enter.\nTo download the data, create an account on GBIF\nThen click on Download, and select Simple (which should have a tab-delimited .csv file)\nSave to the data folder in your working folder.\n\nIf you don’t want to go through all this, you can load this previously downloaded file called Ursus_GBIF.csv from our GitHub repository. The code in the following chunk loads and displays the contents of the tibble:\n\n# you will need ggmap!\nlibrary(ggmap)\nUrsus_data &lt;- read_tsv(\"https://raw.githubusercontent.com/StefanoAllesina/BIOS_26318/master/data/Ursus_GBIF.csv\")\nglimpse(Ursus_data)\n\nRows: 23,498\nColumns: 50\n$ gbifID                           &lt;dbl&gt; 2382421192, 2382420986, 2382420916, 2…\n$ datasetKey                       &lt;chr&gt; \"88d8974c-f762-11e1-a439-00145eb45e9a…\n$ occurrenceID                     &lt;chr&gt; \"http://arctos.database.museum/guid/U…\n$ kingdom                          &lt;chr&gt; \"Animalia\", \"Animalia\", \"Animalia\", \"…\n$ phylum                           &lt;chr&gt; \"Chordata\", \"Chordata\", \"Chordata\", \"…\n$ class                            &lt;chr&gt; \"Mammalia\", \"Mammalia\", \"Mammalia\", \"…\n$ order                            &lt;chr&gt; \"Carnivora\", \"Carnivora\", \"Carnivora\"…\n$ family                           &lt;chr&gt; \"Ursidae\", \"Ursidae\", \"Ursidae\", \"Urs…\n$ genus                            &lt;chr&gt; \"Ursus\", \"Ursus\", \"Ursus\", \"Ursus\", \"…\n$ species                          &lt;chr&gt; \"Ursus arctos\", \"Ursus arctos\", \"Ursu…\n$ infraspecificEpithet             &lt;chr&gt; NA, NA, NA, \"horribilis\", NA, NA, NA,…\n$ taxonRank                        &lt;chr&gt; \"SPECIES\", \"SPECIES\", \"SPECIES\", \"SUB…\n$ scientificName                   &lt;chr&gt; \"Ursus arctos Linnaeus, 1758\", \"Ursus…\n$ verbatimScientificName           &lt;chr&gt; \"Ursus arctos\", \"Ursus arctos\", \"Ursu…\n$ verbatimScientificNameAuthorship &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ countryCode                      &lt;chr&gt; NA, \"US\", NA, NA, \"US\", NA, NA, \"US\",…\n$ locality                         &lt;chr&gt; \"no specific locality recorded\", \"no …\n$ stateProvince                    &lt;chr&gt; NA, \"Alaska\", NA, NA, \"Colorado\", NA,…\n$ occurrenceStatus                 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ individualCount                  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ publishingOrgKey                 &lt;chr&gt; \"4cadac10-3e7b-11d9-8439-b8a03c50a862…\n$ decimalLatitude                  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ decimalLongitude                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ coordinateUncertaintyInMeters    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ coordinatePrecision              &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ elevation                        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ elevationAccuracy                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ depth                            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ depthAccuracy                    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ eventDate                        &lt;dttm&gt; 1800-01-01, 1800-01-01, 1800-01-01, …\n$ day                              &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ month                            &lt;dbl&gt; 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1…\n$ year                             &lt;dbl&gt; 1800, 1800, 1800, 1800, 1914, 1938, 1…\n$ taxonKey                         &lt;dbl&gt; 2433433, 2433433, 2433433, 6163845, 2…\n$ speciesKey                       &lt;dbl&gt; 2433433, 2433433, 2433433, 2433433, 2…\n$ basisOfRecord                    &lt;chr&gt; \"PRESERVED_SPECIMEN\", \"PRESERVED_SPEC…\n$ institutionCode                  &lt;chr&gt; \"UCM\", \"UCM\", \"UCM\", \"UCM\", \"UCM\", \"U…\n$ collectionCode                   &lt;chr&gt; \"Mammal specimens\", \"Mammal specimens…\n$ catalogNumber                    &lt;chr&gt; \"UCM:Mamm:5003\", \"UCM:Mamm:3329\", \"UC…\n$ recordNumber                     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ identifiedBy                     &lt;chr&gt; \"T. C. Hart\", \"unknown\", \"unknown\", \"…\n$ dateIdentified                   &lt;dttm&gt; 2013-01-01, 1936-01-01, NA, 2015-10-…\n$ license                          &lt;chr&gt; \"CC0_1_0\", \"CC0_1_0\", \"CC0_1_0\", \"CC0…\n$ rightsHolder                     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ recordedBy                       &lt;chr&gt; \"Collector(s): T. C. Hart\", \"Collecto…\n$ typeStatus                       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ establishmentMeans               &lt;chr&gt; NA, NA, NA, NA, NA, NA, \"MANAGED\", NA…\n$ lastInterpreted                  &lt;dttm&gt; 2019-09-03 22:11:14, 2019-09-03 22:1…\n$ mediaType                        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ issue                            &lt;chr&gt; NA, NA, NA, NA, \"TAXON_MATCH_HIGHERRA…\n\n\nYou see there are 50 variables in the data set, so it may be useful to remove the ones we don’t need. For this exercise, our objective is to plot the occurrences of this species on the world map, so we need two variables for certain: decimalLatitude and decimalLongitude, as well as the BasisofRecord for additional information. Use your tidyverse skills to create a new tibble with only those variables. In addition, remove duplicate records from the tibble.\n\n# your code goes here!\n\nNow we can plot this data set on the world map, using the useful package maps. To plot, use the ggplot() syntax with the following addition:\n\nmapWorld &lt;- borders(\"world\", colour=\"gray50\", fill=\"gray50\") # create a layer of borders\n# now you can call \n# ggplot() + mapWorld + ...\n\nNote the warning message generated by ggplot. Then consider the map with the locations of the brown bear specimens. Do any of them seem strange to you? What may be the explanation behind these strange data point? Now filter out the points that you identified as suspicious and print out their BasisofRecord. Does this suggest an explanation for the strangeness?\n\n# your code goes here!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#resources",
    "href": "03-wrangling.html#resources",
    "title": "4  Data wrangling",
    "section": "4.17 Resources",
    "text": "4.17 Resources\n\nR for Data Science\nA cool class at U of C in Social Sciences\nData transformation cheat sheet\nDealing with dates cheat sheet\nData import cheat sheet",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data wrangling</span>"
    ]
  },
  {
    "objectID": "04-distributions.html",
    "href": "04-distributions.html",
    "title": "5  Distributions and their properties",
    "section": "",
    "text": "5.1 Objectives:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distributions and their properties</span>"
    ]
  },
  {
    "objectID": "04-distributions.html#objectives",
    "href": "04-distributions.html#objectives",
    "title": "5  Distributions and their properties",
    "section": "",
    "text": "Apply concepts of conditional probability to practical scenarios and questions\nDescribe independence as a concept and apply to data sets\nUse random number generators to simulate various distributions\nBe familiar with the shape of several common distributions and describe the role of their parameters",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distributions and their properties</span>"
    ]
  },
  {
    "objectID": "04-distributions.html#independence",
    "href": "04-distributions.html#independence",
    "title": "5  Distributions and their properties",
    "section": "5.2 Independence",
    "text": "5.2 Independence\n\n5.2.1 Conditional probability\nIn the basic definitions of probability, we considered the probabilities of each outcome and events separately. Let us consider how information about one event affects the probability of another event. The concept is that if one event (let’s call it \\(B\\)) is true, unless the event is the entire space, it rules out some other outcomes. This may affect the probability of other events (e.g., \\(A\\)) in the sample space, because knowledge of \\(B\\) may rule out some of the outcomes in \\(A\\) as well. Here is the formal definition:\n\nDefinition: For two events \\(A\\) and \\(B\\) in a sample space \\(\\Omega\\) with a probability measure \\(P\\), the probability of \\(A\\) given \\(B\\), called the conditional probability, defined as:\n\n\\(P(A \\vert B) = \\frac{P(A \\cap B)}{P(B)}\\)\n\nwhere \\(A \\cap B\\) or \\(A, B\\) is the intersection of events \\(A\\) and \\(B\\), also known as “\\(A\\) and \\(B\\)”—the event consisting of all outcomes that are in both \\(A\\) and \\(B\\).\n\nIn words, given the knowledge that an event \\(B\\) occurs, the sample space is restricted to the subset \\(B\\), which is why the denominator in the definition is \\(P(B)\\). The numerator encompasses all the outcomes we are interested in, (i.e., \\(A\\)), but since we are now restricted to \\(B\\), the numerator consists of all the outcomes of \\(A\\) which are also in \\(B\\), or \\(A \\cap B\\). The definition makes sense in two extreme cases: if \\(A = B\\) and if \\(A\\) and \\(B\\) are mutually exclusive:\n\\(P(B\\vert B) = P(B \\cap B) /P(B) = P(B)/P(B) = 1\\)\nIf \\(P(A\\cap B) = 0\\), then \\(P(A\\vert B) = 0/P(B) = 0\\)\nImportant note: one common source of confusion about conditional probability is the difference between the probability of \\(A\\) and \\(B\\) and the probability of \\(A\\) given \\(B\\). This is a result of the discrepancy between everyday word usage and mathematical terminology, because the statement “what are the odds of finding a tall person who also likes tea?” is hard to distinguish from “what are the odds that a person who is tall likes tea?” The critical difference between these two statements is that in the former you start out with no information and are picking out a person from the entire population, while is in the latter you start out with the knowledge that a person is tall.\nExample: In the classic Mendelian pea experiment, each diploid organism carries two alleles. The allele \\(A\\) is dominant and results in pink flowers, while \\(a\\) is recessive and results in white flowers. There are three possible genotypes (\\(AA\\), \\(Aa\\), \\(aa\\)) and two phenotypes (Pink or White). For the questions below, assume that two heterozygous pea plants (each having genotype \\(Aa\\)) are crossed, producing the following table of genotypes with equal probabilities in each cell:\n\n\n\nparent\nA\na\n\n\n\n\nA\nAA (pink)\nAa (pink)\n\n\na\nAa (pink)\naa (white)\n\n\n\n\nWhat is the probability that a plant with pink flowers has genotype \\(AA\\)? Write this down in terms of conditional probability and explain how it’s different from the probability of a plant having both pink flower and genotype \\(AA\\).\nWhat is the probability that a plant with genotype \\(AA\\) has pink flowers? Again, write down the conditional probability and explain how it’s different from the probability of a plant having both pink flower and genotype \\(AA\\).\n\nLesson: in general, \\[P(X \\vert  Y ) \\neq P(Y \\vert  X)\\]\n\n\n5.2.2 Independence\nIndependence is a fundamental concept in probability that may be misinterpreted without careful thinking. Intuitively, two events (or random variables) are independent if one does not influence the other. More precisely, it means that the probability of one event is the same regardless of whether the other one happens or not. This is expressed precisely using conditional probabilities:\n\nDefinition: Two events \\(A\\) and \\(B\\) in a sample space \\(\\Omega\\) with a probability measure \\(P\\) are independent if \\(P(A\\vert B) = P(A)\\), or equivalently if \\(P(B\\vert A) = P(B)\\).\n\nIndependence is not a straightforward concept. It may be confused with mutual exclusivity, as one might surmise that if \\(A\\) and \\(B\\) have no overlap, then they are independent. That however, is false by definition, since \\(P(A\\vert B)\\) is 0 for two mutually exclusive events. The confusion stems from thinking that if \\(A\\) and \\(B\\) are non-overlapping, then they do not influence each other. But the notion of influence in this definition is about information; so if \\(A\\) and \\(B\\) are mutually exclusive, the knowledge that one of them occurs has an influence of the probability of the other one occurring, specifically it rules the other one out.\nExample: In the sample space of weather phenomena, are the events of snowing and hot weather independent?\nExample: A slightly more subtle example, the lifetime risk of breast cancer is about 1 in 8 for women and about 1 in 1000 for men. Are sex and breast cancer independent?\n\n\n5.2.3 Usefulness of independence\nIndependence is a mathematical abstraction, and reality rarely provides us with perfectly independent variables. But it’s a very useful abstraction in that it enables calculations that would be difficult or impossible to carry out without this assumption.\nFirst, independence allows for calculating the probability of two events or two random variables simultaneously. This is a straightforward consequence of the definition conditional probability (first equality) and independence (second equality):\n\\[\\frac{P(A \\cap B)}{P(B)}= P(A\\vert B) = P(A)\\] Multiplying both sides by \\(P(B)\\), we get the product rule of independence, perhaps the most widely used formula in applied probability:\n\\[P(A \\cap B) = P(A)P(B)\\]\nExample: The probability that two randomly selected individuals have red hair–assuming that the occurrence of this trait is independent–is the square of the probability of red hair in one individual. (Note that this is never exactly the case for a finite population—why?)\nExample: The probability of two alleles of two separate genes (call them A and B) occurring on the same gamete may be independent or may be linked. In population genetics, the concept of linkage disequilibrium describes the extent of such linkage; for example, alleles that are located on separate chromosomes (in eukaryotes) are usually not linked and their occurrence is independent. The coefficient of linkage disequilibrium is defined as the difference between what is expected from independence and the actual probability of both alleles being present: \\[D_{AB} = P(A \\cap B) - P(A)P(B) \\] \\(P(A)\\) and \\(P(B)\\) are the frequencies of the two respective alleles (haplotypes) in the population, while \\(P(A \\cap B)\\) is the frequency of the haplotypes occurring together in the same copy of the genome (that is, on the same gamete). For two independent loci, \\(D_{AB} = 0\\), while for loci that usually occur together the coefficient will be positive, and its magnitude is influenced both by physical proximity of the loci on a chromosome, the evolutionary history of the species, and other factors.\nAnother important consequence of independence has to do with the sum of two independent random variables. The expectation of the sum of any random variables is linear, which can be demonstrated using some work with sums, starting from the definition of expectation (the same can be shown for continuous random variables, using integrals instead of sums):\n\\[E(X + Y) = \\sum_i \\sum_j (x_i + y_j) P(x_i, y_j) =\\]\n\\[= \\sum_i \\sum_j x_iP(x_i, y_j) + \\sum_i \\sum_j y_j P(x_i, y_j) = \\sum_i x_i \\sum_j P(x_i, y_j) + \\sum_j y_j \\sum_i  P(x_i, y_j) = \\] Summing up a joint probability distribution over all values of one variable removes that variable, \\(\\sum_j P(x_i, y_j) = P(x_i)\\) \\(\\sum_i P(x_i, y_j) = P(y_j)\\), so this leave us with the two separate expected values:\n\\[= \\sum_i x_i P(x_i) + \\sum_j y_j P(y_j) = E(X) + E(Y)\\] However, this is not the case for the variance in general (using \\(E_X\\) and \\(E_Y\\) to indicate the expected values of \\(X\\) and \\(Y\\) to reduce the number of parentheses):\n\\[\\text{Var}(X+Y) = E \\left[ (X+Y)-(E_X+E_Y) \\right]^2 = \\] \\[=E[ (X-E_X)^2 +(Y-E_Y)^2 - 2(X-E_X)(Y-E_Y)] =  \\] \\[=E (X-E_X)^2 +  E(Y-E_Y)^2 - 2 E[(X-E_X)(Y-E_Y)]  = \\] The first two terms are the respective variances, while the third term is called the covariance of \\(X\\) and \\(Y\\):\n\\[= \\text{Var}(X) + \\text{Var}(Y) - 2 \\text{Cov}(X,Y) \\] Covariance describes how much two random variables vary together, or more precisely, how much they deviate from their respective means in the same direction. Thus it should be reasonable to think that two independent random variables have covariance 0, which is demonstrated as follows:\n\\[E[(X-E_X)(Y-E_Y)] = E(XY) - E_Y E_X - E_YE_X + E_XE_Y = E(XY) - E_X E_Y  \\]\nWe can write the expression for the expectation of the random variable comprised of all pairs of values of \\(X\\) and \\(Y\\), using the fact that for two independent random variables, \\(P(x_i,y_j) = P(x_i)P(y_j)\\) for all values \\(x_i\\) and \\(y_j\\):\n\\[E(XY) = \\sum_i \\sum_j x_iy_j P(x_i,y_j) = \\sum_i x_i P(x_i) \\sum_j y_j P(y_j) = E_X E_Y\\] The calculation for two continuous random variables is analogous, only with integrals instead of sums.\nThis demonstrates that the covariance of two independent random variables is 0, and thus that the variance of a sum of two independent random variables is the sum of the two separate variables.\nExample: This property of variance is often used in analysis of noise or error in data. It is commonly assumed in least squares fitting that noise in data is independent of the signal or model underlying the data. This is the foundation for statements like “this linear regression explains 80% of the variance in the data.”",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distributions and their properties</span>"
    ]
  },
  {
    "objectID": "04-distributions.html#probability-distribution-examples-discrete",
    "href": "04-distributions.html#probability-distribution-examples-discrete",
    "title": "5  Distributions and their properties",
    "section": "5.3 Probability distribution examples (discrete)",
    "text": "5.3 Probability distribution examples (discrete)\nThe following are examples of distributions of random variables with discrete values. The first two have finite support (finitely many values) while the second two have infinite support.\n\n5.3.1 Uniform\nThe simplest probability distribution in which every value has the same probability (and one which is sometimes called “purely random” even though any random variable with any distribution is just as random). The probability distribution for a uniform random variable with \\(n\\) values is \\(P(x) = 1/n\\) for any value \\(x\\).\n\nlow &lt;- 0 # minimum value\nhigh &lt;- 10 # maximum value\nvalues &lt;- low:high # vector of discrete values of the RV\nnum &lt;- length(values)\nprobs &lt;- rep(1 / num, num) # uniform mass function vector\nbarplot(probs, names.arg = values, xlab = 'values', ylab = 'probability',\n        main = paste(\"uniform  distribution on integers from \", low, \"to \", high))\n\n\n\n\n\n\n\n\n\nunif.exp &lt;- sum(values*probs)\npaste(\"The expected value of uniform distribution is\", unif.exp)\n\n[1] \"The expected value of uniform distribution is 5\"\n\nunif.var &lt;- sum((unif.exp - values)^2*probs)\npaste(\"The variance of uniform distribution is\", unif.var)\n\n[1] \"The variance of uniform distribution is 10\"\n\n\nExercise: experiment with the low and high values to see how the expectation and variance depend on them. Can you postulate a relationship without looking it up?\n\n\n5.3.2 Binomial\nBinary or Bernoulli trials have two discrete outcomes (mutant/wild-type, win/lose, etc.). The number of “successes” out of a sequence of \\(n\\) independent binary trials with probability of success \\(p\\) is described by the binomial distribution.\n\nn &lt;- 10 # the number of trials\np &lt;- 0.3 # the probability of success in one trial\nvalues &lt;- 0:n # vector of discrete values of the binomial\nprobs &lt;- dbinom(values, n, p)\nbarplot(probs, names.arg = values, xlab = 'values', ylab = 'probability',\n        main = paste(\"binomial distribution with n=\", n, \"and p=\", p))\n\n\n\n\n\n\n\n\n\nbin.exp &lt;- sum(values*probs)\npaste(\"The expected value of binomial distribution is\", bin.exp)\n\n[1] \"The expected value of binomial distribution is 3\"\n\nbin.var &lt;- sum((bin.exp - values)^2*probs)\npaste(\"The variance of binomial distribution is\", bin.var)\n\n[1] \"The variance of binomial distribution is 2.1\"\n\n\nExercise: Try different values of \\(n\\) and \\(p\\) and postulate a relationship with the expectation and variance.\n\n\n5.3.3 Geometric\nThe random variable is the first “success” in a string of independent binary trials and the distribution describes the probability of any non-negative value. It may be pretty intuitive that since all the trials have the same probability of success, the distribution with have a geometric (exponential) form—try to figure out the exact formula for the probability density without looking it up!\n\np &lt;- 0.3 # the probability of success\nlow &lt;- 0 # minimum value\nhigh &lt;- 20 # maximum value\nvalues &lt;- low:high # vector of discrete values of the RV\nprobs &lt;- dgeom(values, p)\nbarplot(probs, names.arg = values, xlab = 'values', ylab = 'probability', main = paste(\"geometric distribution with p=\", p))\n\n\n\n\n\n\n\n\n\ngeom.exp &lt;- sum(values*probs)\npaste(\"The expected value of geometric distribution is\", geom.exp)\n\n[1] \"The expected value of geometric distribution is 2.32030059650472\"\n\ngeom.var &lt;- sum((geom.exp - values)^2*probs)\npaste(\"The variance of geometric distribution is\", geom.var)\n\n[1] \"The variance of geometric distribution is 7.52697882945386\"\n\n\nExercise: Calculate the expectations and variances for different values of \\(p\\) and report how they are related.\n\n\n5.3.4 Poisson\nSuppose that there is a discrete process that occurs with some average rate \\(\\lambda\\), which describes the expected number of occurrences of these events in a unit of time. The Poisson random variable is the number of such occurrences, and the distribution describes the probability of any non-negative value.\n\nlow &lt;- 0 # minimum value\nhigh &lt;- 20 # maximum value\nlambda &lt;- 10 # Poisson rate\nvalues &lt;- low:high # vector of discrete values of the RV\nprobs &lt;- dpois(values, lambda)\nbarplot(probs, names.arg = values, xlab = 'values', ylab = 'probability',\n        main = paste(\"Poisson distribution with lambda=\", lambda))\n\n\n\n\n\n\n\n\n\npois.exp &lt;- sum(values*probs)\npaste(\"The expected value of Poisson distribution is\", pois.exp)\n\n[1] \"The expected value of Poisson distribution is 9.96545658024143\"\n\npois.var &lt;- sum((pois.exp - values)^2*probs)\npaste(\"The variance of Poisson distribution is\", pois.var)\n\n[1] \"The variance of Poisson distribution is 9.77875058489889\"\n\n\nExercise: Calculate the expectations and variances for different values of \\(\\lambda\\) and report how they are related.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distributions and their properties</span>"
    ]
  },
  {
    "objectID": "04-distributions.html#probability-distribution-examples-continuous",
    "href": "04-distributions.html#probability-distribution-examples-continuous",
    "title": "5  Distributions and their properties",
    "section": "5.4 Probability distribution examples (continuous)",
    "text": "5.4 Probability distribution examples (continuous)\nIn the following examples with continuous variables we cannot calculate the means and variances directly from the density function. One way to do it is to produce a sample using the random number generator and calculate the mean and variance of that sample.\n\n5.4.1 Uniform\nThe continuous equivalent of the discrete uniform distribution.\n\nlow &lt;- 0 # minimum value\nhigh &lt;- 10 # maximum values\nnumber &lt;- 100\nvalues &lt;- seq(low, high, length.out = number) # vector of discrete values of the RV\nprobs &lt;- dunif(values, min=low, max = high)\nplot(values, probs, t='l', xlab = 'values', ylab = 'density',\n        main = paste(\"Uniform distribution on interval from \", low, \"to \", high))\n\n\n\n\n\n\n\n\n\nn &lt;- 1000 # sample size\nunif.sample &lt;- runif(n, low, high) # generate sample\nunif.exp &lt;- mean(unif.sample)\npaste(\"The expected value of uniform distribution is\", unif.exp)\n\n[1] \"The expected value of uniform distribution is 4.92755680121947\"\n\nunif.var &lt;- var(unif.sample)\npaste(\"The variance of uniform distribution is\", unif.var)\n\n[1] \"The variance of uniform distribution is 7.99400159263435\"\n\n\nExercise: experiment with the width of the interval to see how it affects the expectation and variance.\n\n\n5.4.2 exponential\nThe random variable describes the length of time between independent discrete events occurring with a certain rate, like we saw in the Poisson distribution.\n\nlow &lt;- 0 # minimum value\nhigh &lt;- 20 # maximum values\nnumber &lt;- 100\nr &lt;- 0.5\nvalues &lt;- seq(low,high,length.out = number) # vector of discrete values of the RV\nprobs &lt;- dexp(values, r)\nplot(values, probs, t='l', xlab = 'values', ylab = 'density',\n        main = paste(\"Exponential distribution with rate=\", r))\n\n\n\n\n\n\n\n\n\nn &lt;- 1000 # sample size\nexp.sample &lt;- rexp(n, r) # generate sample\nexp.exp &lt;- mean(exp.sample)\npaste(\"The expected value of exponential distribution is\", exp.exp)\n\n[1] \"The expected value of exponential distribution is 2.00138398436293\"\n\nexp.var &lt;- var(exp.sample)\npaste(\"The variance of exponential distribution is\", exp.var)\n\n[1] \"The variance of exponential distribution is 3.92775890111002\"\n\n\nExercise: What is the relationship between the rate and the expectation and variance?\n\n\n5.4.3 normal distribution\nThe normal distribution, sometimes written \\(N(\\mu, \\sigma)\\) comes up everywhere (e.g., in the limit of the Poisson distribution for large \\(n\\)). The two parameters are simply the mean and the standard deviation. The reason for its ubiquity is that it is that any sum of a large number of independent random variables converges to the normal, formalized by the Central Limit Theorem:\n\nFor a set of \\(n\\) IID random variables \\(\\{X_i\\}\\) with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the sample mean \\(\\bar X_n\\) has the property: \\[\n\\lim_{n \\to \\infty} = \\frac{\\bar X_n - \\mu}{\\sigma} = N(0,1)\n\\] where \\(N(0,1)\\) stands for the normal distribution with mean 0 and standard deviation 1.\n\n\nlow &lt;- 0 # minimum value\nhigh &lt;- 10 # maximum values\nnumber &lt;- 100\nmu &lt;- 5\nsigma &lt;- 0.5 \nvalues &lt;- seq(low,high,length.out = number) # vector of discrete values of the RV\nprobs &lt;- dnorm(values, mu, sigma)\nplot(values, probs, t='l',xlab = 'values', ylab = 'density',\n        main = paste(\"Normal distribution with mean=\", mu, \"and sigma=\", sigma))\n\n\n\n\n\n\n\n\n\nn &lt;- 1000 # sample size\nnorm.sample &lt;- rnorm(n, mu, sigma) # generate sample\nnorm.exp &lt;- mean(norm.sample)\npaste(\"The expected value of normal distribution is\", norm.exp)\n\n[1] \"The expected value of normal distribution is 5.01852257155184\"\n\nnorm.var &lt;- var(norm.sample)\npaste(\"The variance of normal distribution is\", norm.var)\n\n[1] \"The variance of normal distribution is 0.229809168294851\"",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distributions and their properties</span>"
    ]
  },
  {
    "objectID": "04-distributions.html#application-of-normal-distribution-confidence-intervals",
    "href": "04-distributions.html#application-of-normal-distribution-confidence-intervals",
    "title": "5  Distributions and their properties",
    "section": "5.5 Application of normal distribution: confidence intervals",
    "text": "5.5 Application of normal distribution: confidence intervals\nThe most important use of the normal distribution has to do with estimation of means, because the normal distribution describes the sampling distributions of means of IID samples. The mean of that sampling distribution is the mean of the population distribution that is being sampled, and the standard deviation is called the standard error and is related to the standard deviation of the population \\(\\sigma_X\\) as follows: \\(\\sigma_{SE} = \\sigma/n\\), where \\(n\\) is the sample size.\n\nnumsamples &lt;- 1000\nsize &lt;- 100\n# compute mean for different samples\nsamplemeans &lt;- replicate(n = numsamples, mean(sample(0:10, size, replace = TRUE)))\nbreak_points &lt;- seq(min(samplemeans), max(samplemeans), \n                    (max(samplemeans) - min(samplemeans)) / 20)\nhist(samplemeans, breaks = break_points, freq = FALSE, \n     cex.axis = 1.5, cex.lab = 1.5,\n     main= '1000 means of samples of size 100')\nsigma &lt;- 10 / sqrt(12) / sqrt(size)\nmu &lt;- 5\nrange &lt;- seq(min(samplemeans), max(samplemeans), sigma / 100)\nlines(range, \n      dnorm(range, mu, sigma),\n      t = 'l', lwd = 3, col = 2, lty = 1, cex.axis = 1.5, cex.lab = 1.5)\n\n\n\n\n\n\n\n\nExercise: Try using different distributions from above and see if the sample means still converge to the normal distribution.\nThe following script calculates a confidence interval based on a sample.\n\n# Computing confidence intervals\nqnorm(0.5) # the value that divides the density function in two\n\n[1] 0\n\nqnorm(0.95) # the value such that 95% of density is to its left \n\n[1] 1.644854\n\nsize &lt;- 100 # sample size\nalpha &lt;- 0.95 # significance level\nsample &lt;- runif(size)\ns &lt;- sd(sample) / sqrt(size) # standard error\nz &lt;- qnorm((1 - alpha) / 2) # z-value\nleft &lt;- mean(sample) + s * z\nright &lt;- mean(sample) - s * z\nprint(right)\n\n[1] 0.6069685\n\nprint(left)\n\n[1] 0.4933495\n\n\nExercise: Modify that script to report whether the confidence interval captures the true mean. Use a loop structure (as in the script above) to generate 1000 sample means and report how many of them are within the theoretical confidence interval. Does this match the fraction you expect from the significance level? Try different significance levels and sample sizes and report what you discover.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distributions and their properties</span>"
    ]
  },
  {
    "objectID": "04-distributions.html#identifying-type-of-distribution-in-real-data",
    "href": "04-distributions.html#identifying-type-of-distribution-in-real-data",
    "title": "5  Distributions and their properties",
    "section": "5.6 Identifying type of distribution in real data",
    "text": "5.6 Identifying type of distribution in real data\nLet us consider the penguin data set again:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nA simple way to visualize a distribution is to plot a histogram: data are binned, and the height of the bin represents counts (or frequencies). Here are the histograms of distributions of flipper lengths of all the species of penguins separated by sex:\n\nggplot(penguins) +\n  aes(x = flipper_length_mm, color = sex, fill=sex) + geom_histogram(alpha = 0.5, position = \"identity\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nAnd here are the histograms of flipper lengths separated by species:\n\nggplot(penguins) +\n  aes(x = flipper_length_mm, color = species, fill=species) + geom_histogram(alpha = 0.5, position = \"identity\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nTo decide systematically which of these distributions are closer to a theoretical distribution is via a Quantile-Quantile (QQ) plot, which plots the quantile value from a sample against the quantiles from a given distribution with best-fit parameters. If the data were to follow the distribution closely, you should find all the points lying on the identity line. For example, here is how to compare a data set drawn from the normal random number generator with the normal distribution:\n\nlibrary(fitdistrplus)\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nLoading required package: survival\n\ntest_data &lt;- tibble(x = rnorm(n = 500, mean = 3, sd = 1.5))\n# example: find best-fitting Normal\nmy_normal &lt;- fitdistr(test_data$x, densfun = \"normal\")\n# note the slight discrepancies\nprint(my_normal)\n\n     mean         sd    \n  2.9313495   1.5976996 \n (0.0714513) (0.0505237)\n\nggplot(test_data, aes(sample = x)) +\n  stat_qq(distribution = qnorm, dparams = my_normal$estimate) +\n  stat_qq_line(distribution = qnorm, dparams = my_normal$estimate) +\n  geom_abline(intercept = 0, slope = 1, linetype = 2, col = \"red\") +\n  ggtitle(\"Q-Q plot assuming best-fitting Normal distribution\")\n\n\n\n\n\n\n\n\nNow let us assess the “normality” of the flipper length data separated by sex and separated by species:\n\ndataset &lt;- penguins %&gt;%  dplyr::filter(sex == 'female') %&gt;% drop_na() %&gt;%  dplyr::select(flipper_length_mm) \nmy_normal &lt;- fitdistr(x = as_vector(dataset), densfun = \"normal\")\n# note the slight discrepancies\nprint(my_normal)\n\n      mean           sd     \n  197.3636364    12.4628373 \n (  0.9702306) (  0.6860566)\n\nggplot(dataset, aes(sample = flipper_length_mm)) +\n  stat_qq(distribution = qnorm, dparams = my_normal$estimate) +\n  stat_qq_line(distribution = qnorm, dparams = my_normal$estimate) +\n  geom_abline(intercept = 0, slope = 1, linetype = 2, col = \"red\") +\n  ggtitle(\"Q-Q plot assuming best-fitting Normal distribution of flipper lenth for female penguins\")\n\n\n\n\n\n\n\ndataset &lt;- penguins %&gt;%  dplyr::filter(sex == 'male') %&gt;% drop_na() %&gt;%  dplyr::select(flipper_length_mm) \nmy_normal &lt;- fitdistr(x = as_vector(dataset), densfun = \"normal\")\n# note the slight discrepancies\nprint(my_normal)\n\n      mean           sd     \n  204.5059524    14.5045137 \n (  1.1190475) (  0.7912861)\n\nggplot(dataset, aes(sample = flipper_length_mm)) +\n  stat_qq(distribution = qnorm, dparams = my_normal$estimate) +\n  stat_qq_line(distribution = qnorm, dparams = my_normal$estimate) +\n  geom_abline(intercept = 0, slope = 1, linetype = 2, col = \"red\") +\n  ggtitle(\"Q-Q plot assuming best-fitting Normal distribution of flipper lenth for male penguins\")\n\n\n\n\n\n\n\n\nIn contrast with the simulated data, here the data points and the black line that attempts to capture them is quite different from the identity line (red). This means this distribution is for from normal, as can be seen from the histograms of the flipper lengths grouped by sex.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Distributions and their properties</span>"
    ]
  },
  {
    "objectID": "05-hypothesis.html",
    "href": "05-hypothesis.html",
    "title": "6  Hypothesis testing",
    "section": "",
    "text": "6.1 Test results vs. the truth\nA large number of scientific questions can be expressed as an hypothesis test—essentially a yes/no question, such as “are two samples drawn from distributions with the same mean?”, or “Is the frequency of an allele in a population greater than 0.1?”. Several tests have been developed, each with a specific type of question in mind. There is a dangerous tendency to view statistics as a collection of tests, and to practice it by plugging in your data set into the correct test, expecting that the test will spit out the correct decision. The purpose of this lesson is to demonstrate that using and interpreting statistical tests requires careful thinking to avoid serious errors.\nA statistical test begins by stating the null hypothesis, usually one that is expected, or that shows no effect: for example, that two samples come from a distribution with the same mean, or that a rare allele has frequency of less than 0.1. One may state the alternative hypothesis explicitly, although it’s usually the logical converse of the null, i.e., the two samples have different population means, or the allele has frequency greater than 0.1.\nAfter the hypothesis is stated, the data are collected and are used to test the hypothesis. By default, the null hypothesis is assumed to be true, and the test assesses whether the data provide sufficient evidence against the null hypothesis—in which case the null hypothesis is rejected. There is an adversarial relationship: either the data knock off the hypothesis, or else they fail to do so. Standard terminology reflects this somewhat counter-intuitive setup: rejecting the null hypothesis is called a positive test result, while not rejecting it is called a negative result.\nThe fundamental assumption of this process is that the truth value of the hypothesis is set prior to the collection of data. For example, if one could observe all of the genomes, the frequency of the allele would be known exactly, so this truth exists prior to the hypothesis testing. Because we typically can only observe a sample (and not the entire universe of data), we might end up erroneously rejecting the null hypothesis when it is in fact true, or not rejecting it when it is in fact false. The possible outcomes of a test can be organized in the table:\nThe values at the top describe the truth status of the hypothesis, while the decisions in the left column are the result of using data to test the hypothesis. Note: the words false and true in describing the test result do not refer to the hypothesis, but to whether the result is correct! For example, if the frequency of the allele were 0.09 but the test for the hypothesis that the frequency is less than 0.1 resulted in rejecting that hypothesis, that would be a false positive result (the null hypothesis is true but the test rejected it.)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "05-hypothesis.html#test-results-vs.-the-truth",
    "href": "05-hypothesis.html#test-results-vs.-the-truth",
    "title": "6  Hypothesis testing",
    "section": "",
    "text": "H0\nTrue\nFalse\n\n\n\n\nReject\nFalse Positive\nTrue Positive\n\n\nNot Reject\nTrue Negative\nFalse Negative",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "05-hypothesis.html#types-of-errors",
    "href": "05-hypothesis.html#types-of-errors",
    "title": "6  Hypothesis testing",
    "section": "6.2 Types of errors",
    "text": "6.2 Types of errors\nAs mentioned above, sometimes a hypothesis test makes the wrong decision, which is called an error. There are two different kinds of errors: rejecting a true null hypothesis, called a Type I error, and not rejecting a false null hypothesis, called a Type II error.\nExample: In the case above of testing for the same mean: if the samples are taken from distributions with the same mean, but the hypothesis is rejected, this is called a false positive (Type I error). If the samples come from distributions with different means, but the hypothesis is not rejected, this is called a false negative (Type II error.)\nAs a scientist, would you rather make a Type I error (make an erroneous discovery), or a Type II error (fail to make a discovery)?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "05-hypothesis.html#test-parameters-and-p-values",
    "href": "05-hypothesis.html#test-parameters-and-p-values",
    "title": "6  Hypothesis testing",
    "section": "6.3 Test parameters and p-values",
    "text": "6.3 Test parameters and p-values\nThe sensitivity of a test is the probability of obtaining the positive result, given a false hypothesis; and the specificity of a test is the probability of obtaining the negative result, given a true hypothesis. The Type I error rate is the probability of obtaining the positive result, given a true hypothesis (complementary to specificity), and the Type II error rate is the probability of obtaining the negative result, given a false hypothesis (complementary to sensitivity).\nAll four parameters (rates) of a binary test are summarized as follows: \\[\\text{Sen} = \\frac{TP}{TP+FN};  \\; \\text{Spec} = \\frac{TN}{TN+FP}\\] \\[\\text{FPR} = \\frac{FP}{TN+FP};  \\; \\text{FNR} = \\frac{FN}{TP+FN}\\] The notation TP, FP, etc. represents the frequency or count of true positives, false positives, etc., out of a large number of experiments with known truth status of the hypothesis.\nKnowledge of sensitivity and specificity determine the Type I and Type II error rates of a test since they are complementary events.\nOf course, it is desirable for a test to be both very sensitive (reject false null hypotheses, detect disease, convict guilty defendants) and very specific (not reject true null hypotheses, correctly identify healthy patients, acquit innocent defendants), but no test is perfect, and sometimes it makes the wrong decision. This is where statistical inference comes into play: given some information about these parameters, a statistician can calculate the error rate in making different decisions.\nThe probability that a given data set is produced from the model of the null hypothesis is called the p-value of a test. More precisely:\n\nFor a given data set \\(D\\) and a null hypothesis \\(H_0\\), the p-value is the probability of obtaining a result as far from expectation or farther than the observed data, given the null hypothesis.\n\nThe p-value is the most used, misused, and even abused quantity in statistics, so please think carefully about its definition. One reason this notion is frequently misused is because it is very tempting to conclude that the p-value is the probability of the null hypothesis being true, based on the data. That is not the case! The definition has the opposite direction of conditionality—we assume that the null hypothesis is true, and based on that calculate the probability of obtaining a pattern as extreme or more extreme than what observed in the data. There is no way (according to classical “frequentist” statistics) of assigning a probability to the truth of a hypothesis, because it is not the result of an experiment.\nTypically, one sets a critical threshold bounding the probability of making a Type I error in a test to a “small” number (often, \\(\\alpha = 0.05\\) or \\(0.01\\)), and calls the result of a test “significant” if the p-value is less than \\(\\alpha\\).\n\n\n\nFor example, consider samples of size \\(n\\) taken from two normal distributions (with unobserved means \\(\\mu_1\\), \\(\\mu_2\\)). We can generate the data:\n\ngenerate_samples &lt;- function(n, mu1, mu2){\n  return(data.frame(sample1 = rnorm(n = n, mean = mu1, sd = 1),\n               sample2 = rnorm(n = n, mean = mu2, sd = 1)))\n}\n\nmy_sample &lt;- generate_samples(1000, 1, 1.01)\n\nand use a Student’s t-test to probe whether the means differ:\n\n# two-tailed (diff in means = 0)\n# Student's (assumes equal variances)\n# (for Welch's t-test, var.equal = FALSE)\nt.test(my_sample$sample1, \n       my_sample$sample2,\n       var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  my_sample$sample1 and my_sample$sample2\nt = -0.60031, df = 1998, p-value = 0.5484\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.11347161  0.06028499\nsample estimates:\nmean of x mean of y \n0.9813927 1.0079860 \n\n\nExercise: Can you detect a “significant difference in means” (assuming \\(\\alpha = 0.05\\))? What if you take a much larger sample? What if the difference in means is more pronounced?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "05-hypothesis.html#multiple-comparisons",
    "href": "05-hypothesis.html#multiple-comparisons",
    "title": "6  Hypothesis testing",
    "section": "6.4 Multiple comparisons",
    "text": "6.4 Multiple comparisons\nWhat if we were to produce several samples? E.g., measure difference between males and females reflectance in birds at several locations? Suppose that in fact the reflectance is the same for male and female (\\(\\mu_1 = \\mu_2 = 1\\)), that for each location we capture and measure 10 males and 10 females, and that we repeat this across 2500 locations.\nFirst, let’s write a little function that returns the p-values for the t-test\n\nget_p_value_t_test &lt;- function(my_sample){\n  test_results &lt;- t.test(my_sample$sample1, \n                         my_sample$sample2, \n                         var.equal = TRUE)\n  return(test_results$p.value)\n}\n\nand now simulate the data:\n\npvalues &lt;- replicate(n = 2500, \n                     expr = get_p_value_t_test(generate_samples(10, 1, 1)))\n\nHow many times do we detect a “significant difference in reflectance” when setting \\(\\alpha = 0.05\\) (even though we know that males and females are sampled from the same distribution)?\n\nsum(pvalues &lt; 0.05)\n\n[1] 124\n\n\nYou should get a number of “significant” tests that is about \\(2500 \\cdot 0.05 = 125\\). In fact, the distribution of p-values when the data are sampled from the null hypothesis is approximately uniform:\n\nhist(pvalues)\n\n\n\n\n\n\n\n\nThis means that when you are performing multiple tests, some will turn out to find “significant” differences even when there are none. Again, this is better summarized by xkcd:\n\n\n\nExercise: what happens to the distribution of p-values if the means are quite different (e.g., \\(\\mu_1 = 1\\), \\(\\mu_2 = 0.9\\))?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "05-hypothesis.html#corrections-for-multiple-comparisons",
    "href": "05-hypothesis.html#corrections-for-multiple-comparisons",
    "title": "6  Hypothesis testing",
    "section": "6.5 Corrections for multiple comparisons",
    "text": "6.5 Corrections for multiple comparisons\nThe main approach to deal with the problem of multiple comparisons is to adjust the p-values. For example, in Bonferroni correction one consider as significant test results whose associated p-value is \\(\\leq \\alpha / n\\), where \\(n\\) is the number of tests performed (equivalently, redefine the p-values as \\(p' = \\min(p n, 1)\\). Clearly, this correction becomes overly conservative when the number of tests is large. For example, in biology:\n\nGene expression In a typical microarray experiment, we contrast the differential expression of tens of thousands of genes in treatment and control tissues.\nGWAS In Genomewide Association Studies we want to find SNPs associated with a given phenotype. It is common to test tens of thousands or even millions of SNPs for significant associations.\nIdentifying binding sites Identifying candidate binding sites for a transcriptional regulator requires scanning the whole genome, yielding tens of millions of tests.\n\nThe funniest example of this problem is the fMRI of the dead salmon: a dead salmon “was shown a series of photographs depicting human individuals in social situations with a specified emotional valence. The salmon was asked to determine what emotion the individual in the photo must have been experiencing.” The researchers showed that if multiple comparisons were not accounted for, one would detect a cluster of active voxels in the brain, with a cluster-level significance of p = 0.001.\nThe widespread use of GWAS and other techniques that are trying to find a needle in a haystack led to the development of many interesting techniques. Here an interesting account.\nAdjusting p-values in R:\n\noriginal_pvalues &lt;- c(0.01, 0.07, 0.1, 0.44)\np.adjust(original_pvalues, method = \"bonferroni\")\n\n[1] 0.04 0.28 0.40 1.00",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "05-hypothesis.html#two-problems-with-science",
    "href": "05-hypothesis.html#two-problems-with-science",
    "title": "6  Hypothesis testing",
    "section": "6.6 Two problems with science",
    "text": "6.6 Two problems with science\n\n6.6.1 Selective reporting\nWe have seen above that setting \\(\\alpha = 0.05\\) means that we are going to make false discoveries at this rate. In science, we prefer publishing positive results—negative results are difficult to publish and attract little attention. Suppose that 20 research groups around the world set out to test the same hypothesis, which is false. Then there is a good chance at least one group will reject the null hypothesis, and pursue publication for their “discovery”. The tendency to put negative studies in the files drawer and forget about them causes the so called publication bias (aka selective reporting): by favoring positive results over negative ones, we greatly increase the chance that our conclusions are wrong. Note that these would cause the results of the paper to be largely impossible to reproduce, and the reproducibility crisis in the sciences is partially due to selective reporting.\n\n\n6.6.2 P-hacking\nOne big violation of good experimental design is known as p-value “fishing” (or p-hacking): repeating the experiment, or increasing the sample size, until the p-value is below the desired threshold, and then stopping the experiment. Using such defective design dramatically lowers the likelihood that the result is a true positive. And of course there is actual fraud, or fudging of data, which contributes to some bogus results.\nAn insidious cousin of p-hacking was dubbed by Andrew Gelman “the garden of forking paths” in this paper. The issue arises in complex problems with multi-variable noisy datasets (aren’t all interesting ones like that?) Essentially, with many choices and degrees of freedom in a problem, it is easy to convince yourself that the choice you made (data cleaning, parameter combinations, etc.) is the correct one because it gives the strongest results. Without a clearly stated hypothesis, experimental design, and data processing details prior to data collection, this enchanted garden can lead even a well-intentioned researcher astray.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "05-hypothesis.html#readings",
    "href": "05-hypothesis.html#readings",
    "title": "6  Hypothesis testing",
    "section": "6.7 Readings",
    "text": "6.7 Readings\nGood readings on these and related issues:\n\nWhy Most Published Research Findings Are False\nDecline effect\nThe truth wears off\nThe Extent and Consequences of P-Hacking in Science\nA manifesto for reproducible science\nSpoiled Science",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "05-hypothesis.html#how-to-fool-yourself-with-p-hacking-and-possibly-get-fired",
    "href": "05-hypothesis.html#how-to-fool-yourself-with-p-hacking-and-possibly-get-fired",
    "title": "6  Hypothesis testing",
    "section": "6.8 How to fool yourself with p-hacking (and possibly get fired!)",
    "text": "6.8 How to fool yourself with p-hacking (and possibly get fired!)\nWe are going to try our hand at p-hacking, to show how easy it is to get fooled when you have a sufficiently large and complex data set. The file data/medals.csv contains the total number of medals won at the Olympic games (Summer or Winter) by country, sport and gender. We have a simple, and reasonable (?) hypothesis: because the amount of money available to Olympic teams is finite, whenever a country invests in the male team, this will be at the detriment of the female team. To test this hypothesis, we measure whether the number of medals won by a national female team in a year is negatively correlated with the number of medals won by the male team.\nLet’s read the data, and take a peak:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndt &lt;- read_csv(\"data/medals.csv\")\n\nRows: 6915 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): NOC, Sport\ndbl (3): Year, F, M\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndt\n\n# A tibble: 6,915 × 5\n   NOC    Year Sport         F     M\n   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 AFG    2008 Taekwondo     0     1\n 2 AFG    2012 Taekwondo     0     1\n 3 AHO    1988 Sailing       0     1\n 4 ALG    1984 Boxing        0     2\n 5 ALG    1992 Athletics     1     0\n 6 ALG    1992 Boxing        0     1\n 7 ALG    1996 Athletics     0     1\n 8 ALG    1996 Boxing        0     2\n 9 ALG    2000 Athletics     1     3\n10 ALG    2000 Boxing        0     1\n# ℹ 6,905 more rows\n\n\nFirst, let’s see whether our hypothesis works for the whole data:\n\ncor(dt$F, dt$M)\n\n[1] 0.1651691\n\n\nThe correlation is positive: more medals for the men tend to correspond to more medals for the women. This correlation is not very strong, but is it “significant”? We can run a correlation test:\n\ncor.test(dt$F, dt$M)\n\n\n    Pearson's product-moment correlation\n\ndata:  dt$F and dt$M\nt = 13.924, df = 6913, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1421521 0.1880075\nsample estimates:\n      cor \n0.1651691 \n\n\nIndeed! The confidence intervals are far from 0: the correlation is definitely positive. Should we give up? Of course not! Just as for the jelly beans, we can p-hack our way to glory by subsetting the data. We are going to test each discipline independently, and see whether we can get a robustly negative correlation for any discipline. Because we are serious scientists, we are going to consider only disciplines for which we have at least 50 data points, to avoid results that are due to small sample sizes. Let’s write a code:\n\ndt &lt;- dt %&gt;% group_by(Sport) %&gt;% mutate(sample_size = n()) %&gt;% ungroup()\ncorrelations &lt;- dt %&gt;% \n  filter(sample_size &gt;= 50) %&gt;% \n  group_by(Sport) %&gt;% \n  summarise(cor = cor(`M`, `F`), \n            pvalue = cor.test(`M`, `F`)$p.value) %&gt;% \n  ungroup() \n\nNow let’s see whether there are highly significant negative correlations:\n\nmy_results &lt;- correlations %&gt;% filter(pvalue &lt; 0.05, cor &lt; 0)\nmy_results\n\n# A tibble: 9 × 3\n  Sport                cor   pvalue\n  &lt;chr&gt;              &lt;dbl&gt;    &lt;dbl&gt;\n1 Basketball        -0.579 7.86e- 8\n2 Football          -0.796 6.75e-23\n3 Handball          -0.810 5.28e-16\n4 Hockey            -0.585 4.16e- 9\n5 Ice Hockey        -0.302 8.10e- 3\n6 Modern Pentathlon -0.561 3.57e- 8\n7 Volleyball        -0.545 2.18e- 6\n8 Water Polo        -0.688 3.41e-14\n9 Weightlifting     -0.138 2.33e- 2\n\n\nLet’s plot our results to convince ourselves that they are strong:\n\nggplot(dt %&gt;% inner_join(my_results)) + \n  aes(x = `M`, y = `F`) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  facet_wrap(~Sport, scales = \"free\")\n\n\n\n\n\n\n\n\nThat’s it! Should we rush to publish our results? Not quite: we have p-hacked our way to some highly significant results, but we did not correct for the number of tests we’ve made, and what we would do is to selectively reporting our strong results. In fact, we can do something very simple to convince ourselves that our results do not make much sense: just run the code again, but reporting significant positive correlations…\n\nmy_results &lt;- correlations %&gt;% filter(pvalue &lt; 0.05, cor &gt; 0)\nggplot(dt %&gt;% inner_join(my_results)) + \n  aes(x = `M`, y = `F`) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  facet_wrap(~Sport, scales = \"free\")\n\n\n\n\n\n\n\n\nYou can see that we’ve got about the same number of sports testing significant for positive correlation! Bonus question what about figure skating?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  }
]