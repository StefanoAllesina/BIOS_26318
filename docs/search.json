[{"path":"index.html","id":"organization-of-the-class","chapter":"Organization of the class","heading":"Organization of the class","text":"","code":""},{"path":"index.html","id":"learning-goals","chapter":"Organization of the class","heading":"Learning goals","text":"R tools visualizing analyzing data\nexploration tidyverse\ndplyr, tidyr readr data wrangling organization\nggplot2 visualization\nspecific packages functions statistical analysis\nexploration tidyversedplyr, tidyr readr data wrangling organizationggplot2 visualizationspecific packages functions statistical analysisTheory perform statistical inference\nassumptions different methods\nhypothesis testing\nestimation parameters\nmodel building selection\nassumptions different methodshypothesis testingestimation parametersmodel building selectionAvoiding common errors\n() use statistical method\nsneaky paradoxes\nphantom effects\n() use statistical methodsneaky paradoxesphantom effectsWork data\nanalyze data\nproduce graphics\nwrite report\npresent class\nanalyze dataproduce graphicswrite reportpresent class","code":""},{"path":"index.html","id":"approach","chapter":"Organization of the class","heading":"Approach","text":"Mix theory practiceApply ’re learning data","code":""},{"path":"index.html","id":"materials","chapter":"Organization of the class","heading":"Materials","text":"","code":""},{"path":"index.html","id":"week-0","chapter":"Organization of the class","heading":"Week 0","text":"R refresher 1","code":""},{"path":"index.html","id":"week-1","chapter":"Organization of the class","heading":"Week 1","text":"Using ggplot2 produce publication-ready figuresReview probability","code":""},{"path":"index.html","id":"week-2","chapter":"Organization of the class","heading":"Week 2","text":"Data wrangling tidyverseProbability distributions","code":""},{"path":"index.html","id":"week-3","chapter":"Organization of the class","heading":"Week 3","text":"Hypothesis testingLikelihood","code":""},{"path":"index.html","id":"week-4","chapter":"Organization of the class","heading":"Week 4","text":"Linear algebra refresherLinear models","code":""},{"path":"index.html","id":"week-5","chapter":"Organization of the class","heading":"Week 5","text":"Analysis varianceModeling time-series data","code":""},{"path":"index.html","id":"week-6","chapter":"Organization of the class","heading":"Week 6","text":"Generalized Linear ModelsModel selection","code":""},{"path":"index.html","id":"week-7","chapter":"Organization of the class","heading":"Week 7","text":"Principal Component AnalysisMultidimensional Scaling Clustering","code":""},{"path":"index.html","id":"week-8","chapter":"Organization of the class","heading":"Week 8","text":"Phylogenetic reconstructionMachine Learning cross validation","code":""},{"path":"index.html","id":"week-9","chapter":"Organization of the class","heading":"Week 9","text":"Thanksgiving break","code":""},{"path":"index.html","id":"week-10","chapter":"Organization of the class","heading":"Week 10","text":"Student presentations 1Student presentations 2","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Organization of the class","heading":"Acknowledgements","text":"Zach Miller TAing first iteration class, contributing materials comments; Julia Smith TAing second iteration; Cassie Manrique TAing year. Development class partially supported Burroughs Wellcome Fund program “Quantitative statistical thinking life sciences” (Stefano Allesina, PI).","code":""},{"path":"refresher.html","id":"refresher","chapter":"Lecture 1 Refresher","heading":"Lecture 1 Refresher","text":"","code":""},{"path":"refresher.html","id":"goal","chapter":"Lecture 1 Refresher","heading":"1.1 Goal","text":"Introduce statistical software R, show can used analyze biological data automated, replicable way. Showcase RStudio development environment, illustrate notion assignment, present main data structures available R. Show read write data, execute simple programs, modify stream execution program conditional branching looping. Introduce use packages user-defined functions.","code":""},{"path":"refresher.html","id":"motivation","chapter":"Lecture 1 Refresher","heading":"1.2 Motivation","text":"comes analyzing data, two competing paradigms. First, one use point--click software graphical user interface, Excel, perform calculations draw graphs; second, one write programs can run perform analysis data, generation tables statistics, production figures automatically.latter approach preferred, allows automation analysis, requires good documentation procedures, completely replicable.motivating examples:written code analyze data. receive collaborators new batch data. simple modifications code, can update results, tables figures automatically.written code analyze data. receive collaborators new batch data. simple modifications code, can update results, tables figures automatically.new student joins laboratory. new student can read code understand analysis without need lab mate showing procedure step--step.new student joins laboratory. new student can read code understand analysis without need lab mate showing procedure step--step.reviewers manuscript ask slightly alter analysis. Rather start , can modify lines code satisfy reviewers.reviewers manuscript ask slightly alter analysis. Rather start , can modify lines code satisfy reviewers.introduce R, can help write simple programs analyze data, perform statistical analysis, draw beautiful figures.","code":""},{"path":"refresher.html","id":"before-we-start","chapter":"Lecture 1 Refresher","heading":"1.3 Before we start","text":"follow tutorial, need install R RStudioInstall R: download install R page. Choose right architecture (Windows, Mac, Linux). possible, install latest release.Install RStudio: go page download “RStudio Desktop Open Source License.”Install R packages: launch RStudio. Click “Packages” bottom-right panel. Click “Install”: dialog window open. Type tidyverse field “Packages” click “Install.” might take minutes, ask download packages.","code":""},{"path":"refresher.html","id":"what-is-r","chapter":"Lecture 1 Refresher","heading":"1.4 What is R?","text":"R statistical software completely programmable. means one can write program (script) containing series commands analysis data, execute automatically. approach especially good makes analysis data well-documented, completely replicable.R free software: anyone can download source code, modify , improve . R community users vast active. particular, scientists enthusiastically embraced program, creating thousands packages perform specific types analysis, adding many new capabilities. can find list official packages (vetted R core developers) ; many available GitHub websites.main hurdle new users face approaching R based command line interface: launch R, simply open console character > signaling R ready accept input. write command press Enter, command interpreted R, result printed immediately command. example,little history: R modeled commercial statistical software S Robert Gentleman Ross Ihaka. project started 1992, first released 1994, first stable version appeared 2000. Today, R managed R Core Team.","code":"\n1 + 1## [1] 2"},{"path":"refresher.html","id":"rstudio","chapter":"Lecture 1 Refresher","heading":"1.5 RStudio","text":"introduction, ’re going use RStudio, Integrated Development Environment (IDE) R. main advantage environment look identical irrespective computer architecture (Linux, Windows, Mac). Also, RStudio makes writing code much easier automatically completing commands file names (simply type beginning name press Tab), allowing easily inspect data code.Typically, RStudio window contains four panels:Console panel containing instance R. tutorial, work mainly panel.Source code panel, can write program, save file pressing Ctrl + S execute pressing Ctrl + Shift + S.Environment panel lists variables created (later); another tab shows history commands typed.Plots panel shows plots drew. tabs allow access list packages loaded, help page commands (just type help(name_of_command) Console) packages.","code":""},{"path":"refresher.html","id":"how-to-write-a-simple-program","chapter":"Lecture 1 Refresher","heading":"1.6 How to write a simple program","text":"R program simply list commands, executed one . commands written text file (extension .R). R executes program, start beginning file proceed toward end file. Every time R encounters command, execute . Special commands can modify basic flow program , example, executing series commands condition met, repeating execution series commands multiple times.Note copy paste (type) code Console obtain exactly result. Writing program advantageous, however, analysis can automated, code shared researchers. Moreover, large code base, can recycle much code.start working console, start writing simple scripts.","code":""},{"path":"refresher.html","id":"the-most-basic-operation-assignment","chapter":"Lecture 1 Refresher","heading":"1.6.1 The most basic operation: assignment","text":"basic operation programming language assignment. R, assignment marked operator <- (can typed quickly using Alt -). type command R, executed, output printed Console. example:want save result operation, can assign variable. example:happened? wrote command containing assignment operator (<-). R evaluated right-hand-side command (sqrt(9)), stored result (3) newly created variable called x. Now can use x commands: every time command needs evaluated, program look value associated variable x, substitute . example:","code":"\nsqrt(9)## [1] 3\nx <- sqrt(9)\nx## [1] 3\nx * 2 ## [1] 6"},{"path":"refresher.html","id":"data-types","chapter":"Lecture 1 Refresher","heading":"1.6.2 Data types","text":"R provides different types data can used programs. variable x, calling class(x) prints type variable. basic data types :logical, taking two possible values: TRUE FALSEnumeric, storing real numbers (actually, approximations, computers limited memory thus store numbers like π, even 0.2)Real numbers can also specified using scientific notation:integer, storing whole numberscomplex, storing complex numbers (.e., real imaginary part)character, strings, characters textIn R, value type variable evaluated run-time. means can recycle names variables. handy, can make programs difficult read debug (.e., find mistakes). example:","code":"\nv <- TRUE\nclass(v)## [1] \"logical\"\nv <- 3.77\nclass(v)## [1] \"numeric\"\nv <- 6.022e23 # 6.022⋅10^23 (Avogadro's number)\nclass(v)## [1] \"numeric\"\nv <- 23L # the L signals that this should be stored as integer\nclass(v)## [1] \"integer\"\nv <- 23 + 5i # the i marks the imaginary part\nclass(v)## [1] \"complex\"\nv <- 'a string' # you can use single or double quotes\nclass(v)## [1] \"character\"\nx <- '2.3' # this is a string\nx## [1] \"2.3\"\nx <- 2.3 # this is numeric\nx## [1] 2.3"},{"path":"refresher.html","id":"operators-and-functions","chapter":"Lecture 1 Refresher","heading":"1.6.3 Operators and functions","text":"data type supports certain number operators functions. example, numeric variables can combined + (addition), - (subtraction), * (multiplication), / (division), ^ (**, exponentiation). possibly unfamiliar operator modulo (%%), calculating remainder integer division:meaning 5 %/% 3 (5 integer divided 3) 1 remainder 2The modulo operator useful determine whether number divisible another: y divisible x, y %% x 0.R provides many built-functions: functions name, followed round parentheses surrounding (possibly optional) function arguments. example, functions operate numeric variables:abs(x) absolute valuesqrt(x) square rootround(x, digits = 3) round x three decimal digitscos(x) cosine (also supported usual trigonometric functions)log(x) natural logarithm (use log10 base 10 logarithms)exp(x) calculating \\(e^x\\)Similarly, character variables set functions, :toupper(x) make uppercasenchar(x) count number characters stringpaste(x, y, sep = \"_\") concatenate strings, joining using separator _strsplit(x, \"_\") separate string using separator _Calling function meant certain data type another cause errors. sensible, can convert type another. example:sensible, can use comparison operators > (greater), < (lower), == (equals), != (differs), >= <=, returning logical value:Exercise:two equal signs (==) used check two values equal? happens use one = sign?Similarly, can concatenate several comparison logical variables using & (), | (), ! ():","code":"\n5 %% 3## [1] 2\nv <- \"2.13\"\nclass(v)## [1] \"character\"\n# if we call v * 2, we get an error.\n# to avoid it, we can convert v to numeric:\nas.numeric(v) * 2 ## [1] 4.26\n2 == sqrt(4)## [1] TRUE\n2 < sqrt(4)## [1] FALSE\n2 <= sqrt(4)## [1] TRUE\n(2 > 3) & (3 > 1)## [1] FALSE\n(2 > 3) | (3 > 1)## [1] TRUE"},{"path":"refresher.html","id":"getting-help","chapter":"Lecture 1 Refresher","heading":"1.6.4 Getting help","text":"want know function, type ?my_function_name console (e.g., ?abs). open help page one panels right. can accomplished calling help(abs). complex questions, check stackoverflow.","code":""},{"path":"refresher.html","id":"data-structures","chapter":"Lecture 1 Refresher","heading":"1.6.5 Data structures","text":"Besides simple types, R provides structured data types, meant collect organize multiple values.","code":""},{"path":"refresher.html","id":"vectors","chapter":"Lecture 1 Refresher","heading":"1.6.5.1 Vectors","text":"basic data structure R vector, ordered collection values type. Vectors can created concatenating different values function c() (“combine”):can access elements vector index: first element indexed 1, second 2, etc.NA stands “Available.” special values NaN (Number, e.g., 0/0), Inf (Infinity, e.g., 1/0), NULL (variable undefined). can test special values using .na(x), .infinite(x), .null(x), etc.Note R single number (string, logical) vector length 1 default. ’s type 3 console see [1] 3 output.can extract several elements (.e., create another vector), using colon (:) command, concatenating indices:can also use vector logical variables extract values vectors. example, suppose two vectors:want extract weights males.returns vector logical values, can use subset data:Given R born statistics, many statistical\nfunctions can perform vectors:can generate vectors sequential numbers using colon\ncommand:complex sequences, use seq:repeat value sequence several times, use rep:Exercise:Create vector containing even numbers 2 100 (inclusive) store variable z.Extract elements z divisible 12. many elements match criterion?sum elements z?equal \\(51 \\cdot 50\\)?product elements 5, 10 15 z?seq(2, 100, = 2) produce vector (1:50) * 2?happens type z ^ 2?","code":"\nx <- c(2, 3, 5, 27, 31, 13, 17, 19) \nx## [1]  2  3  5 27 31 13 17 19\nx[3]## [1] 5\nx[8]## [1] 19\nx[9] # what if the element does not exist?## [1] NA\nx[1:3]## [1] 2 3 5\nx[4:7]## [1] 27 31 13 17\nx[c(1,3,5)]## [1]  2  5 31\nsex <- c(\"M\", \"M\", \"F\", \"M\", \"F\") # sex of Drosophila\nweight <- c(0.230, 0.281, 0.228, 0.260, 0.231) # weight in mg\nsex == \"M\"## [1]  TRUE  TRUE FALSE  TRUE FALSE\nweight[sex == \"M\"]## [1] 0.230 0.281 0.260\nlength(x)## [1] 8\nmin(x)## [1] 2\nmax(x)## [1] 31\nsum(x) # sum all elements## [1] 117\nprod(x) # multiply all elements## [1] 105436890\nmedian(x) # median value## [1] 15\nmean(x) # arithmetic mean## [1] 14.625\nvar(x) # unbiased sample variance## [1] 119.4107\nmean(x ^ 2) - mean(x) ^ 2 # population variance## [1] 104.4844\nsummary(x) # print a summary##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    2.00    4.50   15.00   14.62   21.00   31.00\nx <- 1:10\nx##  [1]  1  2  3  4  5  6  7  8  9 10\nseq(from = 1, to = 5, by = 0.5)## [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nrep(\"abc\", 3)## [1] \"abc\" \"abc\" \"abc\"\nrep(c(1, 2, 3), 3)## [1] 1 2 3 1 2 3 1 2 3"},{"path":"refresher.html","id":"matrices","chapter":"Lecture 1 Refresher","heading":"1.6.5.2 Matrices","text":"matrix two-dimensional table values. case numeric values, can perform usual operations matrices (product, inverse, decomposition, etc.):determine dimensions matrix, use dim:Use indices access particular row/column matrix:functions use elements matrix:functions apply operation across given dimension (e.g., columns) matrix:","code":"\nA <- matrix(c(1, 2, 3, 4), 2, 2) # values, nrows, ncols\nA ##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\nA %*% A # matrix product##      [,1] [,2]\n## [1,]    7   15\n## [2,]   10   22\nsolve(A) # matrix inverse##      [,1] [,2]\n## [1,]   -2  1.5\n## [2,]    1 -0.5\nA %*% solve(A) # this should return the identity matrix##      [,1] [,2]\n## [1,]    1    0\n## [2,]    0    1\nB <- matrix(1, 3, 2) # you can fill the whole matrix with a single number (1)\nB##      [,1] [,2]\n## [1,]    1    1\n## [2,]    1    1\n## [3,]    1    1\nB %*% t(B) # transpose##      [,1] [,2] [,3]\n## [1,]    2    2    2\n## [2,]    2    2    2\n## [3,]    2    2    2\nZ <- matrix(1:9, 3, 3) # by default, matrices are filled by column\nZ##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\ndim(B)## [1] 3 2\ndim(B)[1]## [1] 3\nnrow(B) ## [1] 3\ndim(B)[2]## [1] 2\nncol(B)## [1] 2\nnrow(B)## [1] 3\nZ##      [,1] [,2] [,3]\n## [1,]    1    4    7\n## [2,]    2    5    8\n## [3,]    3    6    9\nZ[1, ] # first row## [1] 1 4 7\nZ[, 2] # second column## [1] 4 5 6\nZ [1:2, 2:3] # submatrix with coefficients in first two rows, and second and third column##      [,1] [,2]\n## [1,]    4    7\n## [2,]    5    8\nZ[c(1, 3), c(1, 3)] # indexing non-adjacent rows/columns##      [,1] [,2]\n## [1,]    1    7\n## [2,]    3    9\nsum(Z)## [1] 45\nmean(Z)## [1] 5\nrowSums(Z) # returns a vector of the sums of the values in each row## [1] 12 15 18\ncolSums(Z) # does the same for columns## [1]  6 15 24\nrowMeans(Z) # returns a vector of the means of the values in each row## [1] 4 5 6\ncolMeans(Z) # does the same for columns## [1] 2 5 8"},{"path":"refresher.html","id":"arrays","chapter":"Lecture 1 Refresher","heading":"1.6.5.3 Arrays","text":"need tables two dimensions, use arrays:can still determine dimensions using:access elements done matrices. One thing paying attention : R drops dimensions needed. , access “slice” 3-dimensional array:obtain matrix:can problematic, example, code expects array R turns data matrix (expect matrix find vector). avoid behavior, add drop = FALSE subsetting:","code":"\nM <- array(1:24, c(4, 3, 2))\nM ## , , 1\n## \n##      [,1] [,2] [,3]\n## [1,]    1    5    9\n## [2,]    2    6   10\n## [3,]    3    7   11\n## [4,]    4    8   12\n## \n## , , 2\n## \n##      [,1] [,2] [,3]\n## [1,]   13   17   21\n## [2,]   14   18   22\n## [3,]   15   19   23\n## [4,]   16   20   24\ndim(M)## [1] 4 3 2\nM[, , 1]##      [,1] [,2] [,3]\n## [1,]    1    5    9\n## [2,]    2    6   10\n## [3,]    3    7   11\n## [4,]    4    8   12\ndim(M[, , 1])## [1] 4 3\ndim(M[, , 1, drop = FALSE])## [1] 4 3 1"},{"path":"refresher.html","id":"lists","chapter":"Lecture 1 Refresher","heading":"1.6.5.4 Lists","text":"Vectors good element type (e.g., numbers, strings). Lists used want store elements different types, complex objects (e.g., vectors, matrices, even lists lists). element list can referenced either index, label:","code":"\nmylist <- list(Names = c(\"a\", \"b\", \"c\", \"d\"), Values = c(1, 2, 3))\nmylist## $Names\n## [1] \"a\" \"b\" \"c\" \"d\"\n## \n## $Values\n## [1] 1 2 3\nmylist[[1]] # access first element using index## [1] \"a\" \"b\" \"c\" \"d\"\nmylist[[2]] # access second element by index## [1] 1 2 3\nmylist$Names # access second element by label## [1] \"a\" \"b\" \"c\" \"d\"\nmylist[[\"Names\"]] # another way to access by label## [1] \"a\" \"b\" \"c\" \"d\"\nmylist[[\"Values\"]][3]  # access third element in second vector## [1] 3"},{"path":"refresher.html","id":"data-frames","chapter":"Lecture 1 Refresher","heading":"1.6.5.5 Data frames","text":"Data frames contain data organized like spreadsheet. columns (typically representing different measurements) can different types (e.g., column date measurement, another weight individual, volume cell, treatment sample), rows typically represent different samples.read spreadsheet file R, automatically stored data frame. difference matrix data frame matrix values type (e.g., numeric), data frame column can different type.typing data frame hand tedious, let’s use data set already available R:Exercise:average height cherry trees?average girth 75 ft tall?maximum height trees volume 15 35 ft\\(^3\\)?","code":"\ndata(trees) # girth, height and volume of cherry trees\nstr(trees) # structure of data frame## 'data.frame':    31 obs. of  3 variables:\n##  $ Girth : num  8.3 8.6 8.8 10.5 10.7 10.8 11 11 11.1 11.2 ...\n##  $ Height: num  70 65 63 72 81 83 66 75 80 75 ...\n##  $ Volume: num  10.3 10.3 10.2 16.4 18.8 19.7 15.6 18.2 22.6 19.9 ...\nncol(trees)## [1] 3\nnrow(trees)## [1] 31\nhead(trees) # print the first few rows##   Girth Height Volume\n## 1   8.3     70   10.3\n## 2   8.6     65   10.3\n## 3   8.8     63   10.2\n## 4  10.5     72   16.4\n## 5  10.7     81   18.8\n## 6  10.8     83   19.7\nsummary(trees) # Quickly get an overview of the data frame.##      Girth           Height       Volume     \n##  Min.   : 8.30   Min.   :63   Min.   :10.20  \n##  1st Qu.:11.05   1st Qu.:72   1st Qu.:19.40  \n##  Median :12.90   Median :76   Median :24.20  \n##  Mean   :13.25   Mean   :76   Mean   :30.17  \n##  3rd Qu.:15.25   3rd Qu.:80   3rd Qu.:37.30  \n##  Max.   :20.60   Max.   :87   Max.   :77.00\ntrees$Girth # select column by name##  [1]  8.3  8.6  8.8 10.5 10.7 10.8 11.0 11.0 11.1 11.2 11.3 11.4 11.4 11.7 12.0\n## [16] 12.9 12.9 13.3 13.7 13.8 14.0 14.2 14.5 16.0 16.3 17.3 17.5 17.9 18.0 18.0\n## [31] 20.6\ntrees$Height[1:5] # select column by name; return first five elements## [1] 70 65 63 72 81\ntrees[1:3, ] #select rows 1 through 3##   Girth Height Volume\n## 1   8.3     70   10.3\n## 2   8.6     65   10.3\n## 3   8.8     63   10.2\ntrees[1:3, ]$Volume # select rows 1 through 3; return column Volume## [1] 10.3 10.3 10.2\ntrees <- rbind(trees, c(13.25, 76, 30.17)) # add a row\ntrees_double <- cbind(trees, trees) # combine columns\ncolnames(trees) <- c(\"Circumference\", \"Height\", \"Volume\") # change column names"},{"path":"refresher.html","id":"reading-and-writing-data","chapter":"Lecture 1 Refresher","heading":"1.7 Reading and writing data","text":"cases, generate data R, import file. far, best option data comma separated value text file tab separated file. , can use function read.csv (read.table) import data. syntax functions follows:Note columns containing strings typically converted factors (categorical values, useful performing regressions). avoid behavior, can specify stringsAsFactors = FALSE calling function.Similarly, can save data frames using write.table write.csv. Suppose want save data frame MyDF:Let’s look example: Read file containing data 6th chromosome number Europeans (Data adapted Stanford HGDP SNP Genotyping Data John Novembre). example shows can read data directly internet!header = TRUE means want take first line header containing column names. big table?7 columns, 40k rows! Let’s see first :last :data contains number homozygotes (nA1A1, nA2A2) heterozygotes (nA1A2), 43,141 single nucleotide polymorphisms (SNPs) obtained sequencing European individuals:CHR chromosome (6 case)SNP identifier Single Nucleotide PolymorphismA1 One allelesA2 allelenA1A1 number individuals particular combination alleles.Exercise:many individuals sampled? Find maximum sum nA1A1 + nA1A2 + nA2A2. Note: can access columns index (e.g., ch6[,5]), name (e.g., ch6$nA1A1, also ch6[,\"nA1A1\"]).Try using function rowSums obtain result.many SNPs sampled individuals homozygotes (.e., A1A1 A2A2)?many SNPs, 99% sampled individuals homozygous?","code":"\nread.csv(\"MyFile.csv\") # read the file MyFile.csv\nread.csv(\"MyFile.csv\", header = TRUE) # the file has a header\nread.csv(\"MyFile.csv\", sep = ';') # specify the column separator\nread.csv(\"MyFile.csv\", skip = 5) # skip the first 5 lines\nwrite.csv(MyDF, \"MyFile.csv\") \nwrite.csv(MyDF, \"MyFile.csv\", append = TRUE) # append to the end of the file \nwrite.csv(MyDF, \"MyFile.csv\", row.names = TRUE) # include the row names\nwrite.csv(MyDF, \"MyFile.csv\", col.names = FALSE) # do not include column names\n# The actual URL is\n# https://github.com/StefanoAllesina/BSD-QBio4/raw/master/tutorials/basic_computing_1/data/H938_Euro_chr6.geno\nch6 <- read.table(\"https://tinyurl.com/y7vctq3v\", \n                  header = TRUE, stringsAsFactors = FALSE)\ndim(ch6)## [1] 43141     7\nhead(ch6)##   CHR        SNP A1 A2 nA1A1 nA1A2 nA2A2\n## 1   6  rs4959515  A  G     0    17   107\n## 2   6   rs719065  A  G     0    26    98\n## 3   6  rs6596790  C  T     0     4   119\n## 4   6  rs6596796  A  G     0    22   102\n## 5   6  rs1535053  G  A     5    39    80\n## 6   6 rs12660307  C  T     0     3   121\ntail(ch6)##       CHR        SNP A1 A2 nA1A1 nA1A2 nA2A2\n## 43136   6 rs10946282  C  T     0    16   108\n## 43137   6  rs3734763  C  T    19    56    48\n## 43138   6   rs960744  T  C    32    60    32\n## 43139   6  rs4428484  A  G     1    11   112\n## 43140   6  rs7775031  T  C    26    56    42\n## 43141   6 rs12213906  C  T     1    11   112"},{"path":"refresher.html","id":"conditional-branching","chapter":"Lecture 1 Refresher","heading":"1.8 Conditional branching","text":"Now turn writing actual programs Source code panel. start new R program, press Ctrl + Shift + N. open Untitled script. Save script pressing Ctrl + S: save conditional.R directory programming_skills/sandbox/. make sure ’re working directory script contained, menu top choose Session -> Set Working Directory -> Source File Location.Now type following script:execute script pressing Ctrl + Shift + S. see Hello World! 4 printed console.saw simple example, R executes program, starts top proceeds toward end file. Every time encounters command (example, print(x), printing value x console), executes .want certain block code executed certain condition met, can write conditional branching point. syntax follows:example, add lines script conditional.R, run :created conditional branching point, value my_message changes depending whether x even (thus remainder integer division 2 0), odd. Change line x <- 4 x <- 131 run .Exercise: ?","code":"\nprint(\"Hello world!\")\nx <- 4\nprint(x)if (condition is met){\n  # execute this block of code\n} else {\n  # execute this other block of code\n}\nprint(\"Hello world!\")\nx <- 4\nprint(x)\nif (x %% 2 == 0){\n  my_message <- paste(x, \"is even\")\n} else {\n  my_message <- paste(x, \"is odd\")\n}\nprint(my_message)\nx <- 36\nif (x > 20){\n  x <- sqrt(x)\n} else {\n  x <- x ^ 2\n}\nif (x > 7) {\n  print(x)\n} else if (x %% 2 == 1){\n  print(x + 1)\n}"},{"path":"refresher.html","id":"looping","chapter":"Lecture 1 Refresher","heading":"1.9 Looping","text":"Another way change flow program write loop. loop simply series commands repeated number times. example, want run analysis different data sets collected; want plot results contained set files; want test simulation number parameter sets; etc.R provides two ways loop blocks commands: loop, loop. Let’s start loop, used iterate vector (list): value vector, series commands run, shown following example, can type new script called forloop.R.code , variable takes value element myvec sequence. Inside block defined loop, can use variable perform operations.anatomy statement:loops used know want perform analysis using given set values (e.g., run files directory, samples data, sequences fasta file, etc.).loop used commands need repeated certain condition true, shown following example, can type script called whileloop.R:script performs exactly operations wrote loop . Note need update value , (using <- + 1), otherwise loop run forever (infinite loop—terminate click stop button top-right corner console). anatomy statement:can break loop using command break. example:Exercise: ? Try guess loop , create\nrun script confirm intuition.","code":"\nmyvec <- 1:10 # vector with numbers from 1 to 10\n\nfor (i in myvec) {\n  a <- i ^ 2\n  print(a)\n}for (variable in list_or_vector) {\n  execute these commands\n} # automatically moves to the next value\ni <- 1\n\nwhile (i <= 10) {\n  a <- i ^ 2\n  print(a)\n  i <- i + 1 \n}while (condition is met) {\n  execute these commands\n} # beware of infinite loops: remember to update the condition!\ni <- 1\n\nwhile (i <= 10) {\n  if (i > 5) {\n    break\n  }\n  a <- i ^ 2\n  print(a)\n  i <- i + 1\n}\nz <- seq(1, 1000, by = 3)\nfor (k in z) {\n  if (k %% 4 == 0) {\n     print(k)\n  }\n}\nz <- readline(prompt = \"Enter a number: \")\nz <- as.numeric(z)\nisthisspecial <- TRUE\ni <- 2\nwhile (i < z) {\n  if (z %% i == 0) {\n     isthisspecial <- FALSE\n     break\n  }\n  i <- i + 1\n}\nif (isthisspecial == TRUE) {\n  print(z)\n}"},{"path":"refresher.html","id":"useful-functions","chapter":"Lecture 1 Refresher","heading":"1.10 Useful Functions","text":"’s short list useful functions help write programs:range(x): minimum maximum vector xsort(x): sort vector xunique(x): remove duplicate entries vector xwhich(x == ): returns vector indices x value alist.files(\"path_to_directory\"): list files directory (current directory specified)table(x) build table frequenciesExercises: code ? snippet code, first try guess happen. , write script run confirm intuition.","code":"\nv <- c(1, 3, 5, 5, 3, 1, 2, 4, 6, 4, 2)\nv <- sort(unique(v))\nfor (i in v){\n  if (i > 2){\n    print(i)\n  }\n  if (i > 4){\n    break\n  }\n}\nx <- 1:100\nx <- x[which(x %% 7 == 0)]\nmy_amount <- 10\nwhile (my_amount > 0){\n  my_color <- NA\n  while(is.na(my_color)){\n    tmp <- readline(prompt=\"Do you want to bet on black or red? \")\n    tmp <- tolower(tmp)\n    if (tmp == \"black\") my_color <- \"black\"\n    if (tmp == \"red\") my_color <- \"red\"\n    if (is.na(my_color)) print(\"Please enter either red or black\")\n  }\n  my_bet <- NA\n  while(is.na(my_bet)){\n    tmp <- readline(prompt=\"How much do you want to bet? \")\n    tmp <- as.numeric(tmp)\n    if (is.numeric(tmp) == FALSE){\n      print(\"Please enter a number\")\n    } else {\n      if (tmp > my_amount){\n        print(\"You don't have enough money!\")\n      } else {\n        my_bet <- tmp\n        my_amount <- my_amount - tmp\n      }\n    }\n  }\n  lady_luck <- sample(c(\"red\", \"black\"), 1)\n  if (lady_luck == my_color){\n    my_amount <- my_amount + 2 * my_bet\n    print(paste(\"You won!! Now you have\", my_amount, \"gold doubloons\"))\n  } else {\n    print(paste(\"You lost!! Now you have\", my_amount, \"gold doubloons\"))\n  }\n}"},{"path":"refresher.html","id":"packages","chapter":"Lecture 1 Refresher","heading":"1.11 Packages","text":"R popular statistical computing software among biologists due highly specialized packages, often written biologists biologists. can contribute package ! RStudio support (goo.gl/harVqF) provides guidance start developing R packages Hadley Wickham’s free online book (r-pkgs..co.nz) make pro.can find highly specialized packages address research questions. suggestions finding appropriate package. Comprehensive R Archive Network (CRAN) offers several ways find specific packages task. can either browse packages (goo.gl/7oVyKC) short description select scientific field interest (goo.gl/0WdIcu) browse compilation packages related discipline.within R terminal RStudio can also call function RSiteSearch(\"KEYWORD\"), submits search query website search.r-project.org. website rseek.org casts even wider net, includes package names documentation also blogs mailing lists related R. research interests relate high-throughput genomic data, look packages provided Bioconductor (goo.gl/7dwQlq).","code":""},{"path":"refresher.html","id":"installing-a-package","chapter":"Lecture 1 Refresher","heading":"1.11.1 Installing a package","text":"install package typein Console, choose panel Packages click Install RStudio.","code":"\ninstall.packages(\"name_of_package\")"},{"path":"refresher.html","id":"loading-a-package","chapter":"Lecture 1 Refresher","heading":"1.11.2 Loading a package","text":"load package typeor call command script. want script automatically install package case ’s missing, use boilerplate:","code":"\nlibrary(name_of_package)\nif (!require(needed_package, character.only = TRUE, quietly = TRUE)) {\n    install.packages(needed_package)\n    library(needed_package, character.only = TRUE)\n}"},{"path":"refresher.html","id":"example","chapter":"Lecture 1 Refresher","heading":"1.11.3 Example","text":"example, say want access dataset bacteria, reports incidence H. influenzae Australian children. dataset contained package MASS.First, need load package:Now can load data:","code":"\nlibrary(MASS)\ndata(bacteria)\nbacteria[1:3,]##   y ap hilo week  ID     trt\n## 1 y  p   hi    0 X01 placebo\n## 2 y  p   hi    2 X01 placebo\n## 3 y  p   hi    4 X01 placebo"},{"path":"refresher.html","id":"random-numbers","chapter":"Lecture 1 Refresher","heading":"1.12 Random numbers","text":"perform randomization, simulation, typically need draw random numbers. R functions sample random numbers many different statistical distributions. example:sample set values, use sample:","code":"\nrunif(5) # sample 5 numbers from the uniform distribution between 0 and 1## [1] 0.30399040 0.03551637 0.87251430 0.55507836 0.24631152\nrunif(5, min = 1, max = 9) # set the limits of the uniform distribution## [1] 1.117397 6.379311 6.796461 5.129502 8.774611\nrnorm(3) # three values from standard normal## [1] -0.5473180  0.5401668 -0.5594304\nrnorm(3, mean = 5, sd = 4) # specify mean and standard deviation## [1]  8.493577  4.834868 -4.135287\nv <- c(\"a\", \"b\", \"c\", \"d\")\nsample(v, 2) # without replacement## [1] \"b\" \"c\"\nsample(v, 6, replace = TRUE) # with replacement## [1] \"d\" \"b\" \"a\" \"c\" \"a\" \"a\"\nsample(v) # simply shuffle the elements## [1] \"c\" \"a\" \"d\" \"b\""},{"path":"refresher.html","id":"writing-functions","chapter":"Lecture 1 Refresher","heading":"1.13 Writing functions","text":"R community provides 7,000 packages. Still, sometimes isn’t already made function capable need. cases, can write functions. fact, generally good idea always divide analysis functions, write small “master” program calls functions performs analysis. way, code much legible, able recycle functions projects.function R form:examples:can set default value arguments: specified user, function use defaults:return value optional:can return one object. need return multiple values, organize vector/matrix/list return .","code":"\nmy_function_name <- function(optional, arguments, separated, by_commas){\n  # Body of the function\n  # ...\n  # \n  return(return_value) # this is optional\n}\nsum_two_numbers <- function(a, b){\n  apb <- a + b  \n  return(apb)\n}\nsum_two_numbers(5, 7.2)## [1] 12.2\nsum_two_numbers <- function(a = 1, b = 2){\n  apb <- a + b  \n  return(apb)\n}\nsum_two_numbers()## [1] 3\nsum_two_numbers(3)## [1] 5\nsum_two_numbers(b = 9)## [1] 10\nmy_factorial <- function(a = 6){\n  if (as.integer(a) != a) {\n    print(\"Please enter an integer!\")\n  } else {\n    tmp <- 1\n    for (i in 2:a){\n      tmp <- tmp * i\n    }\n    print(paste(a, \"! = \", tmp, sep = \"\"))\n  }\n}\nmy_factorial()## [1] \"6! = 720\"\nmy_factorial(10)## [1] \"10! = 3628800\"\norder_two_numbers <- function(a, b){\n  if (a > b) return(c(a, b)) #nothing after the first return is executed\n  return(c(b,a))\n}\n\norder_two_numbers(runif(1), runif(1))## [1] 0.8900636 0.3339555"},{"path":"refresher.html","id":"organizing-and-running-code","chapter":"Lecture 1 Refresher","heading":"1.14 Organizing and running code","text":"class, write lot code, increasing complexity. ensure programs well-organized, easy understand, easy debug.Take problem, divide basic building blocks. block function.Write code building block separately, test thoroughly.Extensively document code, can understand , , .Combine building blocks master program.example, let’s write code takes data Chromosome 6 seen , tries identify SNPs deviate Hardy-Weinberg equilibrium. Remember infinite population, mating random, selection mutations, proportion people carrying alleles \\(A1A1\\) approximately \\(p_{11} = p^2\\) (\\(p\\) frequency first allele population \\(p = p_{11} + \\frac{1}{2} p_{12}\\)), carrying \\(A1A2\\) \\(p_{12} = 2 p q\\) (\\(q = 1-p\\)) finally carrying \\(A2A2\\) \\(p_{22} = q^2\\). called Hardy-Weinberg equilibrium.want test number different SNPs. First, write function takes input data given SNP, computes probability \\(p\\) carrying first allele.Now can test function:allele conformed Hardy-Weinberg, find approximately \\(p^2 \\cdot n\\) people \\(A1A1\\), \\(n\\) number people sampled. Let’s see whether assumptions met data:test :Pretty good! SNP seems close theoretical expectation.Let’s try another oneBecause many SNPs, surely find comply expectation. example:find largest deviations, can compute statistic:\\[\n\\sum_i \\frac{(e_i - o_i)^2}{e_i}\n\\]\ngenetics, called \\(\\chi^2\\) statistics, data follow assumptions, quantities follow \\(\\chi^2\\) distribution.Now let’s compute statistic SNPs:find ones largest discrepancy, runThis example showed seemingly difficult problem can decomposed smaller problems easier solve.","code":"\ncompute_probabilities_HW <- function(my_data, my_SNP = \"rs1535053\"){\n  # Take a SNP and compute the probabilities\n  # p = frequency of first allele\n  # q = frequency of second allele (1 - p)\n  # p11 = proportion homozygous first allele\n  # p12 = proportion heterozygous\n  # p22 = proportion homozygous second allele\n  my_SNP_data <- my_data[my_data$\"SNP\" == my_SNP,]\n  AA <- my_SNP_data$nA1A1\n  AB <- my_SNP_data$nA1A2\n  BB <- my_SNP_data$nA2A2\n  tot_observations <- AA + AB + BB\n  p11 <- AA / tot_observations\n  p12 <- AB / tot_observations\n  p22 <- BB / tot_observations\n  p <- p11 + p12 / 2\n  q <- 1 - p\n  return(list(SNP = my_SNP,\n              p11 = p11,\n              p12 = p12,\n              p22 = p22,\n              p = p,\n              q = q,\n              tot = tot_observations,\n              AA = AA,\n              AB = AB,\n              BB = BB))\n}\ncompute_probabilities_HW(ch6)## $SNP\n## [1] \"rs1535053\"\n## \n## $p11\n## [1] 0.04032258\n## \n## $p12\n## [1] 0.3145161\n## \n## $p22\n## [1] 0.6451613\n## \n## $p\n## [1] 0.1975806\n## \n## $q\n## [1] 0.8024194\n## \n## $tot\n## [1] 124\n## \n## $AA\n## [1] 5\n## \n## $AB\n## [1] 39\n## \n## $BB\n## [1] 80\nobserved_vs_expected_HW <- function(SNP_data){\n  # compute expectations under Hardy-Weinberg equilibrium\n  # organize expected and observed in a table\n  observed <- c(\"AA\" = SNP_data$AA, \"AB\" = SNP_data$AB, \"BB\" = SNP_data$BB)\n  expected <- c(\"AA\" = SNP_data$p^2 * SNP_data$tot, \n                \"AB\" = 2 * SNP_data$p * SNP_data$q * SNP_data$tot, \n                \"BB\" = SNP_data$q^2 * SNP_data$tot)\n  return(rbind(observed, expected))\n}\nmy_SNP_data <- compute_probabilities_HW(ch6)\nobserved_vs_expected_HW(my_SNP_data)##                AA       AB       BB\n## observed 5.000000 39.00000 80.00000\n## expected 4.840726 39.31855 79.84073\nobserved_vs_expected_HW(compute_probabilities_HW(ch6, \"rs1316662\"))##                AA       AB       BB\n## observed 26.00000 62.00000 36.00000\n## expected 26.20161 61.59677 36.20161\nmy_SNP_data <- compute_probabilities_HW(ch6, \"rs6596835\")\nobserved_vs_expected_HW(my_SNP_data)##                 AA      AB      BB\n## observed 17.000000 24.0000 82.0000\n## expected  6.837398 44.3252 71.8374\ncompute_chi_sq_stat <- function(my_obs_vs_expected){\n  observed <- my_obs_vs_expected[\"observed\",]\n  expected <- my_obs_vs_expected[\"expected\",]\n  return(sum((expected - observed)^2 / expected))\n}\n# because this might take a while, we're going to only analyze the first 1000 SNPs\nall_SNPs <- ch6$SNP[1:1000]\nresults <- data.frame(SNP = all_SNPs, ChiSq = 0)\nfor (i in 1:nrow(results)){\n  results[i, 2] <- compute_chi_sq_stat(observed_vs_expected_HW(compute_probabilities_HW(ch6, results[i, 1])))\n}\nresults <- results[order(results$ChiSq, decreasing = TRUE),]\nhead(results)##           SNP     ChiSq\n## 10  rs2281351 53.993853\n## 221 rs1933650 27.724832\n## 36  rs6596835 25.862675\n## 681  rs689035  9.802277\n## 178 rs6930805  9.491511\n## 179 rs1737539  9.491511"},{"path":"refresher.html","id":"documenting-the-code-using-knitr","chapter":"Lecture 1 Refresher","heading":"1.15 Documenting the code using knitr","text":"Let us change traditional attitude construction programs: Instead imagining main task instruct computer , let us concentrate rather explaining humans want computer . Donald E. Knuth, Literate Programming, 1984When experiments, typically keep track everything laboratory notebook, writing manuscript, responding queries, can go back documentation find exactly , , possibly . true computational work.RStudio makes easy build computational laboratory notebook. First, create new R Markdown file (choose File -> New File -> R Markdown menu).gist write text file (.Rmd). file read interpreter transforms .html .pdf file, even Word document. can use special syntax render text different ways. example, typeThe important feature R Markdown, however, can include blocks code, interpreted executed R. can therefore combine effectively code description .example, includingwill becomeIf don’t want run R code, just display , use {r, eval = FALSE}; want show output code, use {r, echo = FALSE}.can include plots, tables, even render equations using LaTeX. summary, exploring data writing methods paper, give R Markdown try!can find inspiration notes class: written R Markdown.","code":"***********\n\n*Test* **Test2**\n\n# Very large header\n\n## Large header\n\n### Smaller header\n\n## Unordered lists\n\n* First\n* Second\n    + Second 1\n    + Second 2\n\n1. This is\n2. A numbered list\n\nYou can insert `inline code`\n\n-----------```r\nprint(\"hello world!\")  \n```\nprint(\"hello world!\")  ## [1] \"hello world!\""},{"path":"refresher.html","id":"resources","chapter":"Lecture 1 Refresher","heading":"1.16 Resources","text":"many excellent books tutorials can read become proficient programmer R. example:Intro RAdvanced RDataCampComputerWorldR Style guideR Data ScienceRStudio Cheat SheetBase R Cheat SheetAdvanced R Cheat SheetX Y minutesIntro Data WranglingR Boot Camp","code":""},{"path":"visualizing-data-using-ggplot2.html","id":"visualizing-data-using-ggplot2","chapter":"Lecture 2 Visualizing data using ggplot2","heading":"Lecture 2 Visualizing data using ggplot2","text":"","code":""},{"path":"visualizing-data-using-ggplot2.html","id":"goal-1","chapter":"Lecture 2 Visualizing data using ggplot2","heading":"2.1 Goal","text":"Introduce package ggplot2, part tidyverse bundle. Learn use ggplot2 produce publication-quality figures. Discuss philosophical underpinnings “Grammar Graphics,” showcase ggplot2 syntax, produce examples different types graphs. Learn change colors, legends, scales. Visualize histograms, barplots, scatterplots, etc.","code":""},{"path":"visualizing-data-using-ggplot2.html","id":"introduction-to-the-grammar-of-graphics","chapter":"Lecture 2 Visualizing data using ggplot2","heading":"2.2 Introduction to the Grammar of Graphics","text":"salient feature scientific graphs clarity. figure make crystal-clear ) plotted; b) axes; c) colors, shapes, sizes represent; d) message figure wants convey. figure accompanied (sometimes long) caption, details can explained , main message clear glancing figure (often, figures first thing editors referees look ).Many scientific publications contain poor graphics: labels missing, scales unintelligible, explanation graphical elements. Moreover, color graphs impossible understand printed black white, difficult discern color-blind people.Given effort put science, want ensure well presented accessible. investment master plotting software rewarded pleasing graphics convey clear message.section, introduce ggplot2, plotting package R package developed Hadley Wickham contributed many important packages R (included tidyverse bundle ’re going use reminder class). Unlike many plotting systems, ggplot2 deeply rooted “philosophical” vision. goal conceive grammar graphical representation data. Leland Wilkinson collaborators proposed Grammar Graphics. follows idea well-formed sentence composed subject, predicate, object. Grammar Graphics likewise aims describing well-formed graph grammar captures wide range statistical scientific graphics. might clear example – Take simple two-dimensional scatterplot. can describe ? :Data data want plot.Data data want plot.Mapping part data associated particular visual feature? example: column associated x-axis? y-axis? column corresponds shape color points? ggplot2 lingo, called aesthetic mappings (aes).Mapping part data associated particular visual feature? example: column associated x-axis? y-axis? column corresponds shape color points? ggplot2 lingo, called aesthetic mappings (aes).Geometry want draw points? Lines? ggplot2 speak geometries (geom).Geometry want draw points? Lines? ggplot2 speak geometries (geom).Scale want sizes shapes points scale according value? Linearly? Logarithmically? palette\ncolors want use?Scale want sizes shapes points scale according value? Linearly? Logarithmically? palette\ncolors want use?Coordinate need choose coordinate system (e.g., Cartesian, polar).Coordinate need choose coordinate system (e.g., Cartesian, polar).Faceting want produce different panels, partitioning data according one () variables?Faceting want produce different panels, partitioning data according one () variables?basic grammar can extended adding statistical transformations data (e.g., regression, smoothing), multiple layers, adjustment position (e.g., stack bars instead plotting side--side), annotations, .Exactly like grammar natural language, can easily change meaning “sentence” adding removing parts. Also, easy completely change type geometry moving say histogram boxplot violin plot, types plots meant describe one-dimensional distributions. Similarly, can go points lines, changing one “word” code. Finally, look feel graphs controlled theming system, separating content presentation.","code":""},{"path":"visualizing-data-using-ggplot2.html","id":"basic-ggplot2","chapter":"Lecture 2 Visualizing data using ggplot2","heading":"2.3 Basic ggplot2","text":"ggplot2 ships simplified graphing function, called qplot. introduction going use , concentrate instead function ggplot, gives complete control plotting. First, need load package:explore features ggplot2, going use data set detailing total number COVID cases deaths US counties. data provided New York Times.going work date, county, state, cases deaths.Let’s select Illnois, take counties 10k cases (less crowded graph):particularity ggplot2 accepts exclusively data organized tables (data.frame tibble object—tibbles later). Thus, data needs converted data frame format plotting.","code":"\nlibrary(tidyverse)\n# read the data\n# original URL https://github.com/nytimes/covid-19-data/raw/master/live/us-counties.csv\ndt <- read_csv(\"https://rb.gy/zr65gg\")## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   date = col_date(format = \"\"),\n##   county = col_character(),\n##   state = col_character(),\n##   fips = col_character(),\n##   cases = col_double(),\n##   deaths = col_double(),\n##   confirmed_cases = col_double(),\n##   confirmed_deaths = col_double(),\n##   probable_cases = col_double(),\n##   probable_deaths = col_double()\n## )\nhead(dt)## # A tibble: 6 × 10\n##   date       county  state   fips  cases deaths confirmed_cases confirmed_deaths\n##   <date>     <chr>   <chr>   <chr> <dbl>  <dbl>           <dbl>            <dbl>\n## 1 2021-11-10 Autauga Alabama 01001 10350    152            8440              134\n## 2 2021-11-10 Baldwin Alabama 01003 37737    577           26477              393\n## 3 2021-11-10 Barbour Alabama 01005  3620     79            2048               57\n## 4 2021-11-10 Bibb    Alabama 01007  4301     92            3167               61\n## 5 2021-11-10 Blount  Alabama 01009 10548    187            7928              153\n## 6 2021-11-10 Bullock Alabama 01011  1520     44            1330               35\n## # … with 2 more variables: probable_cases <dbl>, probable_deaths <dbl>\ndti <- dt[(dt$state == \"Illinois\") & (dt$cases > 10^4), ]"},{"path":"visualizing-data-using-ggplot2.html","id":"building-a-well-formed-graph","chapter":"Lecture 2 Visualizing data using ggplot2","heading":"2.4 Building a well-formed graph","text":"first plot, ’re going produce barplot detailing many cases reported County:can see, nothing drawn: need specify like associate x axis, y axis, etc. (.e., want set aesthetic mappings). barplot typically classes x axis, y axis reports counts class.Note concatenate pieces “sentence” using + sign! ’ve got aestethic mappings figured , still graph… need specify geometry, .e., type graph want produce. case, barplot height bars specified y value:difficult see labels, let’s swap axes:graph shows , naturally, vast majority cases reported Cook county. written “well-formed sentence,” composed data + mapping + geometry, sufficient produce graph. can add “adjectives” “adverbs” graph, make clearer:","code":"\nggplot(data = dti)\nggplot(data = dti) + aes(x = county, y = cases)\nggplot(data = dti) + aes(x = county, y = cases) + geom_col()\nggplot(data = dti) + \n  aes(x = county, y = cases) + \n  geom_col() + \n  coord_flip()\nggplot(data = dti) + \n  aes(x = reorder(county, cases), y = cases) + # order labels according to cases\n  geom_col() +\n  xlab(\"Number of COVID cases reported\") + # x label\n  ylab(\"Illinois County\") + # y label\n  scale_y_log10() + # transform the counts to logs\n  coord_flip()+\n  ggtitle(dti$date[1]) # main title (use current date)"},{"path":"visualizing-data-using-ggplot2.html","id":"scatterplots","chapter":"Lecture 2 Visualizing data using ggplot2","heading":"2.5 Scatterplots","text":"Using ggplot2, one can produce many types graphs. package works well 2D graphs (3D rendered two dimensions), lack capabilities draw proper 3D graphs, networks.main feature ggplot2 can tinker graph fairly easily, common grammar. don’t settle certain presentation data ’re ready, easy switch one type graph another.example, let’s plot number cases vs. number deaths:Showing number daily cases number daily deaths highly correlated (stronger correlation plot past cases vs. current deaths).","code":"\n# you can store the graph in a variable\npl <- ggplot(data = dti)\npl <- pl + aes(x = cases, y = deaths) # for a scatter plot, we need two aes mappings!\npl <- pl + geom_point() # draw points in a scatterplot\npl <- pl + scale_x_sqrt() + scale_y_sqrt() # transform axes\npl # or show(pl)"},{"path":"visualizing-data-using-ggplot2.html","id":"histograms-density-and-boxplots","chapter":"Lecture 2 Visualizing data using ggplot2","heading":"2.6 Histograms, density and boxplots","text":"nice see distribution ratio deaths/cases. , can produce histogram:can control width bins specifying:Let’s see whether histograms differ Illinois Indiana:plot histogram side side, useSimilarly, can approximate histogram using density plot, interpolates bin height create smooth distribution:see graph better, let’s make coloring semi-transparent:Showing similar distribution death rate two states. type comparison, ideal graph show maybe box-plot violin plot:boxplot shows median (horizontal bar) well inter-quartile range (box size goes 25th 75th percentile), well typical range data (whiskers). dots represent “outliers.” show full distribution, can use violin plot:Note ’re producing “similar” plots (e.g., histogram vs. density, box vs. violin, plot sharing aesthetic mappings) changing single word, changed structure graph considerably!","code":"\npl <- ggplot(data = dti)\npl <- pl + aes(x = deaths / cases)  \npl + geom_histogram(binwidth = 0.0025) \npl + geom_histogram(bins = 30) # specify the number of bins\npl + geom_histogram(binwidth = 0.001) # specify the bin width\nggplot(data = dt[dt$state %in% c(\"Illinois\", \"Indiana\"),]) + \n  aes(x = deaths / cases, fill = state) + # fill the bar colors by state\n  geom_histogram(bins = 30)\nggplot(data = dt[dt$state %in% c(\"Illinois\", \"Indiana\"),]) + \n  aes(x = deaths / cases, fill = state) + # fill the bar colors by state\n  geom_histogram(position = \"dodge\", bins = 30)\nggplot(data = dt[dt$state %in% c(\"Illinois\", \"Indiana\"),]) + \n  aes(x = deaths / cases, fill = state) + # fill by state\n  geom_density()\nggplot(data = dt[dt$state %in% c(\"Illinois\", \"Indiana\"),]) + \n  aes(x = deaths / cases, fill = state) + # fill by state\n  geom_density(alpha = 0.5)\nggplot(data = dt[dt$state %in% c(\"Illinois\", \"Indiana\"),]) + \n  aes(x = state, y = deaths / cases, fill = state) + # we need both x and y\n  geom_boxplot()\nggplot(data = dt[dt$state %in% c(\"Illinois\", \"Indiana\"),]) + \n  aes(x = state, y = deaths / cases, fill = state) + # we need both x and y\n  geom_violin(draw_quantiles = 0.5)"},{"path":"visualizing-data-using-ggplot2.html","id":"scales","chapter":"Lecture 2 Visualizing data using ggplot2","heading":"2.7 Scales","text":"can use scales determine aesthetic mappings displayed. example, set x axis logarithmic scale, can choose colors, shapes sizes used. ggplot2 uses two types scales: continuous scales used continuos variables (e.g., real numbers); discrete scales variables can take certain number values (e.g., colors, shapes, sizes).example, let’s plot deaths vs. cases dti data set:can change scale x axis calling:Similarly, can change use colors, points, etc.","code":"\npl <- ggplot(data = dti) + \n  aes(x = cases, y = deaths, colour = log(deaths)) +\n    geom_point() \npl\npl + scale_x_log10() + scale_y_log10() # log-log plot\npl + scale_x_sqrt() # sqrt of number of cases\npl + scale_x_reverse() # from large to small"},{"path":"visualizing-data-using-ggplot2.html","id":"list-of-aesthetic-mappings","chapter":"Lecture 2 Visualizing data using ggplot2","heading":"2.8 List of aesthetic mappings","text":"’ve seen aesthetic mappings. ’s list main aes:x use x axisy use y axiscolor color points linesfill color shapes (e.g., boxes, bars, etc.)size size points, lines, etc.shape shape pointsalpha level transparency objectlinetype type line (e.g., solid, dashed, etc.)","code":"\n# a more complex example\nggplot(data = dt) + \n  aes(x = cases, y = deaths, \n          color = state) +\n  geom_point() + \n  scale_x_log10() + # note that the points with 0 cases or deaths will not work\n  scale_y_log10() +\n  theme(legend.position = \"bottom\")"},{"path":"visualizing-data-using-ggplot2.html","id":"list-of-geometries","chapter":"Lecture 2 Visualizing data using ggplot2","heading":"2.9 List of geometries","text":"many geometries; useful ones:Lines: geom_abline (line given slope intercept); geom_hline, geom_vline (horizontal, vertical line); geom_line (connect observation scatterplot).Bars: geom_bar (bar height count/sum); geom_col (bar heigts provided data).Boxes: geom_boxplot.Distributions: geom_violin (like boxplots, showing density distribution); geom_density (density 1D distribution), geom_density2d (density bivariate distribution); geom_histogram, geom_bin2d (histograms).Text: geom_text.Smoothing function: geom_smooth (interpolates points scatterplot).Error bars: geom_errorbar.Maps: geom_map (polygons reference map).","code":""},{"path":"visualizing-data-using-ggplot2.html","id":"list-of-scales","chapter":"Lecture 2 Visualizing data using ggplot2","heading":"2.10 List of scales","text":"also many scales. :xlab, ylab, xlim, ylim control labels ranges axes.scale_alpha transparency points/shapes.scale_color (many options) colors points lines.scale_fill (many options) colors boxes, bars shapes.scale_shape shape points.scale_linetype type lines.scale_size size points lines.scale_x, scale_y (many options) transformations axes.","code":""},{"path":"visualizing-data-using-ggplot2.html","id":"themes","chapter":"Lecture 2 Visualizing data using ggplot2","heading":"2.11 Themes","text":"Themes allow manipulate look feel graph just one command. package ggthemes extends themes collection ggplot2 considerably. example:","code":"\n# to install, type install.packages(\"ggthemes\") in the console\nlibrary(ggthemes)\npl <- ggplot(data = dti) + aes(x = cases, y = deaths) +\n    geom_point() + scale_x_log10() + scale_y_log10()\npl + theme_bw() # white background\npl + theme_economist() # like in the magazine \"The Economist\"\npl + theme_wsj() # like \"The Wall Street Journal\""},{"path":"visualizing-data-using-ggplot2.html","id":"faceting","chapter":"Lecture 2 Visualizing data using ggplot2","heading":"2.12 Faceting","text":"many cases, like produce multi-panel graph, panel shows data certain combination parameters. ggplot2 called faceting: command facet_grid used want produce grid panels, panels row (column) axes-ranges common; facet_wrap used different panels necessarily axes-ranges common.example:Let’s add line separating showing best-fit line:Make ranges x y axes equal, add 1:1 line:","code":"\npl <- ggplot(data = dt[dt$state %in% c(\"Illinois\", \"Missouri\", \"Wisconsin\", \"Indiana\"), ]) + \n  aes(x = cases, y = deaths, colour = state) + geom_point() + scale_x_log10() + scale_y_log10()\npl <- pl + facet_wrap(~state)\npl\npl <- pl + geom_smooth()\npl## `geom_smooth()` using method = 'loess' and formula 'y ~ x'\npl <- pl + coord_equal() + geom_abline(slope = 1, intercept = 0)\npl## `geom_smooth()` using method = 'loess' and formula 'y ~ x'"},{"path":"visualizing-data-using-ggplot2.html","id":"setting-features","chapter":"Lecture 2 Visualizing data using ggplot2","heading":"2.13 Setting features","text":"Often, want simply set feature (e.g., color points, shape), rather using display information (.e., mapping aestethic). cases, simply declare feature outside aes:","code":"\npl <- ggplot(data = dt) + \n  aes(x = cases, y = deaths) + \n  scale_x_log10() + \n  scale_y_log10()\npl + geom_point()\npl + geom_point(colour = \"red\")\npl + geom_point(shape = 3)\npl + geom_point(alpha = 0.5)"},{"path":"visualizing-data-using-ggplot2.html","id":"saving-graphs","chapter":"Lecture 2 Visualizing data using ggplot2","heading":"2.14 Saving graphs","text":"can either save graphs done normally R:use function ggsave","code":"\n# save to pdf format\npdf(\"my_output.pdf\", width = 6, height = 4)\nprint(my_plot)\ndev.off()\n# save to svg format\nsvg(\"my_output.svg\", width = 6, height = 4)\nprint(my_plot)\ndev.off()\n# save current graph\nggsave(\"my_output.pdf\")\n# save a graph stored in ggplot object\nggsave(plot = my_plot, filename = \"my_output.svg\")"},{"path":"visualizing-data-using-ggplot2.html","id":"multiple-layers","chapter":"Lecture 2 Visualizing data using ggplot2","heading":"2.15 Multiple layers","text":"can overlay different plots. , however, must share aesthetic mappings.\nsimplest case one dataset:","code":"\nggplot(data = dt) + \n  geom_point(aes(y = state, x = cases), color = \"black\") + \n  geom_point(aes(y = state, x = deaths), color = \"red\") +\n  scale_x_log10() + \n  xlab(\"cases (black), deaths (red)\")"},{"path":"visualizing-data-using-ggplot2.html","id":"try-on-your-own-data","chapter":"Lecture 2 Visualizing data using ggplot2","heading":"2.16 Try on your own data!","text":"Now ’re familiar ggplot2, try producing meaningful plots data.","code":""},{"path":"visualizing-data-using-ggplot2.html","id":"resources-1","chapter":"Lecture 2 Visualizing data using ggplot2","heading":"2.17 Resources","text":"R Data ScienceTidyverse reference websiteData Visualization Cheat Sheet","code":""},{"path":"fundamentals-of-probability.html","id":"fundamentals-of-probability","chapter":"Lecture 3 Fundamentals of probability","heading":"Lecture 3 Fundamentals of probability","text":"","code":""},{"path":"fundamentals-of-probability.html","id":"sample-spaces-and-random-variables","chapter":"Lecture 3 Fundamentals of probability","heading":"3.1 Sample spaces and random variables","text":"observation measurement world perfectly reproducible, matter carefully planned executed. level uncertainly varies, randomness always finds way creep data set. “random” factor come ? classical physics perspective, articulated Laplace, natural phenomena theoretically deterministic omniscient unlimited computational power. Quantum mechanical phenomena (theoretically) truly random, randomness observable scales biology social science. lack predictability data work usually due either intrinsic complexity (e.g., bio-molecular systems, prediction animal behavior), essentially makes impossible know every detail system, external source noise (e.g., measurement error, weather affecting food availability) outside control.probability terminology, random experiment produces outcomes collection outcomes experiment called sample space.Example: specifics experiment can affect degree uncertainty outcome; measurement may random , depending context. example, measuring height person deterministic, one measures height person within short amount time. unless ’re interested studying error stadiometer results, probably won’t consider random experiment. However, measuring heights different people random experiment, source randomness primarily due selection people study, called sampling error, rather due measurement noise one person.measurement interest random experiment called random variable. Sometimes measurement simply outcome, usually reports aspect outcome several outcomes can value random variable. random variable can seen condensing sample space smaller range values. Random variables can numeric categorical, difference categorical variables assigned meaningful numbers. instance, one may report individual phenotype (e.g., white purple flowers), nucleotide , T, G, C particular position, although one assign numbers categories (e.g., 1, 2, 3, 4) used sensible way—one can compare arithmetic numbers, less T + T equal G. Thus different tools describing working numeric categorical random variables.Example: DNA sequence codon triplet represents specific amino acid, redundancy (several triplets may code amino acid). One may think coding DNA sequence outcome, amino acid (sequence single one) random variable. Extending framework, one may think genotype outcome, phenotype (e.g., eye color) random variable—although correct phenotype strictly determined genotype, factors (e.g., environmental epigenetic) influence value random variable besides outcome (genotype).Exercise: package palmerpenguins contains multiple variables measured populations three different species penguins three years three different islands. Identify numeric categorical variables, specify whether numeric variables discrete continuous.","code":"\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nstr(penguins)## tibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n##  $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n##  $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n##  $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n##  $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n##  $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n##  $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n##  $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ..."},{"path":"fundamentals-of-probability.html","id":"probability-axioms","chapter":"Lecture 3 Fundamentals of probability","heading":"3.2 Probability axioms","text":"outcome sample space can assigned probability depending frequency occurrence many trials, number 0 1. Combinations outcomes (events) can assigned probabilities building individual outcomes. probabilities rules, called axioms probability, expressed using set theory notation.total probability outcomes sample space 1. \\(P(\\Omega) = 1\\)total probability outcomes sample space 1. \\(P(\\Omega) = 1\\)probability nothing (empty set) 0. \\(P(\\emptyset) = 0\\)probability nothing (empty set) 0. \\(P(\\emptyset) = 0\\)probability event made union two events sum two probabilities minus probability overlap (intersection.) \\(P(\\cup B) = P() + P(B) - P(\\cap B)\\)probability event made union two events sum two probabilities minus probability overlap (intersection.) \\(P(\\cup B) = P() + P(B) - P(\\cap B)\\)Example: Let’s assign probability every possible three-letter codon. \\(4^3 = 64\\) codons, one assumes one equal probability, equal \\(1/64\\) (axiom 1.) probability codon first letter 1/4, probability second letter. Axiom 3 allows us calculate probability either first second letter:\\[ P(AXX \\cup \\ XAX ) =  P(AXX) + P(XAX) - P(AAX) = 1/4 + 1/4 - 1/16 = 7/16\\]","code":""},{"path":"fundamentals-of-probability.html","id":"probability-distributions","chapter":"Lecture 3 Fundamentals of probability","heading":"3.3 Probability distributions","text":"probability value random variable can calculated probability event corresponds value random variable. collection probabilities values random variable called probability distribution function random variable, formally mass function discrete random variable density function continuous random variable.discrete random variable (let’s call \\(X\\)) probability mass function \\(f\\), probability \\(X\\) taking value \\(\\) can written either \\(f(X=)\\) \\(f()\\), long ’s clear \\(f\\) probability distribution function \\(X\\). one ironclad rule probability values mass function add 1. state mathematically, possible values \\(X\\) can written \\(a_1, a_2, ...\\) (may finitely infinitely many , long ’s countable infinity), sum equal 1:\n\\[ \\sum_i f(a_i) = 1 \\]continuous random variable (let’s call \\(Y\\)) probability density function \\(g\\) bit complicated. continuous part means random variable uncountably many values, even range finite (example, uncountably many real numbers 0 1). Thus, probability single value must vanishingly small (zero), otherwise impossible add (integrate) values get finite result (let alone 1). can measure probability range values \\(Y\\) defined integral density function overall range:\\[ P( < Y < b) = \\int_a ^b g(y) dy \\]total probability entire range \\(Y\\) 1, ’s similarly calculated integration instead summation (\\(R\\) represents range values \\(Y\\)):\\[ \\int_R g(y) dy = 1\\]Example: codons (DNA triplets) code amino acids, can consider genetic code random variable sample space. Assuming codons equal probabilities, probability amino acid number triplets code divided 64. example, probabilities leucine arginine \\(6/64 = 3/32\\), probability threonine \\(4/64 = 1/16\\) probabilities methionine tryptophan \\(1/64\\). defines probability distribution function random variable genetic code. Note sum probabilities amino acids 1. course inherent reason triplet equally probable, different probability structure sample space result different probability distribution (mass) function.","code":""},{"path":"fundamentals-of-probability.html","id":"measures-of-center-medians-and-means","chapter":"Lecture 3 Fundamentals of probability","heading":"3.4 Measures of center: medians and means","text":"standard measures described applicable numeric random variables. measures center spread categorical variables exist well.median random variable value middle distribution, specifically, probability random variable greater value 0.5.mean expectation random variable center mass probability distribution. Specifically, defined mass function :\\[ E(X) = \\sum_i a_i\\, f(a_i)\\]density function defined using integral:\n\\[ E(Y) =  \\int_R y\\, g(y) dy \\]Example: Let us examine factors (categorical variables) penguins data set. described using means medians, can plotted counts category learned introduction ggplot2:One can plot distributions numeric variables like body mass different penguin species using box plots:following code chunk uses dplyr functions learn next chapter calculate mean median values variables aggregated species:Comment descriptive statistics correspond box plots.","code":"\nggplot(data = penguins) +\n  aes(x = species, fill = sex) + \n  geom_bar(position = \"fill\")\nggplot(data = penguins) +\n  aes(x = year, fill = species) + \n  geom_bar(position = \"fill\")\nggplot(data = penguins) + aes(x = as.factor(species), y=body_mass_g) + geom_boxplot()## Warning: Removed 2 rows containing non-finite values (stat_boxplot).\npenguins %>% drop_na() %>% group_by(species) %>% summarise(mean = mean(body_mass_g))## # A tibble: 3 × 2\n##   species    mean\n##   <fct>     <dbl>\n## 1 Adelie    3706.\n## 2 Chinstrap 3733.\n## 3 Gentoo    5092.\npenguins %>% drop_na()  %>% group_by(species) %>% summarise(median = median(body_mass_g))## # A tibble: 3 × 2\n##   species   median\n##   <fct>      <dbl>\n## 1 Adelie      3700\n## 2 Chinstrap   3700\n## 3 Gentoo      5050"},{"path":"fundamentals-of-probability.html","id":"measures-of-spread-quartiles-and-variances","chapter":"Lecture 3 Fundamentals of probability","heading":"3.5 Measures of spread: quartiles and variances","text":"random variables spread values. simplest way describe stating range (interval minimum maximum values) quartiles (medians two halves distribution).standard measure spread distribution variance, defined expected value squared differences mean:\\[\\text{Var}(X) = E [X - E(X)]^2 = \\sum_i (a_i- E(X))^2 f(a_i)\\]density function defined using integral:\n\\[\\text{Var}(Y) =  E[ Y - E(Y)]^2 = \\int_R (y-E(Y))^2 g(y) dy \\]Variances squared units directly comparable values random variable. Taking square root variance converts units called standard deviation distribution:\n\\[ \\sigma_X = \\sqrt{\\text{Var}(X)}\\]\nExample: Let’s go back penguins data set calculate measures spread variable body mass different penguin speciesWhich species wider spread body mass? descriptive stats box plots correspond?","code":"\nggplot(data = penguins) + aes(x = as.factor(species), y=body_mass_g) + geom_boxplot()## Warning: Removed 2 rows containing non-finite values (stat_boxplot).\npenguins %>% drop_na() %>% group_by(species) %>% summarise(var = var(body_mass_g))## # A tibble: 3 × 2\n##   species       var\n##   <fct>       <dbl>\n## 1 Adelie    210332.\n## 2 Chinstrap 147713.\n## 3 Gentoo    251478.\npenguins %>% drop_na() %>% group_by(species) %>% summarise(first_quart = quantile(body_mass_g,0.25))## # A tibble: 3 × 2\n##   species   first_quart\n##   <fct>           <dbl>\n## 1 Adelie          3362.\n## 2 Chinstrap       3488.\n## 3 Gentoo          4700\npenguins %>% drop_na() %>% group_by(species) %>% summarise(third_quart = quantile(body_mass_g,0.75))## # A tibble: 3 × 2\n##   species   third_quart\n##   <fct>           <dbl>\n## 1 Adelie           4000\n## 2 Chinstrap        3950\n## 3 Gentoo           5500"},{"path":"fundamentals-of-probability.html","id":"data-as-samples-from-distributions-statistics","chapter":"Lecture 3 Fundamentals of probability","heading":"3.6 Data as samples from distributions: statistics","text":"scientific practice, collect data one random variables, called sample, try make sense . One basic goals statistical inference: using data set describe population distribution sample drawn. Data sets can plotted histograms frequency/fraction value approximation underlying probability distribution. addition, descriptive statistics sample data (means, variances, medians, etc.) can used estimate true parameters mean variance population distribution.fundamental questions population include:type distribution ?type distribution ?Estimate parameters distribution.Estimate parameters distribution.Test hypothesis, e.g., whether two samples drawn distribution.Test hypothesis, e.g., whether two samples drawn distribution.Describe test relationship two variables.Describe test relationship two variables.","code":""},{"path":"fundamentals-of-probability.html","id":"law-of-large-numbers","chapter":"Lecture 3 Fundamentals of probability","heading":"3.6.1 Law of large numbers","text":"First, sample unbiased, , outcomes systematically - -represented. even unbiased sample differ population due inherent randomness selection (sampling error). law large numbers states sample size increases, mean sample converges true mean population. Formally, set \\(n\\) independent, identically distributed random variables (sample) \\(\\{X_i\\}\\) sample mean \\(\\overline{X}_n\\) converges mean distribution \\(\\mu\\):\\[ \n\\lim _{n \\\\infty} \\frac{\\sum_{=1}^n {X_i}}{n} = \\lim _{n \\\\infty} \\overline{X}_n = \\mu\n\\]","code":""},{"path":"fundamentals-of-probability.html","id":"central-limit-theorem","chapter":"Lecture 3 Fundamentals of probability","heading":"3.6.2 Central Limit Theorem","text":"nice know, doesn’t say exactly large sample needed estimate, example, mean population given precision. , Central Limit Theorem, states distribution sample means (samples independent, identically distributed random variables) sample size increases, approaches normal (Gaussian) distribution mean equal population mean standard deviation equal standard deviation population divided square root sample size. Formally, states set \\(n\\) independent, identically distributed random variables (sample) \\(\\{X_i\\}\\) distribution mean \\(\\mu\\) variance \\(\\sigma^2\\), probability density function sample mean \\(\\overline{X}_n\\) converges large sample size \\(n\\) normal distribution:\\[ \nP(\\overline{X}_n) \\N(\\mu, \\sigma^2/n)\n\\]\\(N(\\mu, \\sigma^2/n\\)) stands normal distribution mean \\(\\mu\\) variance \\(\\sigma^2/n\\). One extremely useful consequence theorem variance sample mean reciprocally related sample size \\(n\\). precisely, allows calculation confidence intervals using normal distribution generate interval around observed sample mean true mean \\(\\mu\\) lies given likelihood.amazing result applies distribution, allows estimation means situation, long condition independent, identically distributed variables sample satisfied (identical distributed condition can actually relaxed). central limit theorems apply situations, including cases random variables sample independent (e.g., Markov models). bottom line unbiased sample contains reflection true population, always distorted uncertainty. Larger sample sizes decrease uncertainty difficult expensive obtain.Discussion: Suggest examples biological data sets made independent identically distributed random variables.","code":""},{"path":"fundamentals-of-probability.html","id":"exploration-misleading-means","chapter":"Lecture 3 Fundamentals of probability","heading":"3.7 Exploration: misleading means","text":"Means common type descriptive statistic sometimes numeric quantity used compare two data sets, e.g. “average GPA school 3.5 vs 3.8 school B.” However, means can misleading measures multiple ways.First, means highly sensitive outliers, points different values. can skew mean value, even pulling completely away bulk values, case mean ceases measure “typical” value.Second, can funny business combining means different subsets data. Normally, might expect group group B, group two subgroups divided another variable (e.g. comparing GPAs students school school B, split students school gender), means subgroup larger means subgroup B (e.g. GPA girls boys school higher counterparts school B), relationship true combined mean group group B (, overall GPA school higher school B). necessarily true!apparent contradiction called Simpson’s paradox. can illustrated data set passengers crew doomed ocean liner Titanic. data set found library stablelearner loaded chunk :chunk calculated survival probability passengers classes compared crew (types:can see 24% crew survived almost 38% passengers survived. week’s assignment calculate explain happens divide people group gender.","code":"\nlibrary(stablelearner)\ndata(titanic)\nstr(titanic)## 'data.frame':    2207 obs. of  11 variables:\n##  $ name    : chr  \"Abbing, Mr. Anthony\" \"Abbott, Mr. Eugene Joseph\" \"Abbott, Mr. Rossmore Edward\" \"Abbott, Mrs. Rhoda Mary 'Rosa'\" ...\n##  $ gender  : Factor w/ 2 levels \"female\",\"male\": 2 2 2 1 1 2 2 1 2 2 ...\n##  $ age     : num  42 13 16 39 16 25 30 28 27 20 ...\n##  $ class   : Factor w/ 7 levels \"1st\",\"2nd\",\"3rd\",..: 3 3 3 3 3 3 2 2 3 3 ...\n##  $ embarked: Factor w/ 4 levels \"B\",\"C\",\"Q\",\"S\": 4 4 4 4 4 4 2 2 2 4 ...\n##  $ country : Factor w/ 48 levels \"Argentina\",\"Australia\",..: 44 44 44 15 30 44 17 17 26 16 ...\n##  $ ticketno: int  5547 2673 2673 2673 348125 348122 3381 3381 2699 3101284 ...\n##  $ fare    : num  7.11 20.05 20.05 20.05 7.13 ...\n##  $ sibsp   : Ord.factor w/ 9 levels \"0\"<\"1\"<\"2\"<\"3\"<..: 1 1 2 2 1 1 2 2 1 1 ...\n##  $ parch   : Ord.factor w/ 10 levels \"0\"<\"1\"<\"2\"<\"3\"<..: 1 3 2 2 1 1 1 1 1 1 ...\n##  $ survived: Factor w/ 2 levels \"no\",\"yes\": 1 1 1 2 2 2 1 2 2 2 ...\ntitanic %>% group_by(Passenger = class %in% c('1st', '2nd', '3rd'), survived) %>% summarise(num = n()) %>% mutate(fraction = num/sum(num)) ## `summarise()` has grouped output by 'Passenger'. You can override using the `.groups` argument.## # A tibble: 4 × 4\n## # Groups:   Passenger [2]\n##   Passenger survived   num fraction\n##   <lgl>     <fct>    <int>    <dbl>\n## 1 FALSE     no         679    0.763\n## 2 FALSE     yes        211    0.237\n## 3 TRUE      no         817    0.620\n## 4 TRUE      yes        500    0.380"},{"path":"fundamentals-of-probability.html","id":"references","chapter":"Lecture 3 Fundamentals of probability","heading":"3.8 References","text":"Laplace’s views probability determinismCentral Limit Theorem RExploration Central Limit TheoremSimpson’s paradox","code":""},{"path":"data-wrangling.html","id":"data-wrangling","chapter":"Lecture 4 Data wrangling","heading":"Lecture 4 Data wrangling","text":"","code":""},{"path":"data-wrangling.html","id":"goal-2","chapter":"Lecture 4 Data wrangling","heading":"4.1 Goal","text":"Learn manipulate large data sets writing efficient, consistent, compact code. Introduce use dplyr, tidyr, “pipeline” operator %>%. Effortlessly produce statistics grouped data. Massage data “tidy” form.","code":""},{"path":"data-wrangling.html","id":"what-is-data-wrangling","chapter":"Lecture 4 Data wrangling","heading":"4.2 What is data wrangling?","text":"biologists living XXI century, often faced tons data, possibly replicated several organisms, treatments, locations. like streamline automate analysis much possible, writing scripts easy read, fast run, easy debug. Base R can get job done, often code contains complicated operations, lot $ signs brackets.’re going learn packages dplyr tidyr, part tidyverse can used manipulate large data frames simple straightforward way. tools also much faster corresponding base R commands, compact, can concatenated “pipelines.”start, need import libraries:going use data set penguins package palmerpenguins, already seen last week.","code":"\nlibrary(tidyverse) # this loads both dplyr and tidyr, along with other packages\nlibrary(palmerpenguins) # a nice data set to play with"},{"path":"data-wrangling.html","id":"a-new-data-type-tibble","chapter":"Lecture 4 Data wrangling","heading":"4.3 A new data type, tibble","text":"now :dplyr ships new data type, called tibble. convert data.frame tibble, use as_tibble:nice feature tbl objects print fits screen, also give useful information size data, well type data column. , tbl object behaves much like data.frame. rare cases, want transform tbl back data.frame. , use function .data.frame(tbl_object).can take look data using one several functions:head(dt) shows first rowstail(dt) shows last rowsglimpse(dt) summary data (similar str base R)View(dt) open spreadsheet-like window","code":"\nclass(penguins)## [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n# load a data frame\ndata(\"trees\")\nclass(trees)\ntrees <- as_tibble(trees)\nclass(trees)"},{"path":"data-wrangling.html","id":"selecting-rows-and-columns","chapter":"Lecture 4 Data wrangling","heading":"4.4 Selecting rows and columns","text":"many ways subset data, either row (subsetting observations), column (subsetting variables). example, let’s select rows observations island Torgersen:52 observations. used command filter(tbl, conditions) select certain observations. can combine several conditions, listing side side, possibly using logical operators.Exercise: ?\nfilter(penguins,  bill_length_mm > 40,  bill_depth_mm > 20, sex == male)can also select particular variables (columns) using function select(tbl, cols select). example, select species island:many species represented data set? can use function distinct(tbl, cols select) retain rows differ :Showing three species, removed duplicates. many ways subset observations:sample_n(tbl, howmany, replace = TRUE) sample howmany rows random (replacement)sample_frac(tbl, proportion, replace = FALSE) sample certain proportion (e.g. 0.2 20%) rows random without replacementslice(tbl, 5:20) extract rows 5 20top_n(penguins, 10, body_mass_g) extract first 10 rows, ordered body_mass_gMore ways select columns:select(penguins, contains(\"mm\")) select columns containing string mmselect(penguins, -year, -body_mass_g) exclude columns year body_mass_gselect(penguins, matches(\"length|bill\")) select columns whose names match regular expression","code":"\nfilter(penguins, island == \"Torgersen\")## # A tibble: 52 × 8\n##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n##    <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n##  1 Adelie  Torgersen           39.1          18.7               181        3750\n##  2 Adelie  Torgersen           39.5          17.4               186        3800\n##  3 Adelie  Torgersen           40.3          18                 195        3250\n##  4 Adelie  Torgersen           NA            NA                  NA          NA\n##  5 Adelie  Torgersen           36.7          19.3               193        3450\n##  6 Adelie  Torgersen           39.3          20.6               190        3650\n##  7 Adelie  Torgersen           38.9          17.8               181        3625\n##  8 Adelie  Torgersen           39.2          19.6               195        4675\n##  9 Adelie  Torgersen           34.1          18.1               193        3475\n## 10 Adelie  Torgersen           42            20.2               190        4250\n## # … with 42 more rows, and 2 more variables: sex <fct>, year <int>\nselect(penguins, species, island)## # A tibble: 344 × 2\n##    species island   \n##    <fct>   <fct>    \n##  1 Adelie  Torgersen\n##  2 Adelie  Torgersen\n##  3 Adelie  Torgersen\n##  4 Adelie  Torgersen\n##  5 Adelie  Torgersen\n##  6 Adelie  Torgersen\n##  7 Adelie  Torgersen\n##  8 Adelie  Torgersen\n##  9 Adelie  Torgersen\n## 10 Adelie  Torgersen\n## # … with 334 more rows\ndistinct(select(penguins, species))## # A tibble: 3 × 1\n##   species  \n##   <fct>    \n## 1 Adelie   \n## 2 Gentoo   \n## 3 Chinstrap"},{"path":"data-wrangling.html","id":"creating-pipelines-using","chapter":"Lecture 4 Data wrangling","heading":"4.5 Creating pipelines using %>%","text":"’ve calling nested functions, distinct(select(penguins, species)). add another layer two, code become unreadable. dplyr allows “un-nest” functions create “pipeline” concatenate commands separated special operator, %>%. example:exactly operations command , much readable. concatenating many commands, can create incredibly complex pipelines retaining readability. also quite easy add another piece pipeline commands, comment pipeline .Another advantage pipelines help name completion. fact, RStudio running background pipeline type . Try typing dt %>% filter( start typing bill press Tab: see options complete column name; choose arrows hit Return. back tick-marks added automatically needed (e.g., column names containing spaces, starting digit).","code":"\npenguins %>% # take a data table\n  select(species) %>% # select a column\n  distinct() # remove duplicates## # A tibble: 3 × 1\n##   species  \n##   <fct>    \n## 1 Adelie   \n## 2 Gentoo   \n## 3 Chinstrap"},{"path":"data-wrangling.html","id":"producing-summaries","chapter":"Lecture 4 Data wrangling","heading":"4.6 Producing summaries","text":"Sometimes need calculate statistics certain columns. example, calculate average number eggs shed infected mice. can using summarise (can use British American spelling):used na.rm = TRUE ignore missing values. command returns tbl object just average egg count. can combine multiple statistics (use first, last, min, max, n [count number rows], n_distinct [count number distinct rows], mean, median, var, sd, etc.):","code":"\npenguins %>% \n  summarise(avg = mean(body_mass_g, na.rm = TRUE))## # A tibble: 1 × 1\n##     avg\n##   <dbl>\n## 1 4202.\n# alternatively, drop_na(body_mass_g) removes all the observations for which\n# body_mass_g is NA\npenguins %>% \n  drop_na(body_mass_g) %>% \n  summarise(avg = mean(body_mass_g, na.rm = TRUE))## # A tibble: 1 × 1\n##     avg\n##   <dbl>\n## 1 4202.\npenguins %>% \n  summarise(avg = mean(body_mass_g, na.rm = TRUE), \n            sd = sd(body_mass_g, na.rm = TRUE), \n            median = median(body_mass_g, na.rm = TRUE))## # A tibble: 1 × 3\n##     avg    sd median\n##   <dbl> <dbl>  <dbl>\n## 1 4202.  802.   4050"},{"path":"data-wrangling.html","id":"summaries-by-group","chapter":"Lecture 4 Data wrangling","heading":"4.7 Summaries by group","text":"One useful features dplyr ability produce statistics data subsetted groups. example, like compute average body mass species sex:showing male penguins heavier three species considered.Exercise: find average bill_depth_mm bill_length_mm species sex. Filter data consider observations year 2008.","code":"\npenguins %>% \n  drop_na() %>% \n  group_by(sex, species) %>% \n  summarise(mean = mean(body_mass_g, na.rm = TRUE))## `summarise()` has grouped output by 'sex'. You can override using the `.groups` argument.## # A tibble: 6 × 3\n## # Groups:   sex [2]\n##   sex    species    mean\n##   <fct>  <fct>     <dbl>\n## 1 female Adelie    3369.\n## 2 female Chinstrap 3527.\n## 3 female Gentoo    4680.\n## 4 male   Adelie    4043.\n## 5 male   Chinstrap 3939.\n## 6 male   Gentoo    5485."},{"path":"data-wrangling.html","id":"ordering-the-data","chapter":"Lecture 4 Data wrangling","heading":"4.8 Ordering the data","text":"order data according one variables, use arrange():","code":"\npenguins %>% \n  arrange(body_mass_g) # ascending## # A tibble: 344 × 8\n##    species   island    bill_length_mm bill_depth_mm flipper_length_… body_mass_g\n##    <fct>     <fct>              <dbl>         <dbl>            <int>       <int>\n##  1 Chinstrap Dream               46.9          16.6              192        2700\n##  2 Adelie    Biscoe              36.5          16.6              181        2850\n##  3 Adelie    Biscoe              36.4          17.1              184        2850\n##  4 Adelie    Biscoe              34.5          18.1              187        2900\n##  5 Adelie    Dream               33.1          16.1              178        2900\n##  6 Adelie    Torgersen           38.6          17                188        2900\n##  7 Chinstrap Dream               43.2          16.6              187        2900\n##  8 Adelie    Biscoe              37.9          18.6              193        2925\n##  9 Adelie    Dream               37.5          18.9              179        2975\n## 10 Adelie    Dream               37            16.9              185        3000\n## # … with 334 more rows, and 2 more variables: sex <fct>, year <int>\npenguins %>% \n  arrange(desc(body_mass_g)) # descending## # A tibble: 344 × 8\n##    species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n##    <fct>   <fct>           <dbl>         <dbl>             <int>       <int>\n##  1 Gentoo  Biscoe           49.2          15.2               221        6300\n##  2 Gentoo  Biscoe           59.6          17                 230        6050\n##  3 Gentoo  Biscoe           51.1          16.3               220        6000\n##  4 Gentoo  Biscoe           48.8          16.2               222        6000\n##  5 Gentoo  Biscoe           45.2          16.4               223        5950\n##  6 Gentoo  Biscoe           49.8          15.9               229        5950\n##  7 Gentoo  Biscoe           48.4          14.6               213        5850\n##  8 Gentoo  Biscoe           49.3          15.7               217        5850\n##  9 Gentoo  Biscoe           55.1          16                 230        5850\n## 10 Gentoo  Biscoe           49.5          16.2               229        5800\n## # … with 334 more rows, and 2 more variables: sex <fct>, year <int>"},{"path":"data-wrangling.html","id":"renaming-columns","chapter":"Lecture 4 Data wrangling","heading":"4.9 Renaming columns","text":"rename one columns, use rename():","code":"\npenguins %>% \n  rename(bm = body_mass_g)## # A tibble: 344 × 8\n##    species island    bill_length_mm bill_depth_mm flipper_length_mm    bm sex   \n##    <fct>   <fct>              <dbl>         <dbl>             <int> <int> <fct> \n##  1 Adelie  Torgersen           39.1          18.7               181  3750 male  \n##  2 Adelie  Torgersen           39.5          17.4               186  3800 female\n##  3 Adelie  Torgersen           40.3          18                 195  3250 female\n##  4 Adelie  Torgersen           NA            NA                  NA    NA <NA>  \n##  5 Adelie  Torgersen           36.7          19.3               193  3450 female\n##  6 Adelie  Torgersen           39.3          20.6               190  3650 male  \n##  7 Adelie  Torgersen           38.9          17.8               181  3625 female\n##  8 Adelie  Torgersen           39.2          19.6               195  4675 male  \n##  9 Adelie  Torgersen           34.1          18.1               193  3475 <NA>  \n## 10 Adelie  Torgersen           42            20.2               190  4250 <NA>  \n## # … with 334 more rows, and 1 more variable: year <int>"},{"path":"data-wrangling.html","id":"adding-new-variables-using-mutate","chapter":"Lecture 4 Data wrangling","heading":"4.10 Adding new variables using mutate","text":"want add one new columns, content function columns, use function mutate. example, going add new column showing z-score body mass individual:can pipe results ggplot plotting!can use function transmute() create new column drop original columns.importantly, can use mutate transmute grouped data. example, let’s recompute z-score body_mass_g data grouped species sex:","code":"\npenguins %>% \n  mutate(zscore_bm = scale(body_mass_g)) %>% \n  select(species, sex, body_mass_g, zscore_bm)## # A tibble: 344 × 4\n##    species sex    body_mass_g zscore_bm[,1]\n##    <fct>   <fct>        <int>         <dbl>\n##  1 Adelie  male          3750       -0.563 \n##  2 Adelie  female        3800       -0.501 \n##  3 Adelie  female        3250       -1.19  \n##  4 Adelie  <NA>            NA       NA     \n##  5 Adelie  female        3450       -0.937 \n##  6 Adelie  male          3650       -0.688 \n##  7 Adelie  female        3625       -0.719 \n##  8 Adelie  male          4675        0.590 \n##  9 Adelie  <NA>          3475       -0.906 \n## 10 Adelie  <NA>          4250        0.0602\n## # … with 334 more rows\npenguins %>% \n  mutate(zscore_bm = scale(body_mass_g)) %>% \n  select(species, sex, body_mass_g, zscore_bm) %>% \n  ggplot() + aes(x = species, y = zscore_bm, colour = sex) + \n    geom_jitter()## Warning: Removed 2 rows containing missing values (geom_point).\npenguins %>% \n  drop_na() %>% \n  select(species, sex, body_mass_g) %>% \n  group_by(species, sex) %>% \n  mutate(zscore_bm = scale(body_mass_g)) %>% \n  arrange(body_mass_g)## # A tibble: 333 × 4\n## # Groups:   species, sex [6]\n##    species   sex    body_mass_g zscore_bm[,1]\n##    <fct>     <fct>        <int>         <dbl>\n##  1 Chinstrap female        2700         -2.90\n##  2 Adelie    female        2850         -1.93\n##  3 Adelie    female        2850         -1.93\n##  4 Adelie    female        2900         -1.74\n##  5 Adelie    female        2900         -1.74\n##  6 Adelie    female        2900         -1.74\n##  7 Chinstrap female        2900         -2.20\n##  8 Adelie    female        2925         -1.65\n##  9 Adelie    female        3000         -1.37\n## 10 Adelie    female        3000         -1.37\n## # … with 323 more rows"},{"path":"data-wrangling.html","id":"data-wrangling-1","chapter":"Lecture 4 Data wrangling","heading":"4.11 Data wrangling","text":"Data rarely format good computing, much effort goes reading data wrestling make good format. name implies, tidyverse strongly advocates use data tidy form. mean?variable forms columnEach observation forms rowEach type observational unit forms tableThis often called narrow table format. form data (e.g., wide table format) considered messy. However, often data organized tidy form, want produce tables human consumption rather computer consumption. package tidyr allows accomplish just . contains , powerful functions. explore issue, build data set containing average body mass species sex:","code":"\npenguin_bm <- penguins %>% \n  drop_na() %>% \n  group_by(sex, species) %>% \n  summarise(body_mass = mean(body_mass_g), .groups = \"drop\") # remove groups after calculation\n\npenguin_bm## # A tibble: 6 × 3\n##   sex    species   body_mass\n##   <fct>  <fct>         <dbl>\n## 1 female Adelie        3369.\n## 2 female Chinstrap     3527.\n## 3 female Gentoo        4680.\n## 4 male   Adelie        4043.\n## 5 male   Chinstrap     3939.\n## 6 male   Gentoo        5485."},{"path":"data-wrangling.html","id":"from-narrow-to-wide","chapter":"Lecture 4 Data wrangling","heading":"4.12 From narrow to wide","text":"data tidy form. paper, want show difference males females table:created new column names using values found sex (hence, names_from), filled cell corresponding value found body_mass (hence, values_from). Similarly, want show data species column names, sex rows, can use:","code":"\npenguin_bm %>% \n  pivot_wider(names_from = sex, values_from = body_mass)## # A tibble: 3 × 3\n##   species   female  male\n##   <fct>      <dbl> <dbl>\n## 1 Adelie     3369. 4043.\n## 2 Chinstrap  3527. 3939.\n## 3 Gentoo     4680. 5485.\npenguin_bm %>% \n  pivot_wider(names_from = species, values_from = body_mass)## # A tibble: 2 × 4\n##   sex    Adelie Chinstrap Gentoo\n##   <fct>   <dbl>     <dbl>  <dbl>\n## 1 female  3369.     3527.  4680.\n## 2 male    4043.     3939.  5485."},{"path":"data-wrangling.html","id":"from-wide-to-narrow","chapter":"Lecture 4 Data wrangling","heading":"4.13 From wide to narrow","text":"real-world example, make data :Tree-ring analysis sustainable harvest Millettia stuhlmannii Mozambique, ..D.Remane M.D.Therrell, South African Journal Botany Volume 125, September 2019, Pages 120-125You can read tab-separated file :column besides YEAR represents single tree, cell contains diameter (cm) tree given age. make tidy form, first create columns tree diameter:remove NAs:Now easy plot growth trajectory tree (Fig. 3 original paper):","code":"\ndt <- read_tsv(\"data/annual_increment.txt\") %>% \n  select(Age, contains(\"CAT\"))## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   .default = col_double(),\n##   X38 = col_logical(),\n##   X39 = col_logical(),\n##   X40 = col_logical(),\n##   X41 = col_logical(),\n##   X42 = col_logical(),\n##   X43 = col_logical(),\n##   X44 = col_logical(),\n##   X45 = col_logical(),\n##   X46 = col_logical(),\n##   X47 = col_logical(),\n##   X48 = col_logical(),\n##   X49 = col_logical(),\n##   X50 = col_logical(),\n##   X51 = col_logical(),\n##   X52 = col_logical(),\n##   X53 = col_logical(),\n##   X54 = col_logical(),\n##   X55 = col_logical()\n## )\n## ℹ Use `spec()` for the full column specifications.\n# selecting only age and samples\ndt <- dt %>% \n  pivot_longer(-Age, names_to = \"tree\", values_to = \"diameter\")\ndt <- dt %>% filter(!is.na(diameter))\ndt %>% \n  ggplot() + \n  aes(x = Age, y = diameter) + \n  geom_line(aes(group = tree)) + # note---this makes a line for each tree\n  geom_smooth(method = \"loess\") # while the smoothing function considers all trees## `geom_smooth()` using formula 'y ~ x'"},{"path":"data-wrangling.html","id":"separate-split-a-column-into-two-or-more","chapter":"Lecture 4 Data wrangling","heading":"4.14 Separate: split a column into two or more","text":"complement separate called unite.","code":"\ntest <- tibble(name = c(\"Allesina, Stefano\", \"Kondrashov, Dmitry\", \"Smith, Julia\"))\ntest## # A tibble: 3 × 1\n##   name              \n##   <chr>             \n## 1 Allesina, Stefano \n## 2 Kondrashov, Dmitry\n## 3 Smith, Julia\ntest %>% separate(name, into = c(\"last_name\", \"first_name\"), sep = \", \")## # A tibble: 3 × 2\n##   last_name  first_name\n##   <chr>      <chr>     \n## 1 Allesina   Stefano   \n## 2 Kondrashov Dmitry    \n## 3 Smith      Julia"},{"path":"data-wrangling.html","id":"separate-rows-from-one-row-to-many","chapter":"Lecture 4 Data wrangling","heading":"4.15 Separate rows: from one row to many","text":"make tidy form, one record per row:","code":"\ntest <- tibble(id = c(1, 2, 3, 4), records = c(\"a;b;c\", \"c;d\", \"a;e\", \"f\"))\ntest## # A tibble: 4 × 2\n##      id records\n##   <dbl> <chr>  \n## 1     1 a;b;c  \n## 2     2 c;d    \n## 3     3 a;e    \n## 4     4 f\ntest %>% separate_rows(records, sep = \";\")## # A tibble: 8 × 2\n##      id records\n##   <dbl> <chr>  \n## 1     1 a      \n## 2     1 b      \n## 3     1 c      \n## 4     2 c      \n## 5     2 d      \n## 6     3 a      \n## 7     3 e      \n## 8     4 f"},{"path":"data-wrangling.html","id":"example-brown-bear-brown-bear-what-do-you-see","chapter":"Lecture 4 Data wrangling","heading":"4.16 Example: brown bear, brown bear, what do you see?","text":"exercise uses dataset GBIF, Global Biodiversity Information Facility. can download latest version following (just skip ahead want use data provided us).Go GBIF click Occurrences.Scientific Name type Ursus arctos (brown bear), hit enter.download data, create account GBIFThen click Download, select Simple (tab-delimited .csv file)Save data folder working folder.don’t want go , can use downloaded file called Ursus_GBIF.csv data folder week. following command loads displays contents tibble:see 50 variables data set, may useful remove ones don’t need. exercise, objective plot occurrences species world map, need two variables certain: decimalLatitude decimalLongitude, well BasisofRecord additional information. Use tidyverse skills create new tibble variables. addition, remove duplicate records tibble.Now can plot data set world map, using useful package maps. plot, use ggplot() syntax following addition:Note warning message generated ggplot. consider map locations brown bear specimens. seem strange ? may explanation behind strange data point? Now filter points identified suspicious print BasisofRecord. suggest explanation strangeness?","code":"\n# you will need ggmap!\nlibrary(ggmap)\nUrsus_data <- read_tsv(\"data/Ursus_GBIF.csv\")\nglimpse(Ursus_data)## Rows: 23,498\n## Columns: 50\n## $ gbifID                           <dbl> 2382421192, 2382420986, 2382420916, 2…\n## $ datasetKey                       <chr> \"88d8974c-f762-11e1-a439-00145eb45e9a…\n## $ occurrenceID                     <chr> \"http://arctos.database.museum/guid/U…\n## $ kingdom                          <chr> \"Animalia\", \"Animalia\", \"Animalia\", \"…\n## $ phylum                           <chr> \"Chordata\", \"Chordata\", \"Chordata\", \"…\n## $ class                            <chr> \"Mammalia\", \"Mammalia\", \"Mammalia\", \"…\n## $ order                            <chr> \"Carnivora\", \"Carnivora\", \"Carnivora\"…\n## $ family                           <chr> \"Ursidae\", \"Ursidae\", \"Ursidae\", \"Urs…\n## $ genus                            <chr> \"Ursus\", \"Ursus\", \"Ursus\", \"Ursus\", \"…\n## $ species                          <chr> \"Ursus arctos\", \"Ursus arctos\", \"Ursu…\n## $ infraspecificEpithet             <chr> NA, NA, NA, \"horribilis\", NA, NA, NA,…\n## $ taxonRank                        <chr> \"SPECIES\", \"SPECIES\", \"SPECIES\", \"SUB…\n## $ scientificName                   <chr> \"Ursus arctos Linnaeus, 1758\", \"Ursus…\n## $ verbatimScientificName           <chr> \"Ursus arctos\", \"Ursus arctos\", \"Ursu…\n## $ verbatimScientificNameAuthorship <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ countryCode                      <chr> NA, \"US\", NA, NA, \"US\", NA, NA, \"US\",…\n## $ locality                         <chr> \"no specific locality recorded\", \"no …\n## $ stateProvince                    <chr> NA, \"Alaska\", NA, NA, \"Colorado\", NA,…\n## $ occurrenceStatus                 <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ individualCount                  <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n## $ publishingOrgKey                 <chr> \"4cadac10-3e7b-11d9-8439-b8a03c50a862…\n## $ decimalLatitude                  <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ decimalLongitude                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ coordinateUncertaintyInMeters    <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ coordinatePrecision              <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ elevation                        <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ elevationAccuracy                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ depth                            <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ depthAccuracy                    <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ eventDate                        <dttm> 1800-01-01, 1800-01-01, 1800-01-01, …\n## $ day                              <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n## $ month                            <dbl> 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1…\n## $ year                             <dbl> 1800, 1800, 1800, 1800, 1914, 1938, 1…\n## $ taxonKey                         <dbl> 2433433, 2433433, 2433433, 6163845, 2…\n## $ speciesKey                       <dbl> 2433433, 2433433, 2433433, 2433433, 2…\n## $ basisOfRecord                    <chr> \"PRESERVED_SPECIMEN\", \"PRESERVED_SPEC…\n## $ institutionCode                  <chr> \"UCM\", \"UCM\", \"UCM\", \"UCM\", \"UCM\", \"U…\n## $ collectionCode                   <chr> \"Mammal specimens\", \"Mammal specimens…\n## $ catalogNumber                    <chr> \"UCM:Mamm:5003\", \"UCM:Mamm:3329\", \"UC…\n## $ recordNumber                     <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ identifiedBy                     <chr> \"T. C. Hart\", \"unknown\", \"unknown\", \"…\n## $ dateIdentified                   <dttm> 2013-01-01, 1936-01-01, NA, 2015-10-…\n## $ license                          <chr> \"CC0_1_0\", \"CC0_1_0\", \"CC0_1_0\", \"CC0…\n## $ rightsHolder                     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ recordedBy                       <chr> \"Collector(s): T. C. Hart\", \"Collecto…\n## $ typeStatus                       <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ establishmentMeans               <chr> NA, NA, NA, NA, NA, NA, \"MANAGED\", NA…\n## $ lastInterpreted                  <dttm> 2019-09-03 22:11:14, 2019-09-03 22:1…\n## $ mediaType                        <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ issue                            <chr> NA, NA, NA, NA, \"TAXON_MATCH_HIGHERRA…\n# your code goes here!\nmapWorld <- borders(\"world\", colour=\"gray50\", fill=\"gray50\") # create a layer of borders\n# now you can call \n# ggplot() + mapWorld + ...\n# your code goes here!"},{"path":"data-wrangling.html","id":"resources-2","chapter":"Lecture 4 Data wrangling","heading":"4.17 Resources","text":"R Data ScienceA cool class U C Social SciencesData transformation cheat sheetDealing dates cheat sheetData import cheat sheet","code":""},{"path":"distributions-and-their-properties.html","id":"distributions-and-their-properties","chapter":"Lecture 5 Distributions and their properties","heading":"Lecture 5 Distributions and their properties","text":"","code":""},{"path":"distributions-and-their-properties.html","id":"objectives","chapter":"Lecture 5 Distributions and their properties","heading":"5.1 Objectives:","text":"Apply concepts conditional probability practical scenarios questionsDescribe independence concept apply data setsUse random number generators simulate various distributionsBe familiar shape several common distributions describe role parameters","code":""},{"path":"distributions-and-their-properties.html","id":"independence","chapter":"Lecture 5 Distributions and their properties","heading":"5.2 Independence","text":"","code":""},{"path":"distributions-and-their-properties.html","id":"conditional-probability","chapter":"Lecture 5 Distributions and their properties","heading":"5.2.1 Conditional probability","text":"basic definitions probability, considered probabilities outcome events separately. Let us consider information one event affects probability another event. concept one event (let’s call \\(B\\)) true, unless event entire space, rules outcomes. may affect probability events (e.g., \\(\\)) sample space, knowledge \\(B\\) may rule outcomes \\(\\) well. formal definition:Definition: two events \\(\\) \\(B\\) sample space \\(\\Omega\\) probability measure \\(P\\), probability \\(\\) given \\(B\\), called conditional probability, defined :\\(P(\\vert B) = \\frac{P(\\cap B)}{P(B)}\\)\\(\\cap B\\) \\(, B\\) intersection events \\(\\) \\(B\\), also known “\\(\\) \\(B\\)”—event consisting outcomes \\(\\) \\(B\\).words, given knowledge event \\(B\\) occurs, sample space restricted subset \\(B\\), denominator definition \\(P(B)\\). numerator encompasses outcomes interested , (.e., \\(\\)), since now restricted \\(B\\), numerator consists outcomes \\(\\) also \\(B\\), \\(\\cap B\\). definition makes sense two extreme cases: \\(= B\\) \\(\\) \\(B\\) mutually exclusive:\\(P(B\\vert B) = P(B \\cap B) /P(B) = P(B)/P(B) = 1\\)\\(P(\\cap B) = 0\\), \\(P(\\vert B) = 0/P(B) = 0\\)Important note: one common source confusion conditional probability difference probability \\(\\) \\(B\\) probability \\(\\) given \\(B\\). result discrepancy everyday word usage mathematical terminology, statement “odds finding tall person also likes tea?” hard distinguish “odds person tall likes tea?” critical difference two statements former start information picking person entire population, latter start knowledge person tall.Example: classic Mendelian pea experiment, diploid organism carries two alleles. allele \\(\\) dominant results pink flowers, \\(\\) recessive results white flowers. three possible genotypes (\\(AA\\), \\(Aa\\), \\(aa\\)) two phenotypes (Pink White). questions , assume two heterozygous pea plants (genotype \\(Aa\\)) crossed, producing following table genotypes equal probabilities cell:probability plant pink flowers genotype \\(AA\\)? Write terms conditional probability explain ’s different probability plant pink flower genotype \\(AA\\).probability plant pink flowers genotype \\(AA\\)? Write terms conditional probability explain ’s different probability plant pink flower genotype \\(AA\\).probability plant genotype \\(AA\\) pink flowers? , write conditional probability explain ’s different probability plant pink flower genotype \\(AA\\).probability plant genotype \\(AA\\) pink flowers? , write conditional probability explain ’s different probability plant pink flower genotype \\(AA\\).Lesson: general,\n\\[P(X \\vert  Y ) \\neq P(Y \\vert  X)\\]","code":""},{"path":"distributions-and-their-properties.html","id":"independence-1","chapter":"Lecture 5 Distributions and their properties","heading":"5.2.2 Independence","text":"Independence fundamental concept probability may misinterpreted without careful thinking. Intuitively, two events (random variables) independent one influence . precisely, means probability one event regardless whether one happens . expressed precisely using conditional probabilities:Definition: Two events \\(\\) \\(B\\) sample space \\(\\Omega\\) probability measure \\(P\\) independent \\(P(\\vert B) = P()\\), equivalently \\(P(B\\vert ) = P(B)\\).Independence straightforward concept. may confused mutual exclusivity, one might surmise \\(\\) \\(B\\) overlap, independent. however, false definition, since \\(P(\\vert B)\\) 0 two mutually exclusive events. confusion stems thinking \\(\\) \\(B\\) non-overlapping, influence . notion influence definition information; \\(\\) \\(B\\) mutually exclusive, knowledge one occurs influence probability one occurring, specifically rules one .Example: sample space weather phenomena, events snowing hot weather independent?Example: slightly subtle example, lifetime risk breast cancer 1 8 women 1 1000 men. sex breast cancer independent?","code":""},{"path":"distributions-and-their-properties.html","id":"usefulness-of-independence","chapter":"Lecture 5 Distributions and their properties","heading":"5.2.3 Usefulness of independence","text":"Independence mathematical abstraction, reality rarely provides us perfectly independent variables. ’s useful abstraction enables calculations difficult impossible carry without assumption.First, independence allows calculating probability two events two random variables simultaneously. straightforward consequence definition conditional probability (first equality) independence (second equality):\\[\\frac{P(\\cap B)}{P(B)}= P(\\vert B) = P()\\]\nMultiplying sides \\(P(B)\\), get product rule independence, perhaps widely used formula applied probability:\\[P(\\cap B) = P()P(B)\\]Example: probability two randomly selected individuals red hair–assuming occurrence trait independent–square probability red hair one individual. (Note never exactly case finite population—?)Example: probability two alleles two separate genes (call B) occurring gamete may independent may linked. population genetics, concept linkage disequilibrium describes extent linkage; example, alleles located separate chromosomes (eukaryotes) usually linked occurrence independent. coefficient linkage disequilibrium defined difference expected independence actual probability alleles present:\n\\[D_{AB} = P(\\cap B) - P()P(B) \\]\n\\(P()\\) \\(P(B)\\) frequencies two respective alleles (haplotypes) population, \\(P(\\cap B)\\) frequency haplotypes occurring together copy genome (, gamete). two independent loci, \\(D_{AB} = 0\\), loci usually occur together coefficient positive, magnitude influenced physical proximity loci chromosome, evolutionary history species, factors.Another important consequence independence sum two independent random variables. expectation sum random variables linear, can demonstrated using work sums, starting definition expectation (can shown continuous random variables, using integrals instead sums):\\[E(X + Y) = \\sum_i \\sum_j (x_i + y_j) P(x_i, y_j) =\\]\\[= \\sum_i \\sum_j x_iP(x_i, y_j) + \\sum_i \\sum_j y_j P(x_i, y_j) = \\sum_i x_i \\sum_j P(x_i, y_j) + \\sum_j y_j \\sum_i  P(x_i, y_j) = \\]\nSumming joint probability distribution values one variable removes variable, \\(\\sum_j P(x_i, y_j) = P(x_i)\\) \\(\\sum_i P(x_i, y_j) = P(y_j)\\), leave us two separate expected values:\\[= \\sum_i x_i P(x_i) + \\sum_j y_j P(y_j) = E(X) + E(Y)\\]\nHowever, case variance general (using \\(E_X\\) \\(E_Y\\) indicate expected values \\(X\\) \\(Y\\) reduce number parentheses):\\[\\text{Var}(X+Y) = E \\left[ (X+Y)-(E_X+E_Y) \\right]^2 = \\]\n\\[=E[ (X-E_X)^2 +(Y-E_Y)^2 - 2(X-E_X)(Y-E_Y)] =  \\]\n\\[=E (X-E_X)^2 +  E(Y-E_Y)^2 - 2 E[(X-E_X)(Y-E_Y)]  = \\]\nfirst two terms respective variances, third term called covariance \\(X\\) \\(Y\\):\\[= \\text{Var}(X) + \\text{Var}(Y) - 2 \\text{Cov}(X,Y) \\]\nCovariance describes much two random variables vary together, precisely, much deviate respective means direction. Thus reasonable think two independent random variables covariance 0, demonstrated follows:\\[E[(X-E_X)(Y-E_Y)] = E(XY) - E_Y E_X - E_YE_X + E_XE_Y = E(XY) - E_X E_Y  \\]can write expression expectation random variable comprised pairs values \\(X\\) \\(Y\\), using fact two independent random variables, \\(P(x_i,y_j) = P(x_i)P(y_j)\\) values \\(x_i\\) \\(y_j\\):\\[E(XY) = \\sum_i \\sum_j x_iy_j P(x_i,y_j) = \\sum_i x_i P(x_i) \\sum_j y_j P(y_j) = E_X E_Y\\]\ncalculation two continuous random variables analogous, integrals instead sums.demonstrates covariance two independent random variables 0, thus variance sum two independent random variables sum two separate variables.Example: property variance often used analysis noise error data. commonly assumed least squares fitting noise data independent signal model underlying data. foundation statements like “linear regression explains 80% variance data.”","code":""},{"path":"distributions-and-their-properties.html","id":"probability-distribution-examples-discrete","chapter":"Lecture 5 Distributions and their properties","heading":"5.3 Probability distribution examples (discrete)","text":"following examples distributions random variables discrete values. first two finite support (finitely many values) second two infinite support.","code":""},{"path":"distributions-and-their-properties.html","id":"uniform","chapter":"Lecture 5 Distributions and their properties","heading":"5.3.1 Uniform","text":"simplest probability distribution every value probability (one sometimes called “purely random” even though random variable distribution just random). probability distribution uniform random variable \\(n\\) values \\(P(x) = 1/n\\) value \\(x\\).Exercise: experiment low high values see expectation variance depend . Can postulate relationship without looking ?","code":"\nlow <- 0 # minimum value\nhigh <- 10 # maximum value\nvalues <- low:high # vector of discrete values of the RV\nnum <- length(values)\nprobs <- rep(1 / num, num) # uniform mass function vector\nbarplot(probs, names.arg = values, xlab = 'values', ylab = 'probability',\n        main = paste(\"uniform  distribution on integers from \", low, \"to \", high))\nunif.exp <- sum(values*probs)\npaste(\"The expected value of uniform distribution is\", unif.exp)## [1] \"The expected value of uniform distribution is 5\"\nunif.var <- sum((unif.exp - values)^2*probs)\npaste(\"The variance of uniform distribution is\", unif.var)## [1] \"The variance of uniform distribution is 10\""},{"path":"distributions-and-their-properties.html","id":"binomial","chapter":"Lecture 5 Distributions and their properties","heading":"5.3.2 Binomial","text":"Binary Bernoulli trials two discrete outcomes (mutant/wild-type, win/lose, etc.). number “successes” sequence \\(n\\) independent binary trials probability success \\(p\\) described binomial distribution.Exercise: Try different values \\(n\\) \\(p\\) postulate relationship expectation variance.","code":"\nn <- 10 # the number of trials\np <- 0.3 # the probability of success in one trial\nvalues <- 0:n # vector of discrete values of the binomial\nprobs <- dbinom(values, n, p)\nbarplot(probs, names.arg = values, xlab = 'values', ylab = 'probability',\n        main = paste(\"binomial distribution with n=\", n, \"and p=\", p))\nbin.exp <- sum(values*probs)\npaste(\"The expected value of binomial distribution is\", bin.exp)## [1] \"The expected value of binomial distribution is 3\"\nbin.var <- sum((bin.exp - values)^2*probs)\npaste(\"The variance of binomial distribution is\", bin.var)## [1] \"The variance of binomial distribution is 2.1\""},{"path":"distributions-and-their-properties.html","id":"geometric","chapter":"Lecture 5 Distributions and their properties","heading":"5.3.3 Geometric","text":"random variable first “success” string independent binary trials distribution describes probability non-negative value. may pretty intuitive since trials probability success, distribution geometric (exponential) form—try figure exact formula probability density without looking !Exercise: Calculate expectations variances different values \\(p\\) report related.","code":"\np <- 0.3 # the probability of success\nlow <- 0 # minimum value\nhigh <- 20 # maximum value\nvalues <- low:high # vector of discrete values of the RV\nprobs <- dgeom(values, p)\nbarplot(probs, names.arg = values, xlab = 'values', ylab = 'probability', main = paste(\"geometric distribution with p=\", p))\ngeom.exp <- sum(values*probs)\npaste(\"The expected value of geometric distribution is\", geom.exp)## [1] \"The expected value of geometric distribution is 2.32030059650472\"\ngeom.var <- sum((geom.exp - values)^2*probs)\npaste(\"The variance of geometric distribution is\", geom.var)## [1] \"The variance of geometric distribution is 7.52697882945385\""},{"path":"distributions-and-their-properties.html","id":"poisson","chapter":"Lecture 5 Distributions and their properties","heading":"5.3.4 Poisson","text":"Suppose discrete process occurs average rate \\(\\lambda\\), describes expected number occurrences events unit time. Poisson random variable number occurrences, distribution describes probability non-negative value.Exercise: Calculate expectations variances different values \\(\\lambda\\) report related.","code":"\nlow <- 0 # minimum value\nhigh <- 20 # maximum value\nlambda <- 10 # Poisson rate\nvalues <- low:high # vector of discrete values of the RV\nprobs <- dpois(values, lambda)\nbarplot(probs, names.arg = values, xlab = 'values', ylab = 'probability',\n        main = paste(\"Poisson distribution with lambda=\", lambda))\npois.exp <- sum(values*probs)\npaste(\"The expected value of Poisson distribution is\", pois.exp)## [1] \"The expected value of Poisson distribution is 9.96545658024143\"\npois.var <- sum((pois.exp - values)^2*probs)\npaste(\"The variance of Poisson distribution is\", pois.var)## [1] \"The variance of Poisson distribution is 9.77875058489889\""},{"path":"distributions-and-their-properties.html","id":"probability-distribution-examples-continuous","chapter":"Lecture 5 Distributions and their properties","heading":"5.4 Probability distribution examples (continuous)","text":"following examples continuous variables calculate means variances directly density function. One way produce sample using random number generator calculate mean variance sample.","code":""},{"path":"distributions-and-their-properties.html","id":"uniform-1","chapter":"Lecture 5 Distributions and their properties","heading":"5.4.1 Uniform","text":"continuous equivalent discrete uniform distribution.Exercise: experiment width interval see affects expectation variance.","code":"\nlow <- 0 # minimum value\nhigh <- 10 # maximum values\nnumber <- 100\nvalues <- seq(low, high, length.out = number) # vector of discrete values of the RV\nprobs <- dunif(values, min=low, max = high)\nplot(values, probs, t='l', xlab = 'values', ylab = 'density',\n        main = paste(\"Uniform distribution on interval from \", low, \"to \", high))\nn <- 1000 # sample size\nunif.sample <- runif(n, low, high) # generate sample\nunif.exp <- mean(unif.sample)\npaste(\"The expected value of uniform distribution is\", unif.exp)## [1] \"The expected value of uniform distribution is 5.05435579599114\"\nunif.var <- var(unif.sample)\npaste(\"The variance of uniform distribution is\", unif.var)## [1] \"The variance of uniform distribution is 8.10399628496118\""},{"path":"distributions-and-their-properties.html","id":"exponential","chapter":"Lecture 5 Distributions and their properties","heading":"5.4.2 exponential","text":"random variable describes length time independent discrete events occurring certain rate, like saw Poisson distribution.Exercise: relationship rate expectation variance?","code":"\nlow <- 0 # minimum value\nhigh <- 20 # maximum values\nnumber <- 100\nr <- 0.5\nvalues <- seq(low,high,length.out = number) # vector of discrete values of the RV\nprobs <- dexp(values, r)\nplot(values, probs, t='l', xlab = 'values', ylab = 'density',\n        main = paste(\"Exponential distribution with rate=\", r))\nn <- 1000 # sample size\nexp.sample <- rexp(n, r) # generate sample\nexp.exp <- mean(exp.sample)\npaste(\"The expected value of exponential distribution is\", exp.exp)## [1] \"The expected value of exponential distribution is 2.04254779565838\"\nexp.var <- var(exp.sample)\npaste(\"The variance of exponential distribution is\", exp.var)## [1] \"The variance of exponential distribution is 4.16109958923049\""},{"path":"distributions-and-their-properties.html","id":"normal-distribution","chapter":"Lecture 5 Distributions and their properties","heading":"5.4.3 normal distribution","text":"normal distribution, sometimes written \\(N(\\mu, \\sigma)\\) comes everywhere (e.g., limit Poisson distribution large \\(n\\)). two parameters simply mean standard deviation. reason ubiquity sum large number independent random variables converges normal, formalized Central Limit Theorem:set \\(n\\) IID random variables \\(\\{X_i\\}\\) mean \\(\\mu\\) standard deviation \\(\\sigma\\), sample mean \\(\\bar X_n\\) property:\n\\[\n\\lim_{n \\\\infty} = \\frac{\\bar X_n - \\mu}{\\sigma} = N(0,1)\n\\]\n\\(N(0,1)\\) stands normal distribution mean 0 standard deviation 1.","code":"\nlow <- 0 # minimum value\nhigh <- 10 # maximum values\nnumber <- 100\nmu <- 5\nsigma <- 0.5 \nvalues <- seq(low,high,length.out = number) # vector of discrete values of the RV\nprobs <- dnorm(values, mu, sigma)\nplot(values, probs, t='l',xlab = 'values', ylab = 'density',\n        main = paste(\"Normal distribution with mean=\", mu, \"and sigma=\", sigma))\nn <- 1000 # sample size\nnorm.sample <- rnorm(n, mu, sigma) # generate sample\nnorm.exp <- mean(norm.sample)\npaste(\"The expected value of normal distribution is\", norm.exp)## [1] \"The expected value of normal distribution is 4.99123436672613\"\nnorm.var <- var(norm.sample)\npaste(\"The variance of normal distribution is\", norm.var)## [1] \"The variance of normal distribution is 0.256197544658377\""},{"path":"distributions-and-their-properties.html","id":"application-of-normal-distribution-confidence-intervals","chapter":"Lecture 5 Distributions and their properties","heading":"5.5 Application of normal distribution: confidence intervals","text":"important use normal distribution estimation means, normal distribution describes sampling distributions means IID samples. mean sampling distribution mean population distribution sampled, standard deviation called standard error related standard deviation population \\(\\sigma_X\\) follows: \\(\\sigma_{SE} = \\sigma/n\\), \\(n\\) sample size.Exercise: Try using different distributions see sample means still converge normal distribution.following script calculates confidence interval based sample.Exercise: Modify script report whether confidence interval captures true mean. Use loop structure (script ) generate 1000 sample means report many within theoretical confidence interval. match fraction expect significance level? Try different significance levels sample sizes report discover.","code":"\nnumsamples <- 1000\nsize <- 100\n# compute mean for different samples\nsamplemeans <- replicate(n = numsamples, mean(sample(0:10, size, replace = TRUE)))\nbreak_points <- seq(min(samplemeans), max(samplemeans), \n                    (max(samplemeans) - min(samplemeans)) / 20)\nhist(samplemeans, breaks = break_points, freq = FALSE, \n     cex.axis = 1.5, cex.lab = 1.5,\n     main= '1000 means of samples of size 100')\nsigma <- 10 / sqrt(12) / sqrt(size)\nmu <- 5\nrange <- seq(min(samplemeans), max(samplemeans), sigma / 100)\nlines(range, \n      dnorm(range, mu, sigma),\n      t = 'l', lwd = 3, col = 2, lty = 1, cex.axis = 1.5, cex.lab = 1.5)\n# Computing confidence intervals\nqnorm(0.5) # the value that divides the density function in two## [1] 0\nqnorm(0.95) # the value such that 95% of density is to its left ## [1] 1.644854\nsize <- 100 # sample size\nalpha <- 0.95 # significance level\nsample <- runif(size)\ns <- sd(sample) / sqrt(size) # standard error\nz <- qnorm((1 - alpha) / 2) # z-value\nleft <- mean(sample) + s * z\nright <- mean(sample) - s * z\nprint(right)## [1] 0.5316953\nprint(left)## [1] 0.4217696"},{"path":"distributions-and-their-properties.html","id":"identifying-type-of-distribution-in-real-data","chapter":"Lecture 5 Distributions and their properties","heading":"5.6 Identifying type of distribution in real data","text":"Let us consider penguin data set :simple way visualize distribution plot histogram: data binned, height bin represents counts (frequencies). histograms distributions flipper lengths species penguins separated sex:histograms flipper lengths separated species:decide systematically distributions closer theoretical distribution via Quantile-Quantile (QQ) plot, plots quantile value sample quantiles given distribution best-fit parameters. data follow distribution closely, find points lying identity line. example, compare data set drawn normal random number generator normal distribution:contrast simulated data, data points black line attempts capture quite different identity line (red). means distribution normal, can seen histograms flipper lengths grouped sex.","code":"\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nstr(penguins)## tibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n##  $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n##  $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n##  $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n##  $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n##  $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n##  $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n##  $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\nggplot(penguins) +\n  aes(x = flipper_length_mm, color = sex, fill=sex) + geom_histogram(alpha = 0.5, position = \"identity\")## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.## Warning: Removed 2 rows containing non-finite values (stat_bin).\nggplot(penguins) +\n  aes(x = flipper_length_mm, color = species, fill=species) + geom_histogram(alpha = 0.5, position = \"identity\")## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.## Warning: Removed 2 rows containing non-finite values (stat_bin).\nlibrary(fitdistrplus)## Loading required package: survival\ntest_data <- tibble(x = rnorm(n = 500, mean = 3, sd = 1.5))\n# example: find best-fitting Normal\nmy_normal <- fitdistr(test_data$x, densfun = \"normal\")\n# note the slight discrepancies\nprint(my_normal)##       mean          sd    \n##   3.00835546   1.47477339 \n##  (0.06595387) (0.04663643)\nggplot(test_data, aes(sample = x)) +\n  stat_qq(distribution = qnorm, dparams = my_normal$estimate) +\n  stat_qq_line(distribution = qnorm, dparams = my_normal$estimate) +\n  geom_abline(intercept = 0, slope = 1, linetype = 2, col = \"red\") +\n  ggtitle(\"Q-Q plot assuming best-fitting Normal distribution\")\ndataset <- penguins %>%  dplyr::filter(sex == 'female') %>% drop_na() %>%  dplyr::select(flipper_length_mm) \nmy_normal <- fitdistr(x = as_vector(dataset), densfun = \"normal\")\n# note the slight discrepancies\nprint(my_normal)##       mean           sd     \n##   197.3636364    12.4628373 \n##  (  0.9702306) (  0.6860566)\nggplot(dataset, aes(sample = flipper_length_mm)) +\n  stat_qq(distribution = qnorm, dparams = my_normal$estimate) +\n  stat_qq_line(distribution = qnorm, dparams = my_normal$estimate) +\n  geom_abline(intercept = 0, slope = 1, linetype = 2, col = \"red\") +\n  ggtitle(\"Q-Q plot assuming best-fitting Normal distribution of flipper lenth for female penguins\")\ndataset <- penguins %>%  dplyr::filter(sex == 'male') %>% drop_na() %>%  dplyr::select(flipper_length_mm) \nmy_normal <- fitdistr(x = as_vector(dataset), densfun = \"normal\")\n# note the slight discrepancies\nprint(my_normal)##       mean           sd     \n##   204.5059524    14.5045137 \n##  (  1.1190475) (  0.7912861)\nggplot(dataset, aes(sample = flipper_length_mm)) +\n  stat_qq(distribution = qnorm, dparams = my_normal$estimate) +\n  stat_qq_line(distribution = qnorm, dparams = my_normal$estimate) +\n  geom_abline(intercept = 0, slope = 1, linetype = 2, col = \"red\") +\n  ggtitle(\"Q-Q plot assuming best-fitting Normal distribution of flipper lenth for male penguins\")"},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"Lecture 6 Hypothesis testing","heading":"Lecture 6 Hypothesis testing","text":"large number scientific questions can expressed hypothesis test—essentially yes/question, “two samples drawn distributions mean?” “frequency allele population greater 0.1?” Several tests developed, specific type question mind. dangerous tendency view statistics collection tests, practice plugging data set correct test, expecting test spit correct decision. purpose lesson demonstrate using interpreting statistical tests requires careful thinking avoid serious errors.","code":""},{"path":"hypothesis-testing.html","id":"test-results-vs.-the-truth","chapter":"Lecture 6 Hypothesis testing","heading":"6.1 Test results vs. the truth","text":"statistical test begins stating null hypothesis, usually one expected, shows effect: example, two samples come distribution mean, rare allele frequency less 0.1. One may state alternative hypothesis explicitly, although ’s usually logical converse null, .e., two samples different population means, allele frequency greater 0.1.hypothesis stated, data collected used test hypothesis. default, null hypothesis assumed true, test assesses whether data provide sufficient evidence null hypothesis—case null hypothesis rejected. adversarial relationship: either data knock hypothesis, else fail . Standard terminology reflects somewhat counter-intuitive setup: rejecting null hypothesis called positive test result, rejecting called negative result.fundamental assumption process truth value hypothesis set prior collection data. example, one observe genomes, frequency allele known exactly, truth exists prior hypothesis testing. typically can observe sample (entire universe data), might end erroneously rejecting null hypothesis fact true, rejecting fact false. possible outcomes test can organized table:values top describe truth status hypothesis, decisions left column result using data test hypothesis. Note: words false true describing test result refer hypothesis, whether result correct! example, frequency allele 0.09 test hypothesis frequency less 0.1 resulted rejecting hypothesis, false positive result (null hypothesis true test rejected .)","code":""},{"path":"hypothesis-testing.html","id":"types-of-errors","chapter":"Lecture 6 Hypothesis testing","heading":"6.2 Types of errors","text":"mentioned , sometimes hypothesis test makes wrong decision, called error. two different kinds errors: rejecting true null hypothesis, called Type error, rejecting false null hypothesis, called Type II error.Example: case testing mean: samples taken distributions mean, hypothesis rejected, called false positive (Type error). samples come distributions different means, hypothesis rejected, called false negative (Type II error.)scientist, rather make Type error (make erroneous discovery), Type II error (fail make discovery)?","code":""},{"path":"hypothesis-testing.html","id":"test-parameters-and-p-values","chapter":"Lecture 6 Hypothesis testing","heading":"6.3 Test parameters and p-values","text":"sensitivity test probability obtaining positive result, given false hypothesis; specificity test probability obtaining negative result, given true hypothesis. Type error rate probability obtaining positive result, given true hypothesis (complementary specificity), Type II error rate probability obtaining negative result, given false hypothesis (complementary sensitivity).four parameters (rates) binary test summarized follows:\n\\[\\text{Sen} = \\frac{TP}{TP+FN};  \\; \\text{Spec} = \\frac{TN}{TN+FP}\\]\n\\[\\text{FPR} = \\frac{FP}{TN+FP};  \\; \\text{FNR} = \\frac{FN}{TP+FN}\\]\nnotation TP, FP, etc. represents frequency count true positives, false positives, etc., large number experiments known truth status hypothesis.Knowledge sensitivity specificity determine Type Type II error rates test since complementary events.course, desirable test sensitive (reject false null hypotheses, detect disease, convict guilty defendants) specific (reject true null hypotheses, correctly identify healthy patients, acquit innocent defendants), test perfect, sometimes makes wrong decision. statistical inference comes play: given information parameters, statistician can calculate error rate making different decisions.probability given data set produced model null hypothesis called p-value test. precisely:given data set \\(D\\) null hypothesis \\(H_0\\), p-value probability obtaining result far expectation farther observed data, given null hypothesis.p-value used, misused, even abused quantity statistics, please think carefully definition. One reason notion frequently misused tempting conclude p-value probability null hypothesis true, based data. case! definition opposite direction conditionality—assume null hypothesis true, based calculate probability obtaining pattern extreme extreme observed data. way (according classical “frequentist” statistics) assigning probability truth hypothesis, result experiment.Typically, one sets critical threshold bounding probability making Type error test “small” number (often, \\(\\alpha = 0.05\\) \\(0.01\\)), calls result test “significant” p-value less \\(\\alpha\\).example, consider samples size \\(n\\) taken two normal distributions (unobserved means \\(\\mu_1\\), \\(\\mu_2\\)). can generate data:use Student’s t-test probe whether means differ:Exercise: Can detect “significant difference means” (assuming \\(\\alpha = 0.05\\))? take much larger sample? difference means pronounced?","code":"\ngenerate_samples <- function(n, mu1, mu2){\n  return(data.frame(sample1 = rnorm(n = n, mean = mu1, sd = 1),\n               sample2 = rnorm(n = n, mean = mu2, sd = 1)))\n}\n\nmy_sample <- generate_samples(1000, 1, 1.01)\n# two-tailed (diff in means = 0)\n# Student's (assumes equal variances)\n# (for Welch's t-test, var.equal = FALSE)\nt.test(my_sample$sample1, \n       my_sample$sample2,\n       var.equal = TRUE)## \n##  Two Sample t-test\n## \n## data:  my_sample$sample1 and my_sample$sample2\n## t = -0.46549, df = 1998, p-value = 0.6416\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -0.1072355  0.0660945\n## sample estimates:\n## mean of x mean of y \n##   1.00157   1.02214"},{"path":"hypothesis-testing.html","id":"multiple-comparisons","chapter":"Lecture 6 Hypothesis testing","heading":"6.4 Multiple comparisons","text":"produce several samples? E.g., measure difference males females reflectance birds several locations? Suppose fact reflectance male female (\\(\\mu_1 = \\mu_2 = 1\\)), location capture measure 10 males 10 females, repeat across 2500 locations.First, let’s write little function returns p-values t-testand now simulate data:many times detect “significant difference reflectance” setting \\(\\alpha = 0.05\\) (even though know males females sampled distribution)?get number “significant” tests \\(2500 \\cdot 0.05 = 125\\). fact, distribution p-values data sampled null hypothesis approximately uniform:means performing multiple tests, turn find “significant” differences even none. , better summarized xkcd:Exercise: happens distribution p-values means quite different (e.g., \\(\\mu_1 = 1\\), \\(\\mu_2 = 0.9\\))?","code":"\nget_p_value_t_test <- function(my_sample){\n  test_results <- t.test(my_sample$sample1, \n                         my_sample$sample2, \n                         var.equal = TRUE)\n  return(test_results$p.value)\n}\npvalues <- replicate(n = 2500, \n                     expr = get_p_value_t_test(generate_samples(10, 1, 1)))\nsum(pvalues < 0.05)## [1] 135\nhist(pvalues)"},{"path":"hypothesis-testing.html","id":"corrections-for-multiple-comparisons","chapter":"Lecture 6 Hypothesis testing","heading":"6.5 Corrections for multiple comparisons","text":"main approach deal problem multiple comparisons adjust p-values. example, Bonferroni correction one consider significant test results whose associated p-value \\(\\leq \\alpha / n\\), \\(n\\) number tests performed (equivalently, redefine p-values \\(p' = \\min(p n, 1)\\). Clearly, correction becomes overly conservative number tests large. example, biology:Gene expression typical microarray experiment, contrast differential expression tens thousands genes treatment control tissues.Gene expression typical microarray experiment, contrast differential expression tens thousands genes treatment control tissues.GWAS Genomewide Association Studies want find SNPs associated given phenotype. common test tens thousands even millions SNPs significant associations.GWAS Genomewide Association Studies want find SNPs associated given phenotype. common test tens thousands even millions SNPs significant associations.Identifying binding sites Identifying candidate binding sites transcriptional regulator requires scanning whole genome, yielding tens millions tests.Identifying binding sites Identifying candidate binding sites transcriptional regulator requires scanning whole genome, yielding tens millions tests.funniest example problem fMRI dead salmon: dead salmon “shown series photographs depicting human individuals social situations specified emotional valence. salmon asked determine emotion individual photo must experiencing.” researchers showed multiple comparisons accounted , one detect cluster active voxels brain, cluster-level significance p = 0.001.widespread use GWAS techniques trying find needle haystack led development many interesting techniques. interesting account.Adjusting p-values R:","code":"\noriginal_pvalues <- c(0.01, 0.07, 0.1, 0.44)\np.adjust(original_pvalues, method = \"bonferroni\")## [1] 0.04 0.28 0.40 1.00"},{"path":"hypothesis-testing.html","id":"two-problems-with-science","chapter":"Lecture 6 Hypothesis testing","heading":"6.6 Two problems with science","text":"","code":""},{"path":"hypothesis-testing.html","id":"selective-reporting","chapter":"Lecture 6 Hypothesis testing","heading":"6.6.1 Selective reporting","text":"seen setting \\(\\alpha = 0.05\\) means going make false discoveries rate. science, prefer publishing positive results—negative results difficult publish attract little attention. Suppose 20 research groups around world set test hypothesis, false. good chance least one group reject null hypothesis, pursue publication “discovery.” tendency put negative studies files drawer forget causes called publication bias (aka selective reporting): favoring positive results negative ones, greatly increase chance conclusions wrong. Note cause results paper largely impossible reproduce, reproducibility crisis sciences partially due selective reporting.","code":""},{"path":"hypothesis-testing.html","id":"p-hacking","chapter":"Lecture 6 Hypothesis testing","heading":"6.6.2 P-hacking","text":"One big violation good experimental design known p-value “fishing” (p-hacking): repeating experiment, increasing sample size, p-value desired threshold, stopping experiment. Using defective design dramatically lowers likelihood result true positive. course actual fraud, fudging data, contributes bogus results.insidious cousin p-hacking dubbed Andrew Gelman “garden forking paths” paper. issue arises complex problems multi-variable noisy datasets (aren’t interesting ones like ?) Essentially, many choices degrees freedom problem, easy convince choice made (data cleaning, parameter combinations, etc.) correct one gives strongest results. Without clearly stated hypothesis, experimental design, data processing details prior data collection, enchanted garden can lead even well-intentioned researcher astray.","code":""},{"path":"hypothesis-testing.html","id":"readings","chapter":"Lecture 6 Hypothesis testing","heading":"6.7 Readings","text":"Good readings related issues:Published Research Findings FalseDecline effectThe truth wears offThe Extent Consequences P-Hacking ScienceA manifesto reproducible scienceSpoiled Science","code":""},{"path":"hypothesis-testing.html","id":"how-to-fool-yourself-with-p-hacking-and-possibly-get-fired","chapter":"Lecture 6 Hypothesis testing","heading":"6.8 How to fool yourself with p-hacking (and possibly get fired!)","text":"going try hand p-hacking, show easy get fooled sufficiently large complex data set. file data/medals.csv contains total number medals won Olympic games (Summer Winter) country, sport gender. simple, reasonable (?) hypothesis: amount money available Olympic teams finite, whenever country invests male team, detriment female team. test hypothesis, measure whether number medals won national female team year negatively correlated number medals won male team.Let’s read data, take peak:First, let’s see whether hypothesis works whole data:correlation positive: medals men tend correspond medals women. correlation strong, “significant?” can run correlation test:Indeed! confidence intervals far 0: correlation definitely positive. give ? course ! Just jelly beans, can p-hack way glory subsetting data. going test discipline independently, see whether can get robustly negative correlation discipline. serious scientists, going consider disciplines least 50 data points, avoid results due small sample sizes. Let’s write code:Now let’s see whether highly significant negative correlations:Let’s plot results convince strong:’s ! rush publish results? quite: p-hacked way highly significant results, correct number tests ’ve made, selectively reporting strong results. fact, can something simple convince results make much sense: just run code , reporting significant positive correlations…can see ’ve got number sports testing significant positive correlation! Bonus question figure skating?","code":"\nlibrary(tidyverse)\ndt <- read_csv(\"data/medals.csv\")## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   NOC = col_character(),\n##   Year = col_double(),\n##   Sport = col_character(),\n##   F = col_double(),\n##   M = col_double()\n## )\ndt## # A tibble: 6,915 × 5\n##    NOC    Year Sport         F     M\n##    <chr> <dbl> <chr>     <dbl> <dbl>\n##  1 AFG    2008 Taekwondo     0     1\n##  2 AFG    2012 Taekwondo     0     1\n##  3 AHO    1988 Sailing       0     1\n##  4 ALG    1984 Boxing        0     2\n##  5 ALG    1992 Athletics     1     0\n##  6 ALG    1992 Boxing        0     1\n##  7 ALG    1996 Athletics     0     1\n##  8 ALG    1996 Boxing        0     2\n##  9 ALG    2000 Athletics     1     3\n## 10 ALG    2000 Boxing        0     1\n## # … with 6,905 more rows\ncor(dt$F, dt$M)## [1] 0.1651691\ncor.test(dt$F, dt$M)## \n##  Pearson's product-moment correlation\n## \n## data:  dt$F and dt$M\n## t = 13.924, df = 6913, p-value < 2.2e-16\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.1421521 0.1880075\n## sample estimates:\n##       cor \n## 0.1651691\ndt <- dt %>% group_by(Sport) %>% mutate(sample_size = n()) %>% ungroup()\ncorrelations <- dt %>% \n  filter(sample_size >= 50) %>% \n  group_by(Sport) %>% \n  summarise(cor = cor(`M`, `F`), \n            pvalue = cor.test(`M`, `F`)$p.value) %>% \n  ungroup() \nmy_results <- correlations %>% filter(pvalue < 0.05, cor < 0)\nmy_results## # A tibble: 9 × 3\n##   Sport                cor   pvalue\n##   <chr>              <dbl>    <dbl>\n## 1 Basketball        -0.579 7.86e- 8\n## 2 Football          -0.796 6.75e-23\n## 3 Handball          -0.810 5.28e-16\n## 4 Hockey            -0.585 4.16e- 9\n## 5 Ice Hockey        -0.302 8.10e- 3\n## 6 Modern Pentathlon -0.561 3.57e- 8\n## 7 Volleyball        -0.545 2.18e- 6\n## 8 Water Polo        -0.688 3.41e-14\n## 9 Weightlifting     -0.138 2.33e- 2\nggplot(dt %>% inner_join(my_results)) + \n  aes(x = `M`, y = `F`) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  facet_wrap(~Sport, scales = \"free\")\nmy_results <- correlations %>% filter(pvalue < 0.05, cor > 0)\nggplot(dt %>% inner_join(my_results)) + \n  aes(x = `M`, y = `F`) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  facet_wrap(~Sport, scales = \"free\")"},{"path":"likelihood-and-bayes.html","id":"likelihood-and-bayes","chapter":"Lecture 7 Likelihood and Bayes","heading":"Lecture 7 Likelihood and Bayes","text":"understand difference likelihood probabilitymaximum likelihood estimationcalculate positive predictive value hypothesis testinterpret results Bayesian inference","code":""},{"path":"likelihood-and-bayes.html","id":"likelihood-and-estimation","chapter":"Lecture 7 Likelihood and Bayes","heading":"7.1 Likelihood and estimation","text":"","code":""},{"path":"likelihood-and-bayes.html","id":"likelihood-vs.-probability","chapter":"Lecture 7 Likelihood and Bayes","heading":"7.1.1 likelihood vs. probability","text":"everyday English, probability likelihood synonymous. probability statistics, however, two distinct, although related, concepts. definition likelihood based notion conditional probability defined week 2, applied data set particular probability model \\(M\\):\\[ \nL(M \\ \\ \\vert \\ \\ D) = P(D \\ \\ \\vert \\ \\ M)\n\\]model based set assumptions allow us calculate probabilities outcomes random experiment, typically random variable well-defined probability distribution function.Example. \\(M\\) may represent binomial random variable, based assumptions data strings \\(n\\) independent binary outcomes set probability \\(p\\) “success.” following formula probability obtaining \\(k\\) successes:\\[ \nP(k ; n , p) =  {n \\choose k} p^k (1-p)^{n-k}\n\\]Suppose think fair coin flip ten times obtain 4 heads 6 tails. likelihood model (binomial random variable \\(p=0.5\\) \\(n=10\\)) based data (\\(k=4\\)) :\\[\nL(p=0.5, n=10 \\ \\vert \\ k=4 ) = P(k =4 \\ \\vert \\ n=10 , p=0.5) = {10 \\choose 4} 0.5^4 (0.5)^{6}\n\\]calculate precisely, easiest use R function dbinom():likelihood data set produced fair coin 20.5%.certainly looks like probability — fact calculated probability distribution function, call likelihood? two fundamental differences two, one mostly abstract, grounded.First, model (model parameters) random variable, comes assumption made heads, outcome random process. may seem abstract, almost philosophical distinction, go assigning probabilities models one can come ? vary person person, one may prefer use binomial random variable, another prefers Poisson? see can get dicey think terms traditional “frequency outcomes” framework probability.Second, quantitatively relevant, likelihoods satisfy fundamental axiom probability: add one. Remember probabilities defined sample space outcomes random experiment. Likelihoods apply models parameters, usually uncountably many models - fact ’s possible even describe possible models vague terms! Even agree ’re evaluating one type model, e.g. binomial random variable, likelihood parameter \\(p\\) work like probability, non-zero likelihood value \\(p\\) (technically, coin degree unfairness!) adding likelihoods results infinity.","code":"\nprint(dbinom(4,10,0.5))## [1] 0.2050781"},{"path":"likelihood-and-bayes.html","id":"maximizing-likelihood","chapter":"Lecture 7 Likelihood and Bayes","heading":"7.1.2 maximizing likelihood","text":"One common applications likelihood find model model parameters give highest likelihood based data, call best statistical estimate. symbols use discussion:\\(D\\): observed data\\(\\theta\\): free parameter(s) statistical model\\(L(\\theta \\ \\vert \\ D)\\): likelihood function, read “likelihood \\(\\theta\\) given data”\\(\\hat{\\theta}\\): maximum-likelihood estimates (m.l.e.) parameters","code":""},{"path":"likelihood-and-bayes.html","id":"discrete-probability-distributions","chapter":"Lecture 7 Likelihood and Bayes","heading":"7.1.3 discrete probability distributions","text":"simplest case probability distribution function takes discrete values. , likelihood \\(\\theta\\) given data simply probability obtaining data parametrizing model parameters \\(\\theta\\):\\[L(\\theta \\ \\vert \\ D) = P(X = D \\ \\vert \\ \\theta)\\]Finding m.l.e. \\(\\theta\\) simply means finding value(s) maximizing probability obtaining given data model. cases likelihood function simple algebraic form, can find maximum value using classic method taking derivative setting zero.Example. Let’s go back binomial example. Based data set 4 heads 10 coin tosses, maximum likelihood estimate probability head \\(p\\)? range values \\(p\\) 0 1, since functional expression \\(P(k=4 ; n=10, p)\\) (see ) can plot using dbinom() function:’s probably surprising maximum likelihood function occurs \\(p=0.4\\), observed fraction heads! Using magic derivatives, can show data set \\(k\\) success \\(n\\) trials, maximum likelihood value \\(p\\) \\(\\hat p = k/n\\):\\[ \n\\begin{aligned}\nL(p  \\ \\vert \\ n, k) &= {n \\choose k} p^k (1-p)^{n-k} \\\\ \\hline\nL'(p | n, k ) &= {n \\choose k}\\left [ kp^{k-1}(1-p)^{n-k} - (n-k) (1-p)^{n-k-1}p^k \\right] \\\\\n&={n \\choose k} p^{k-1} (1-p)^{n-k-1} \\left [ k(1-p) - (n-k)p \\right] = 0 \\\\\n\\hline\nk(1-p) &= (n-k)p \\\\\n\\hat p &= k/n\n\\end{aligned}\n\\]","code":"\nlibrary(tidyverse)\nn <- 10\nk <- 4\npl <- ggplot(data = data.frame(x = 0, y = 0)) + xlim(c(0,1))\nlike_fun <- function(p) {\n  lik <- dbinom(k, n, p)\n  return(lik)\n}\npl <- pl + stat_function(fun = like_fun) + \n  xlab('probability of success (p)') + \n  ylab('likelihood') +\n  geom_vline(xintercept = 0.4, linetype='dotted', color = 'red')\nshow(pl)"},{"path":"likelihood-and-bayes.html","id":"continuous-probability-distributions","chapter":"Lecture 7 Likelihood and Bayes","heading":"7.1.4 continuous probability distributions","text":"definition complex continuous variables (\\(P(X = x; \\theta) = 0\\) infinitely many values…). commonly done use density function \\(f(x; \\theta)\\) considering probability obtaining value \\(x \\[x_j, x_j + h]\\), \\(x_j\\) observed data point, \\(h\\) small. :\\[\nL(\\theta \\ \\vert \\ x_j) = \\lim_{h \\0^+} \\frac{1}{h} \\int_{x_j}^{x_j + h} f(x ; \\theta) dx = f(x_j ; \\theta)\n\\]Note , contrary probabilities, density values can take values greater 1. , dispersion small, one end values likelihood greater 1 (positive log-likelihoods). fact, likelihood function proportional necessarily equal probability generating data given parameters: \\(L(\\theta\\vert X) \\propto P(X; \\theta)\\).classical statistical estimations based maximizing likelihood function. example, linear regression estimates slope intercept based minimizing sum squares, generally, \\(\\chi\\)-squared statistic. amounts maximizing likelihood underlying model, based assumptions normally distributed independent residuals.","code":""},{"path":"likelihood-and-bayes.html","id":"bayesian-thinking","chapter":"Lecture 7 Likelihood and Bayes","heading":"7.2 Bayesian thinking","text":"formalize process incorporation prior knowledge probabilistic inference going back notion conditional probability introduced week 2. First, multiply sides definition \\(P(B)\\), obtain probability intersection events \\(\\) \\(B\\):\\[P(\\cap B) = P(\\ \\vert \\ B) P(B); \\;  P(\\cap B) = P(B \\ \\vert \\ ) P() \\]Second, can partition sample space two complementary sets, \\(\\) \\(\\bar \\), set \\(B\\) can partitioned two parts, intersect \\(\\) \\(\\bar \\), respectively, probability \\(B\\) \\[P(B) = P(\\cap B) + P( \\bar \\cap B)\\]two formulas together lead important result called law total probability:\\[\nP(B) =  P(B \\ \\vert \\ ) P() + P(B \\ \\vert \\ \\bar )P(\\bar )\n\\]may clear first glance useful: , replaced something simple (\\(P(B)\\)) something much complex right hand side. see formula enables us calculate quantities otherwise accessible.Example: Suppose know probability patient disease 1% (called prevalence disease population), sensitivity specificity test 80%. probability obtaining negative test result randomly selected patient? Let us call \\(P(H) = 0.99\\) probability healthy patient \\(P(D) = 0.01\\) probability diseased patient. :\n\\[ P(Neg) =  P(Neg  \\ \\vert \\  H) P(H) + P(Neg  \\ \\vert \\  D)P(D)  = \\]\n\\[ = 0.8 \\times 0.99 + 0.2 \\times 0.01 = 0.794\\]","code":""},{"path":"likelihood-and-bayes.html","id":"bayes-formula","chapter":"Lecture 7 Likelihood and Bayes","heading":"7.2.1 Bayes’ formula","text":"Take first formula section, expresses probability \\(P(\\cap B)\\) two different ways. Since expressions equal, can combine one equation, dividing sides \\(P(B)\\), obtain ’s known Bayes’ formula:\n\\[ P(\\ \\vert \\ B) = \\frac{P(B \\ \\vert \\ ) P()}{P(B) }\\]Another version Bayes’ formula re-writes denominator using Law total probability :\n\\[\nP(\\ \\vert \\ B) = \\frac{P(B \\ \\vert \\ )P()}{P(B \\ \\vert \\ ) P() + P(B \\ \\vert \\ \\bar )P( \\bar )}\n\\]Bayes’ formula gives us probability \\(\\) given \\(B\\) probabilities \\(B\\) given \\(\\) given \\(-\\), prior (baseline) probability \\(P()\\). enormously useful easy calculate conditionals one way . Among many applications, computes effect test result given sensitivity specificity (conditional probabilities) probability hypothesis true.","code":""},{"path":"likelihood-and-bayes.html","id":"positive-predictive-value","chapter":"Lecture 7 Likelihood and Bayes","heading":"7.2.2 positive predictive value","text":"reality, doctor doesn’t true information patient’s health, rather information test hopefully information population working. Let us assume know rate false positives \\(P(Pos \\ \\vert \\ H\\)) rate false negatives \\(P(Neg \\ \\vert \\  D)\\), well prevalence disease whole population \\(P(D)\\). can use Bayes’ formula answer practical question, test result positive, probability patient actually sick? called positive predictive value test. deep Bayesian fact one make inferences health patient test without prior knowledge, specifically prevalence disease population:\\[ P(D  \\ \\vert \\  Pos) =  \\frac{P(Pos \\ \\vert \\ D)P(D)}{P(Pos \\ \\vert \\ D) P(D) + P(Pos  \\ \\vert \\  H)P(H)}\\]Example. Suppose test 0.01 probability false positive false negatives, overall prevalence disease population 0.02. may surprised epidemiological perspective, positive result far definitive:\\[ P(D  \\ \\vert \\  Pos)  = \\frac{0.99 \\times 0.02}{0.99 \\times 0.02 + 0.01 \\times 0.98} = 0.67 \\]disease rare, even though test quite accurate, going lot false positives (1/3 time) since 98% patients healthy.can also calculate probability patient tests negative actually healthy, called negative predictive value. example, far definitive:\\[ P(H  \\ \\vert \\  Neg)  = \\frac{P(Neg \\ \\vert \\ H)P(H)}{P(Neg \\ \\vert \\ H) P(H) + P(Neg  \\ \\vert \\  D)P(D)} = \\]\\[ = \\frac{0.99 \\times 0.98}{0.99 \\times 0.98 + 0.01 \\times 0.02} =  0.9998\\]\ndisease quite rare population, negative test result almost guaranteed correct. another population, disease prevalent, may case.Bayesian hypothesis testing tree prior probability 0.1Bayesian hypothesis testing tree prior probability 0.01Exercise: Simulate medical testing rolling dice rare disease (1/6 prevalence) common disease (1/2 prevalence), sensitivity specificity 5/6. Compare positive predictive values two cases.","code":""},{"path":"likelihood-and-bayes.html","id":"prosecutors-fallacy","chapter":"Lecture 7 Likelihood and Bayes","heading":"7.2.3 prosecutor’s fallacy","text":"basic principle Bayesian thinking one interpret reliability result, e.g. hypothesis test, without factoring prior probability true. seems like commonsensical concept, often neglected results interpreted various contexts, can lead perilous mistakes.scenario called “prosecutor’s fallacy.” Suppose defendant accused crime, physical evidence collected crime scene matches person (e.g. fingerprint DNA sample), evidence exists connect defendant crime. prosecutor calls expert witness testify fewer one million randomly chosen people match sample. Therefore, argues, overwhelming probability defendant guilty less 1 million chance innocent.spot problem argument?’s fallacy saw medical testing scenario, portrayed xkcd cartoon . prosecutor conflating probability match (positive result) given person innocent, probability person innocent, given match. probability former one million, want know latter! latter depends prior probability person committing crime, investigated detectives: defendant conflict victim never met? opportunity commit crime, different city time? Without information, impossible decide whether ’s likely DNA/fingerprint match false positive (country 300 million, can find 300 false matches everyone database!) true positive.","code":""},{"path":"likelihood-and-bayes.html","id":"reproducibility-in-science","chapter":"Lecture 7 Likelihood and Bayes","heading":"7.2.4 reproducibility in science","text":"2005 John Ioannidis published paper entitled “published research findings false”. paper, can see title, intended provocative, based solidly classic formula Bayes. motivation paper came observation often modern science, big, splashy studies published reproduced verified researchers. behind epidemic questionable scientific work?problem described Ioannidis many others, nutshell, unthinking use traditional hypothesis testing leads high probability false positive results published. paper outlines several ways can occur.often, hypothesis tested resultant p-value less arbitrary threshold (often 0.05, absurdly high number), results published. However, one testing hypothesis low prior probability, positive hypothesis test result likely false positive. often, modern biomedical research involves digging large amount information, like entire human genome, search associations different genes phenotype, like disease. priori unlikely specific gene linked given phenotype, genes specific functions, expressed quite selectively, specific times specific types cells. However, publishing studies results splashy headlines (“Scientists find gene linked autism!”) lot false positive results reported, refuted later, much less publicized studies.Ioannidis performed basic calculations probability published study true (, positive reported result true positive), affected pre-study (prior) probability, number conducted studies hypothesis, level bias. prediction fairly typical scenario (e.g. pre-study probability 10%, ten groups working simultaneously, reasonable amount bias) probability published result correct less 50%. followed another paper [2] investigated 49 top-cited medical research publications decade, looked whether follow-studies replicate results, found significant fraction findings replicated found weaker effects subsequent investigations.","code":""},{"path":"likelihood-and-bayes.html","id":"bayesian-inference","chapter":"Lecture 7 Likelihood and Bayes","heading":"7.3 Bayesian inference","text":"alternative frequentist maximum likelihood approaches modeling biological data, Bayesian statistics seen impressive growth recent years, due improved computational power.heart Bayesian inference application Bayes’ theorem: take model parameters \\(\\theta\\), data \\(D\\). Bayes’ theorem gives us disciplined way “update” belief distribution \\(\\theta\\) ’ve seen data \\(D\\):\\[\nP(\\theta \\ \\vert \\ D) = \\frac{P(D \\ \\vert \\ \\theta) P(\\theta)}{P(D)}\n\\]\n:\\(P(\\theta \\ \\vert \\ D)\\) posterior distribution \\(\\theta\\), .e., updated belief values \\(\\theta\\).\\(P(D \\ \\vert \\ \\theta)\\) likelihood function: \\(P(DX \\ \\vert \\ \\theta) = L(\\theta \\ \\vert \\ D)\\).\\(P(\\theta)\\) prior distribution, .e. belief distribution \\(\\theta\\) seeing data.\\(P(D)\\) called evidence: \\(P(D) = \\int P(D \\ \\vert \\ \\theta) d \\theta\\) (practice, need calculated).","code":""},{"path":"likelihood-and-bayes.html","id":"example-capture-recapture","chapter":"Lecture 7 Likelihood and Bayes","heading":"7.3.1 Example: capture-recapture","text":"well-established method population ecology estimating size population repeatedly capturing tagging number individuals later repeating experiment see many recaptured. Suppose \\(n\\) captured initially \\(k\\) recaptured later. assume probability \\(p\\) recapturing individual individuals. likelihood function , based binomial distribution.\\[\nL(p \\ \\vert \\ k, n) = \\binom{n}{k}p^k (1-p)^{n-k}\n\\]\nmaximum likelihood estimate \\(\\hat{p} = k /n\\). allows estimation total population size \\(P = n_2/\\hat p\\), \\(n_2\\) total number individuals captured second experiment. sophisticated estimators, one reasonable large enough populations.Let us plot likelihood function \\(p\\) case \\(n = 100\\) \\(k = 33\\)Now choose prior. convenience, choose Beta distribution, \\(P(p) = \\text{Beta}(\\alpha, \\beta) = \\frac{p^{\\alpha - 1} (1-p)^{\\beta - 1}}{B(\\alpha, \\beta)}\\), \\(B(\\alpha, \\beta)\\) Beta function, \\(B(\\alpha, \\beta) = \\int_0^1 t^{\\alpha -1} (1-t)^{\\beta - 1} dt\\).Therefore:\\[\n\\begin{aligned}\nP(p \\ \\ \\vert \\ \\ m,n) & \\propto L(p \\ \\ \\vert \\ \\ m,n) P(p) \\\\\n&= \\left(\\binom{n}{m} p^m (1-p)^{n-m} \\right) \\left( \\frac{p^{\\alpha - 1} (1-p)^{\\beta - 1}}{B(\\alpha, \\beta)} \\right)\\\\\n& \\propto p^{m+\\alpha -1} (1-p)^{n-m + \\beta -1} \\\\\n& \\propto \\text{Beta}(m + \\alpha, \\beta + m - n)\n\\end{aligned}\n\\]can explore effect choosing prior posterior. Suppose past seen probabilities close 50%. choose prior \\(\\text{Beta}(10,10)\\) (called “strong” “informative” prior). Let’s see happens posterior:can see posterior “mediates” prior likelihood curve. use weak prior, posterior closer likelihood function:fact posterior depends prior controversial aspect Bayesian inference. Different schools thought treat feature differently (e.g., “Subjective Bayes” interprets priors beliefs seeing data; “Empirical Bayes” relies previous experiments data derive prior; “Objective Bayes” tries derive least-informative prior given data). practice, larger data, cleaner signal, lesser influence prior resulting posterior.","code":"\nlibrary(tidyverse)\nn <- 100\nm <- 33\npl <- ggplot(data = data.frame(x = 0, y = 0)) + xlim(c(0,1))\nlikelihood_function <- function(p) {\n  lik <- choose(n, m) * p^m * (1-p)^(n - m)\n  # divide by the evidence to make into density function\n  return(lik * (n + 1))\n}\npl <- pl + stat_function(fun = likelihood_function)\nshow(pl)\n# a strong prior\nalpha <- 10\nbeta <- 10\nprior_function <- function(p) dbeta(p, alpha, beta)\nposterior_function <- function(p) dbeta(p, alpha + m, beta + n - m)\npl + stat_function(fun = prior_function, colour = \"blue\") + \n  stat_function(fun = posterior_function, colour = \"red\")\n# a weak prior\nalpha <- 1/2\nbeta <- 1/2\npl + stat_function(fun = prior_function, colour = \"blue\") + \n  stat_function(fun = posterior_function, colour = \"red\")"},{"path":"likelihood-and-bayes.html","id":"mcmc","chapter":"Lecture 7 Likelihood and Bayes","heading":"7.3.2 MCMC","text":"type calculation performed feasible simple models, appropriately chosen priors (called “conjugate priors”). complex models, rely simulations. particular, one can use Markov-Chain Monte Carlo (MCMC) sample posterior distribution complex models. briefly, one builds Markov-Chain states represent sets parameters; parameters sampled prior, probability moving one state another proportional difference likelihood. MC converges, one obtains posterior distribution parameters.Bayesian inference used many complex problems, including phylogenetic tree building [5].","code":""},{"path":"likelihood-and-bayes.html","id":"reading","chapter":"Lecture 7 Likelihood and Bayes","heading":"7.4 Reading:","text":"Maximum likelihood estimation scratchMaximum likelihood estimation scratchPhylogenetic Analysis Maximum LikelihoodPhylogenetic Analysis Maximum LikelihoodWhy published scientific studies falseWhy published scientific studies falseContradicted Initially Stronger Effects Highly Cited Clinical ResearchContradicted Initially Stronger Effects Highly Cited Clinical ResearchMrBayesMrBayesQuick Dirty Tree Building RQuick Dirty Tree Building RMark RecaptureMark Recapture","code":""},{"path":"review-of-linear-algebra.html","id":"review-of-linear-algebra","chapter":"Lecture 8 Review of linear algebra","heading":"Lecture 8 Review of linear algebra","text":"GoalsSolving linear equationsBest-fit line multiple data pointsConcepts linearity vector spacesRepresentation vectors multiple basesEigenvalues eigenvectors matrices","code":"\nlibrary(tidyverse) # our friend the tidyverse\nlibrary(ggfortify) "},{"path":"review-of-linear-algebra.html","id":"solving-multivariate-linear-equations","chapter":"Lecture 8 Review of linear algebra","heading":"8.1 Solving multivariate linear equations","text":"Linear algebra intimately tied linear equations, , equations variables multiplied constant terms added together. Linear equations one variable easily solved division, example:\\[\n4x = 20\n\\]solved dividing sides 4, obtaining unique solution \\(x=5\\).situation gets interesting multiple variables involved, multiple equations, example:\\[\n\\begin{aligned}\n4x - 3y &= 5 \\\\\n -x + 2y &= 10\n\\end{aligned}\n\\]multiple ways solve , example solving one equation one variable terms , substituting second equation obtain one-variable problem. general approach involves writing problem terms matrix \\(\\mathbf \\) contains multiplicative constants \\(x\\) \\(y\\) vector \\(\\vec b\\) contains right-hand side constants:\\[\n\\begin{aligned}\n\\mathbf{} = \\begin{pmatrix} 4 & -3 \\\\ -1 & 2 \\end{pmatrix} \\;\\;\\; \\vec{b} = \\begin{pmatrix}5 \\\\ 10 \\end{pmatrix} &\\;\\;\\;\n\\vec{x} = \\begin{pmatrix} x \\\\ y \\end{pmatrix} \\\\\n\\mathbf{} \\vec x = \\vec b &\n\\end{aligned}\n\\]now looks analogous one-variable equation , solved dividing sides multiple \\(x\\). difficulty matrix operations complicated scalar multiplication division. Matrix multiplication used equation multiply coefficients matrix respective variables, involves relatively complicated procedure general.“division” equivalent called matrix inversion even complicated. First, need define identity matrix, equivalent number 1 matrix multiplication. identity matrix defined square matrices (equal number rows columns), size \\(n\\) \\(n\\) identity matrix define 1s diagonal zeros -diagonal:\n\\[\n= \\begin{pmatrix} 1 & 0 & \\dots & 0 \\\\ 0 & 1  &\\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 &\\dots & 1\\end{pmatrix}\n\\]\nidentity matrix special multiplying matrix (compatible size) results exact matrix (easy check couple examples \\(2 \\times 2\\) \\(3 \\times 3\\) matrices):\\[\n= = \n\\]\n\\(n\\) \\(n\\) matrix \\(\\) inverse \\(^{-1}\\) defined matrix multiplication results identity matrix, :\n\\[\n^{-1} = ^{-1}  = \n\\]\nDefining inverse one task, calculating given matrix, especially large size, quite laborious. describe algorithms , can read Gauss-Jordan elimination, one classic example. One important point matrices invertible, inverse matrix exists, analogous zero real number division. difference infinitely many matrices case, called singular matrices.cases inverse matrix exists, linear system equations can solved multiplying sides inverse matrix, like :\n\\[\n \\vec x = \\mathbf{}^{-1}\\vec b\n\\]Example: Take linear \\(2 \\times 2\\) system equations solve using matrix inversion.R function solve() calculates inverse multiplies constant vector b:","code":"\nA <- matrix(c(4,-1,-3,2), nrow = 2)\nb <- c(5,10)\nsolve(A,b)## [1] 8 9"},{"path":"review-of-linear-algebra.html","id":"fitting-a-line-to-data","chapter":"Lecture 8 Review of linear algebra","heading":"8.2 Fitting a line to data","text":"One geometric application solving multiple linear equations find coefficients line passes two points 2-dimensional plane (plane passes three points three-dimensional space, won’t go .) case, coordinates points data, unknown variables parameters slope \\(m\\) intercept \\(b\\) line want find.Example: data set consists two points \\((3,5), (6, 2)\\), finding best fit values \\(m\\) \\(b\\) means solving following two equations:\\[\n\\begin{aligned}\n 3m + b &= 5 \\\\\n 6m + b &= 2\n\\end{aligned}\n\\]equations solution slope intercept, can calculated R using solve() plot line parameters solution vector beta:However, data set two points small nobody accept values reasonable estimates. Let us add one data point, increase sample size three: \\((3,5), (6, 2), (9, 1)\\). find best fit slope intercept?Bad idea: take two points find line, slope intercept, passes two. clear bad idea: arbitrarily ignoring data, perfectly fitting two points. use data? Let us write equations line slope \\(m\\) intercept \\(b\\) satisfy order fit data points:\\[\n\\begin{aligned}\n3m + b = 5 \\\\\n6m + b = 2\\\\\n9m + b = 1\n\\end{aligned}\n\\]system exact solution, since three equations two unknowns. need find \\(m\\) \\(b\\) “best fit” data, perfect solution.","code":"\nxs <- c(3, 6)\nys <- c(5, 2)\nA <- matrix(c(xs[1], xs[2], 1, 1), nrow = 2)\nb <- c(ys[1], ys[2])\nbeta <- solve(A, b)\ndata1 <- tibble(xs, ys)\nggplot(data = data1) + \n  aes(x = xs, y = ys) + \n  geom_point() + \n  geom_abline(slope = beta[1], intercept = beta[2])"},{"path":"review-of-linear-algebra.html","id":"least-squares-line","chapter":"Lecture 8 Review of linear algebra","heading":"8.2.1 Least-squares line","text":"Let us write equation matrix form follows:\\[\n\\begin{aligned}\n\\mathbf{} = \\begin{pmatrix} 3 & 1 \\\\ 6 & 1 \\\\ 9 & 1 \\end{pmatrix} \\;\\;\\; \\vec{b} = \\begin{pmatrix}5 \\\\ 2 \\\\ 1 \\end{pmatrix} \\;\\;\\;\n\\vec{\\beta} = \\begin{pmatrix} m \\\\ b \\end{pmatrix} \\\\\n\\mathbf{} \\vec \\beta = \\vec b \n\\end{aligned}\n\\]Mathematically, problem one invert non-square matrix. However, way turning matrix square one, multiplying transpose (matrix rows columns reversed):\\[\n\\mathbf{}^T \\mathbf{} \\vec \\beta = \\mathbf{}^T \\vec b\n\\]\nExercise: Carry matrix multiplications verify \\(\\mathbf{}^T \\mathbf{}\\) \\(2 \\times 2\\) matrix \\(\\mathbf{}^T \\vec b\\) vector length 2.Now can solve equation square matrix \\(\\mathbf{}^T \\mathbf{}\\) multiplying sides inverse! general, \\(n\\)-dimensional data set consisting bunch values \\(x\\) \\(y\\), process looks like :\\[\n\\vec Y = \\begin{pmatrix} y_1\\\\ y_2\\\\ \\vdots \\\\ y_n \\end{pmatrix} \\;\\;\\; \n\\mathbf{X} = \\begin{pmatrix} 1 & x_1\\\\ 1 & x_2\\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{pmatrix}\n \\;\\;\\; \n\\mathbf{\\beta} = \\begin{pmatrix} m \\\\ b \\end{pmatrix} \\\\\n\\beta = (\\mathbf{X}^{T} \\mathbf{X})^{-1} \\mathbf{X}^{T}\\vec Y\n\\]\nExample: Let us see best-fit line 3-point data set :Let us use classic data set Karl Pearson’s 1903 containing height fathers sons, return next week tackle linear regression properly:Exercise: Let’s try find best fit line data set (hard way) using process three - point data set:course, R can calculation just one command:feels good know black magic! fact, plotting top data even require computing coefficients:","code":"\nxs <- c(3, 6, 9)\nys <- c(5, 2, 1)\nA <- matrix(c(xs[1], xs[2], xs[3], 1, 1, 1), nrow = 3)\nb <- c(ys[1], ys[2], ys[3])\nbeta <- solve(t(A) %*% A, t(A) %*% b)\ndata1 <- tibble(xs, ys)\nggplot(data = data1) + \n  aes(x = xs, y = ys) + \n  geom_point() + \n  geom_abline(slope = beta[1], intercept = beta[2])\nheights <- read_tsv(\"http://www.randomservices.org/random/data/Pearson.txt\")\npl <- ggplot(data = heights) + \n  aes(x = Father, y = Son) + \n  geom_point() + \n  coord_equal()\npl\nbest_beta_easy <- lm(Son ~ Father, data = heights)\nbest_beta_easy## \n## Call:\n## lm(formula = Son ~ Father, data = heights)\n## \n## Coefficients:\n## (Intercept)       Father  \n##      33.893        0.514\npl + geom_smooth(method = \"lm\") # lm stands for linear model## `geom_smooth()` using formula 'y ~ x'"},{"path":"review-of-linear-algebra.html","id":"linearity-and-vector-spaces","chapter":"Lecture 8 Review of linear algebra","heading":"8.3 Linearity and vector spaces","text":"dealt linear models various guises, now good time define properly linearity means. word comes shape graphs linear functions one variable, e.g. \\(f(x) = x + b\\), algebraic meaning rests following two general properties:Definition. linear transformation linear operator mapping \\(L\\) two sets vectors following properties:(scalar multiplication) \\(L(c \\vec v) = c L(\\vec v)\\); \\(c\\) scalar \\(\\vec v\\) vector(additive) \\(L(\\vec v_1 + \\vec v_2) = L(\\vec v_1) + L(\\vec v_2)\\); \\(\\vec v_1\\) \\(\\vec v_2\\) vectorsHere two types objects: vectors transformations/operators act vectors. basic example vectors matrices, matrix multiplied vector (right) results another vector, provided number columns matrix number rows vector. can interpreted matrix transforming vector \\(\\vec v\\) another one: $ v = u$.Example: Let us multiply following matrix vector (specially chosen make point):see particular vector \\((1,-1)\\) unchanged multiplied matrix, can say matrix multiplication equivalent multiplication 1. another vector matrix:case, vector changed, multiplication constant (4). Thus geometric direction vector remained unchanged.notion linearity leads important idea combining different vectors:Definition: linear combination \\(n\\) vectors \\(\\{ \\vec v_i \\}\\) weighted sum vectors real numbers \\(\\{a_i\\}\\):\n\\[ a_1 \\vec v_1+ a_2 \\vec v_2... + a_n \\vec v_n\\]Linear combinations arise naturally notion linearity, combining additive property scalar multiplication property. Speaking intuitively, linear combination vectors produces new vector related original set. Linear combinations give simple way generating new vectors, thus invite following definition collection vectors closed linear combinations:Definition. vector space collection vectors linear combination \\(n\\) vectors contained vector space.common examples spaces real-valued vectors dimension \\(n\\), denoted \\(\\mathbb{R}^n\\). instance, \\(\\mathbb{R}^2\\) (pronounced “r two”) vector space two dimensional real-valued vectors \\((1,3)\\) \\((\\pi, -\\sqrt{17})\\); similarly, \\(\\mathbb{R}^3\\) vector space consisting three dimensional real-valued vectors \\((0.1,0,-5.6)\\). can convince , taking linear combinations vectors, vector spaces contain points usual Euclidean plane three-dimensional space. real number line can also thought vector space \\(\\mathbb{R}^1\\).","code":"\nA <- matrix(c(2, 2, 1, 3), nrow = 2)\nvec1 <- c(1, -1)\nvec2 <- A %*% vec1\nprint(vec1)## [1]  1 -1\nprint(vec2)##      [,1]\n## [1,]    1\n## [2,]   -1\nvec1 <- c(1, 2)\nvec2 <- A %*% vec1\nprint(vec1)## [1] 1 2\nprint(vec2)##      [,1]\n## [1,]    4\n## [2,]    8"},{"path":"review-of-linear-algebra.html","id":"linear-independence-and-basis-vectors","chapter":"Lecture 8 Review of linear algebra","heading":"8.3.1 Linear independence and basis vectors","text":"can describe vector space without trying list elements? know one can generate element taking linear combinations vectors. turns possible generate (“span”) vector space taking linear combinations subset vectors. challenge find minimal subset subset redundant. order , first introduce new concept:Definition: set vectors \\(\\{ \\vec v_i \\}\\) called linearly independent linear combination involving equals zero vector coefficients zero. ( \\(a_1 \\vec v_1 + a_2 \\vec v_2 + ... + a_n \\vec v_n = 0\\) \\(a_i = 0\\) \\(\\).)familiar Euclidean spaces, e.g. \\(\\mathbb{R}^2\\), linear independence geometric meaning: two vectors linearly independent segments origin endpoint lie line. can shown set three vectors plane linearly dependent, two dimensions vector space. brings us key definition section:Definition: basis vector space linearly independent set vectors generate (span) vector space. number vectors (cardinality) set called dimension vector space.vector space generally many possible bases, illustrated figure. case \\(\\mathbb{R}^2\\), usual (canonical) basis set \\(\\{(1,0); (0,1)\\}\\) obviously generates point plane linearly independent. two linearly independent vectors can generate vector plane.Example: vector \\(\\vec r = (2,1)\\) can represented linear combination two canonical vectors: \\(\\vec r = 2\\times (1,0)+1\\times (0,1)\\). Let us choose another basis set, say \\(\\{(1,1); (-1,1)\\}\\) (canonical basis vectors rotated \\(\\pi/2\\).) vector can represented linear combination two vectors, coefficients \\(1.5\\) \\(-0.5\\): \\(\\vec r = 1.5\\times (1,1) - 0.5 \\times (-1,1)\\). call first basis \\(C\\) canonical second basis \\(D\\) different, can write vector using different sets coordinates basis:\\[ \n\\vec r_{C} = (2,1); \\; \\vec r_D = (1.5, -0.5)\n\\]","code":""},{"path":"review-of-linear-algebra.html","id":"projections-and-changes-of-basis","chapter":"Lecture 8 Review of linear algebra","heading":"8.3.2 Projections and changes of basis","text":"representation arbitrary vector (point) vector space linear combination given basis set called decomposition point terms basis, gives coordinates vector terms basis vector. decomposition point terms particular basis useful high-dimensional spaces, clever choice basis can allow description set points (data set) terms contributions basis vectors, data set primarily extends dimensions.obtain coefficients basis vectors decomposition vector \\(\\vec r\\), need perform termed projection vector onto basis vectors. Think shining light perpendicular basis vector, measuring length shadow cast vector \\(\\vec r\\) onto \\(\\vec v_i\\). vectors parallel, shadow equal length \\(\\vec r\\); orthogonal, shadow nonexistent. find length shadow, use inner product \\(\\vec r\\) \\(\\vec v\\), recall corresponds cosine angle two vectors multiplied norms: $r, v=rv() $. care length vector \\(\\vec v\\) projecting onto, thus divide inner product square norm \\(\\vec v\\), multiply vector \\(\\vec v\\) projection coefficient:\\[ \nProj(\\vec r ; \\vec v) = \\frac{ \\langle \\vec r , \\vec v \\rangle  } {\\langle \\vec v , \\vec v \\rangle } \\vec v = \\frac{ \\langle \\vec r ,  \\vec v \\rangle  } {\\vert \\vec v \\vert^2} \\vec v= \\frac{  \\vert\\vec r\\vert \\cos(\\theta) } {\\vert \\vec v \\vert}\\vec v\n\\]formula gives projection vector \\(\\vec r\\) onto \\(\\vec v\\), result new vector direction \\(\\vec v\\), scalar coefficient \\(= \\ \\langle \\vec r , \\vec v \\rangle /\\vert \\vec v \\vert^2\\).Example: one might calculate projection point \\((2,1)\\) onto basis set \\(\\{(1,1); (-1,1)\\}\\):quite right: projection coefficients factor two compared correct values example . neglected normalize basis vectors, modify script follows:example convert vector/point representation one basis set another. new basis vectors, expressed original basis set, arranged matrix row, scaled norm squared, multiplied vector one wants express new basis. resulting vector contains coordinates new basis.","code":"\nv1 <- c(1, 1)\nv2 <- c(-1, 1)\nu <- c(2, 1)\nProjMat <- matrix(cbind(v1, v2), \n                  byrow = T, nrow = 2)\nprint(ProjMat)##      [,1] [,2]\n## [1,]    1    1\n## [2,]   -1    1\nProjMat %*% u##      [,1]\n## [1,]    3\n## [2,]   -1\nv1 <- c(1, 1)\nv1 <- v1 / (sum(v1^2))\nv2 <- c(-1, 1)\nv2 <- v2 / (sum(v2^2))\nu <- c(2, 1)\nProjMat <- matrix(cbind(v1, v2), \n                  byrow = T, nrow = 2)\nprint(ProjMat)##      [,1] [,2]\n## [1,]  0.5  0.5\n## [2,] -0.5  0.5\nprint(ProjMat %*% u)##      [,1]\n## [1,]  1.5\n## [2,] -0.5"},{"path":"review-of-linear-algebra.html","id":"matrices-as-linear-operators","chapter":"Lecture 8 Review of linear algebra","heading":"8.4 Matrices as linear operators","text":"","code":""},{"path":"review-of-linear-algebra.html","id":"matrices-transform-vectors","chapter":"Lecture 8 Review of linear algebra","heading":"8.4.1 Matrices transform vectors","text":"section learn characterize square matrices finding special numbers vectors associated . core analysis lies concept matrix operator transforms vectors multiplication. clear, section take default matrices \\(\\) square, vectors \\(\\vec v\\) column vectors, thus multiply matrix right: \\(\\times \\vec v\\).matrix multiplied vector produces another vector, provided number columns matrix number rows vector. can interpreted matrix transforming vector \\(\\vec v\\) another one: $ v = u$. resultant vector \\(\\vec u\\) may may resemble \\(\\vec v\\), special vectors transformation simple.Example. Let us multiply following matrix vector (specially chosen make point):\\[\n\\left(\\begin{array}{cc}2 & 1 \\\\ 2& 3\\end{array}\\right)\\left(\\begin{array}{c}1 \\\\ -1 \\end{array}\\right) = \\left(\\begin{array}{c}2 -1 \\\\ 2 - 3 \\end{array}\\right) =  \\left(\\begin{array}{c} 1 \\\\ -1 \\end{array}\\right)\n\\]see particular vector unchanged multiplied matrix, can say matrix multiplication equivalent multiplication 1. another vector matrix:\\[\n\\left(\\begin{array}{cc}2 & 1 \\\\ 2& 3\\end{array}\\right)\\left(\\begin{array}{c}1 \\\\ 2 \\end{array}\\right) = \\left(\\begin{array}{c}2 +2 \\\\ 2 + 6 \\end{array}\\right) =  \\left(\\begin{array}{c} 4 \\\\ 8 \\end{array}\\right)\n\\]case, vector changed, multiplication constant (4). Thus geometric direction vector remained unchanged.Generally, square matrix associated set vectors multiplication matrix equivalent multiplication constant. can written definition:Definition. eigenvector square matrix \\(\\) vector \\(\\vec v\\) matrix multiplication \\(\\) equivalent multiplication constant. constant \\(\\lambda\\) called eigenvalue \\(\\) corresponding eigenvector \\(\\vec v\\). relationship summarized following equation:\\[\n \\times  \\vec v = \\lambda \\vec v\n\\]Note equation combines matrix (\\(\\)), vector (\\(\\vec v\\)) scalar \\(\\lambda\\), sides equation column vectors.definition specify many eigenvectors eigenvalues can exist given matrix \\(\\). usually many vectors \\(\\vec v\\) corresponding numbers \\(\\lambda\\) number rows columns square matrix \\(\\), 2 2 matrix two eigenvectors two eigenvalues, 5x5 matrix 5 , etc. One ironclad rule distinct eigenvalues matrix dimension. matrices possess fewer eigenvalues matrix dimension, said degenerate set eigenvalues, least two eigenvectors share eigenvalue.situation eigenvectors trickier. matrices vector eigenvector, others limited set eigenvectors. difficult counting eigenvectors eigenvector still eigenvector multiplied constant. can show matrix, multiplication constant commutative: $cA = Ac $, \\(\\) matrix \\(c\\) constant. leads us important result \\(\\vec v\\) eigenvector eigenvalue \\(\\lambda\\), scalar multiple \\(c \\vec v\\) also eigenvector eigenvalue. following demonstrates algebraically:\\[\n \\times  (c \\vec v) = c  \\times  \\vec v = c \\lambda \\vec v =  \\lambda (c \\vec v)\n\\]shows vector \\(c \\vec v\\) multiplied matrix \\(\\), results multiplied number \\(\\lambda\\), definition eigenvector. Therefore, eigenvector \\(\\vec v\\) unique, constant multiple \\(c \\vec v\\) also eigenvector. useful think single eigenvector \\(\\vec v\\), collection vectors can inter-converted scalar multiplication essentially eigenvector. Another way represent , eigenvector real, eigenvector direction remains unchanged multiplication matrix, direction vector \\(v\\) figure . mentioned , true real eigenvalues eigenvectors, since complex eigenvectors \nused define direction real space.Illustration geometry matrix \\(\\) multiplying eigenvector \\(v\\), resulting vector direction\n\\(\\lambda v\\)summarize, eigenvalues eigenvectors matrix set numbers set vectors (scalar multiple) describe action matrix multiplicative operator vectors. “Well-behaved” square \\(n \\times n\\) matrices \\(n\\) distinct eigenvalues \\(n\\) eigenvectors pointing distinct directions. deep sense, collection eigenvectors eigenvalues defines matrix \\(\\), older name characteristic vectors values.","code":""},{"path":"review-of-linear-algebra.html","id":"calculating-eigenvalues","chapter":"Lecture 8 Review of linear algebra","heading":"8.4.2 calculating eigenvalues","text":"Finding eigenvalues eigenvectors analytically, paper, quite laborious even 3 3 4 4 matrices larger ones analytical solution. practice, task outsourced computer, MATLAB number functions purpose. Nevertheless, useful go process 2 dimensions order gain understanding involved. definition eigenvalues eigenvectors, condition can written terms four elements 2 2 matrix:\\[\n\\left(\\begin{array}{cc}& b \\\\c & d\\end{array}\\right)\\left(\\begin{array}{c}v_1 \\\\ v_2 \\end{array}\\right) = \\left(\\begin{array}{c}av_1 +b v_2\\\\ cv_1+ dv_2 \\end{array}\\right) = \\lambda \\left(\\begin{array}{c}v_1 \\\\ v_2 \\end{array}\\right)\n\\]now system two linear algebraic equations, can solve substitution. First, let us solve \\(v_1\\) first row, get \\[v_1 = \\frac{-bv_2}{-\\lambda}\\] substitute second equation get:\\[\n\\frac{-bcv_2}{-\\lambda} +(d-\\lambda)v_2 = 0\n\\]Since \\(v_2\\) multiplies terms, necessarily zero, require multiplicative factor zero. little algebra, obtain following, known characteristic equation matrix:\\[\n-bc +(-\\lambda)(d-\\lambda) = \\lambda^2-(+d)\\lambda +ad-bc = 0\n\\]equation can simplified using two quantities defined beginning section: sum diagonal elements called trace \\(\\tau = +d\\), determinant \\(\\Delta = ad-bc\\). quadratic equation two solutions, dependent solely \\(\\tau\\) \\(\\Delta\\):\\[\n\\lambda = \\frac{\\tau \\pm \\sqrt{\\tau^2-4\\Delta}}{2}\n\\]general expression 2 2 matrix, showing two possible eigenvalues. Note \\(\\tau^2-4\\Delta>0\\), eigenvalues real, \\(\\tau^2-4\\Delta<0\\), complex (real imaginary parts), \\(\\tau^2-4\\Delta=0\\), one eigenvalue. situation known degenerate, two eigenvectors share eigenvalue.Example. Let us take matrix looked previous subsection:\\[\n= \\left(\\begin{array}{cc}2 & 1 \\\\ 2& 3\\end{array}\\right)\n\\]trace matrix \\(\\tau = 2+3 =5\\) determinant \n\\(\\Delta = 6 - 2 = 4\\). formula, eigenvalues :\\[\n\\lambda = \\frac{5 \\pm \\sqrt{5^2-4 \\times 4}}{2}  =  \\frac{5 \\pm 3}{2}  = 4, 1\n\\]multiples found example , expected. course R functions calculate instead hand:Note: real-valued matrix can complex eigenvalues eigenvectors, whenever acts real vector, result still real. complex numbers cancel others imaginary parts.","code":"\nA <- matrix(c(2, 2, 1, 3), nrow =2)\neigs <- eigen(A)\neigs$values## [1] 4 1\neigs$vectors##            [,1]       [,2]\n## [1,] -0.4472136 -0.7071068\n## [2,] -0.8944272  0.7071068"},{"path":"linear-models.html","id":"linear-models","chapter":"Lecture 9 Linear models","heading":"Lecture 9 Linear models","text":"Learn perform linear regression, make sure assumptions model violated, interpret results.Note need new library:","code":"\nlibrary(tidyverse)\nlibrary(lindia) # regression diagnostic in ggplot2"},{"path":"linear-models.html","id":"regression-toward-the-mean","chapter":"Lecture 9 Linear models","heading":"9.1 Regression toward the mean","text":"Francis Galton (Darwin’s half-cousin) biologist interested evolution, one main proponents eugenics (coined term ). advance research program, set measure several features human populations, started trying explain variation observed, incidentally becoming one founding fathers modern statistics.“Regression towards mediocrity hereditary stature” showed interesting pattern: children tall parents tended shorter parents, children short parents tended taller parents. called phenomenon “regression toward mediocrity” (now called regression toward [] mean).’re going explore phenomenon using Karl Pearson’s (another founding father statistics) data 1903, recording height fathers sons:Let’s add 1:1 line comparison:can see sons tend taller fathers. Let’s see much:let’s add line intercept 1:can see line divide cloud points evenly: even though tall fathers tend produce tall sons, short fathers short sons, sons short fathers tend taller fathers (example, look sons fathers less 60 inches tall), sons tall fathers tend shorter fathers (example, sons fathers taller 75 inches).phenomenon called “regression toward mean”: take two measurement sample (related samples, ), variable extreme first measurement, tend closer average second measurement; extreme second measurement, tend closer average first.Regression mean: dangers interpretationA city sees unusual growth crime given neighborhood, decide patrol neighborhood heavily. next year, crime rates close normal. due heavy presence police?teacher sees scolding students ’ve low score test makes perform better next test. (praising unusually high scores lead slacking next test?)huge problem science: effect sizes tend decrease time. Problem selective reporting?phenomenon gave name one simplest statistical models: linear regression.","code":"\nheights <- read_tsv(\"http://www.randomservices.org/random/data/Pearson.txt\")\npl <- ggplot(data = heights) + aes(x = Father, y = Son) + geom_point() + coord_equal()\npl\npl + geom_abline(slope = 1, intercept = 0, linetype = 2, color = \"red\")\nmean(heights$Father)## [1] 67.68683\nmean(heights$Son)## [1] 68.68423\n# difference\nmean(heights$Son) - mean(heights$Father)## [1] 0.9974026\npl <- pl + geom_abline(slope = 1, intercept = 1, linetype = 2, color = \"blue\")\npl"},{"path":"linear-models.html","id":"finding-the-best-fitting-line-linear-regression","chapter":"Lecture 9 Linear models","heading":"9.2 Finding the best fitting line: Linear Regression","text":"can explain relationship height fathers sons? One simplest models can use called “Linear Model.” Basically, want express height son function height father:\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\]\\(y_i\\) height son (response variable), \\(x_i\\) height father (explanatory variable), \\(\\beta_0\\) \\(\\beta_1\\) two numbers (intercept slope line) vary within population (parameters want fit). Finally, term \\(\\epsilon_i\\) measures “error” making \\(^{th}\\) son. simplicity, assume \\(\\epsilon_i \\overset{\\text{iid}}{\\sim} \\mathcal N(0, \\sigma^2)\\) (\\(\\sigma\\) therefore another parameter want fit).multiple explanatory variables (example, recorded also height mother, whether son born full term premature, average caloric intake family, etc.), speak Multiple Linear Regression:\\[\ny_i = \\beta_0 + \\sum_{k=1}^n \\beta_k x_{ik} + \\epsilon_i\n\\]","code":""},{"path":"linear-models.html","id":"solving-a-linear-model-some-linear-algebra","chapter":"Lecture 9 Linear models","heading":"9.2.1 Solving a linear model — some linear algebra","text":"section, ’re going look mechanics linear regression. Suppose simplicity single explanatory variable, can write linear model compact form :\\[\n\\mathbf{Y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon}\n\\]:\\[\n\\mathbf{Y} = \\begin{pmatrix} y_1\\\\ y_2\\\\ \\vdots \\\\ y_n \\end{pmatrix} \\;\\;\\; \n\\mathbf{X} = \\begin{pmatrix} 1 & x_1\\\\ 1 & x_2\\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{pmatrix}\n \\;\\;\\; \n\\mathbf{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1\\end{pmatrix} \\;\\;\\; \\mathbf{\\epsilon} = \\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}\n\\]Solving linear regression means finding best-fitting \\(\\beta_0\\), \\(\\beta_1\\) \\(\\sigma\\) (controlling spread distribution \\(\\epsilon_i\\)). goal find values \\(\\beta\\) minimize \\(\\sigma\\) (meaning points fall closer line). Rearranging:\\[\n\\sum_i \\epsilon_i^2 = \\sum_i (y_i - \\beta_0 - \\beta_1 x_i)^2 =  \\Vert \\mathbf{Y} - \\mathbf{X} \\mathbf{\\beta} \\Vert\n\\], want find vector \\(\\beta\\) minimizes norm \\(\\Vert \\mathbf{Y} - \\mathbf{X} \\mathbf{\\beta} \\Vert\\). One can prove accomplished using:\\[\n\\hat{\\mathbf{\\beta}} = \\left( \\mathbf{X}^T \\mathbf{X} \\right)^{-1} \\mathbf{X}^T \\mathbf{Y}\n\\]matrix \\(\\left( \\mathbf{X}^T \\mathbf{X} \\right)^{-1} \\mathbf{X}^T\\) known (left) Moore-Penrose pseudo-inverse \\(\\mathbf{X}\\). Let’s try R (“hard” way):find best fitting line intercept 34 inches, slope 0.51. course, R can calculation just one command:feels good know black magic! fact, plotting top data even require computing coefficients:","code":"\nX <- cbind(1, heights$Father)\nY <- cbind(heights$Son)\nbest_beta <- solve(t(X) %*% X) %*% t(X) %*% Y\nbest_beta##            [,1]\n## [1,] 33.8928005\n## [2,]  0.5140059\nbest_beta_easy <- lm(Son ~ Father, data = heights)\nbest_beta_easy## \n## Call:\n## lm(formula = Son ~ Father, data = heights)\n## \n## Coefficients:\n## (Intercept)       Father  \n##      33.893        0.514\npl + geom_smooth(method = \"lm\") # lm stands for linear model"},{"path":"linear-models.html","id":"minimizing-the-sum-of-squares","chapter":"Lecture 9 Linear models","heading":"9.2.2 Minimizing the sum of squares","text":"just called “ordinary least-squares”: trying minimize distance data points projection best-fitting line. can compute “predicted” heights :\\[\n\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\mathbf{\\beta}}\n\\], ’re minimizing \\(\\Vert \\mathbf{Y} - \\hat{\\mathbf{Y}}\\Vert\\). call \\(\\hat{\\mathbf{\\epsilon}} = \\mathbf{Y} - \\hat{\\mathbf{Y}}\\) vector residuals. , can estimate final parameter, \\(\\sigma\\):\\[\n\\sigma = \\sqrt{\\frac{\\sum_i \\hat{\\epsilon_i}^2}{n -  p}}\n\\]\\(n\\) number data points, \\(p\\) number parameters \\(\\mathbf{\\beta}\\) (2 case); measures number degrees freedom. Let’s try compute :R, find reported Residual standard error call summary model:Finally, coefficient determination \\(R^2\\) computed :\\[\nR^2 = \\frac{\\sum_i (\\hat{y}_i - \\bar{y})^2}{\\sum_i ({y}_i - \\bar{y})^2}\n\\]\\(\\bar{y}\\) mean \\(y_i\\). regression intercept, \\(R^2\\) can vary 0 1, values close 1 indicating good fit data. , let’s compute hard way easy way:","code":"\ndegrees_of_freedom <- length(Y) - 2\ndegrees_of_freedom## [1] 1076\nepsilon_hat <- X %*% best_beta - Y\nsigma <- sqrt(sum(epsilon_hat^2) / degrees_of_freedom)\nsigma## [1] 2.438134\nsummary(best_beta_easy)## \n## Call:\n## lm(formula = Son ~ Father, data = heights)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -8.8910 -1.5361 -0.0092  1.6359  8.9894 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 33.89280    1.83289   18.49   <2e-16 ***\n## Father       0.51401    0.02706   19.00   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.438 on 1076 degrees of freedom\n## Multiple R-squared:  0.2512, Adjusted R-squared:  0.2505 \n## F-statistic: 360.9 on 1 and 1076 DF,  p-value: < 2.2e-16\ny_bar <- mean(Y)\nR_2 <- sum((X %*% best_beta - y_bar)^2) / sum((Y - y_bar)^2)\nR_2## [1] 0.251164\n# look for Multiple R-squared:\nsummary(best_beta_easy)## \n## Call:\n## lm(formula = Son ~ Father, data = heights)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -8.8910 -1.5361 -0.0092  1.6359  8.9894 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 33.89280    1.83289   18.49   <2e-16 ***\n## Father       0.51401    0.02706   19.00   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.438 on 1076 degrees of freedom\n## Multiple R-squared:  0.2512, Adjusted R-squared:  0.2505 \n## F-statistic: 360.9 on 1 and 1076 DF,  p-value: < 2.2e-16"},{"path":"linear-models.html","id":"assumptions-of-linear-regression","chapter":"Lecture 9 Linear models","heading":"9.2.3 Assumptions of linear regression","text":"practice, performing linear regression, making number assumptions data. main ones:Model structure: assume process generating data linear.Explanatory variable: assume measured without errors (!).Residuals: assume residuals ..d. Normal.Strict exogeneity: residuals conditional mean 0.\\[\n\\mathbb E[\\epsilon_i | x_i] = 0\n\\]linear dependence: columns \\(\\mathbf{X}\\) linearly independent.Homoscedasticity: variance residuals independent \\(x_i\\).\\[\n\\mathbb V[\\epsilon_i | x_i] =  \\sigma^2\n\\]Errors uncorrelated observations.\\[\n\\mathbb E[\\epsilon_i \\epsilon_j | x] = 0 \\; \\forall j \\neq \n\\]","code":""},{"path":"linear-models.html","id":"linear-regression-in-action","chapter":"Lecture 9 Linear models","heading":"9.3 Linear regression in action","text":"perform slightly complicated linear regression, take data :Piwowar HA, Day RS, Fridsma DB (2007) Sharing detailed research data associated increased citation rate. PLoS ONE 2(3): e308.authors set demonstrate sharing data accompanying papers tends increase number citations received paper.First, let’s run model logarithm number citations + 1 regressed “Impact Factor” journal (measure “prestige” based average number citations per paper received):can see higher impact factor, higher number citations received (unsurprisingly!). Now let’s add another variable, detailing whether publicly available data accompany paper:find sharing data associated larger number citations.","code":"\n# original URL \n# https://datadryad.org/stash/dataset/doi:10.5061/dryad.j2c4g\ndat <- read_csv(\"data/Piwowar_2011.csv\") \n# rename variables for easier handling\ndat <- dat %>% rename(IF = `Impact factor of journal`, \n                      NCIT = `Number of Citations in first 24 months after publication`, \n                      SHARE = `Is the microarray data publicly available`) %>% \n      select(NCIT, IF, SHARE)\nmy_model <- lm(log(NCIT + 1) ~ log(IF + 1), data = dat)\nsummary(my_model)## \n## Call:\n## lm(formula = log(NCIT + 1) ~ log(IF + 1), data = dat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.65443 -0.44272 -0.00769  0.43414  1.62817 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   0.1046     0.2951   0.355    0.724    \n## log(IF + 1)   1.2920     0.1196  10.802   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6887 on 83 degrees of freedom\n## Multiple R-squared:  0.5844, Adjusted R-squared:  0.5794 \n## F-statistic: 116.7 on 1 and 83 DF,  p-value: < 2.2e-16\nmy_model2 <- lm(log(NCIT + 1) ~ log(IF + 1) + SHARE, data = dat)\nsummary(my_model2)## \n## Call:\n## lm(formula = log(NCIT + 1) ~ log(IF + 1) + SHARE, data = dat)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.98741 -0.43768  0.08726  0.41847  1.35634 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   0.4839     0.3073   1.575  0.11918    \n## log(IF + 1)   1.0215     0.1442   7.084  4.4e-10 ***\n## SHARE         0.5519     0.1802   3.062  0.00297 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6564 on 82 degrees of freedom\n## Multiple R-squared:  0.627,  Adjusted R-squared:  0.6179 \n## F-statistic: 68.92 on 2 and 82 DF,  p-value: < 2.2e-16"},{"path":"linear-models.html","id":"a-regression-gone-wild","chapter":"Lecture 9 Linear models","heading":"9.4 A regression gone wild","text":"Even fit good, assumptions met, one can still end fantastic blunder. show , going repeat study published Nature (less!) Tatem et al. can find study . Briefly, Authors gathered data 100m sprint Olympics 1900 2004, men women. can :, fitted linear regression points, men women. far, good:fit quite good:\\(R^2\\) 0.93, pinnacle good linear regression. Now however, comes problem. Authors noticed times recorded women falling faster men, meaning gender gap reducing. ever disappear? Just extend regression project forward:can see lines touching sometimes 2200! women overrun men.number things wrong result. First, logic, computers soon go faster speed light, number people planet Earth hundreds billions, price sequencing drop much paid instead paying get samples sequenced…Second, extend backwards, rather forward, find Roman women take minute run 100m (possibly, uncomfortable tunics sandals…).Neil Bohr allegedly said (disputed), “Prediction difficult, especially future.” fact non-linear curve looks quite linear considering small range values x-axis. prove point, let’s add data 2004 today:can see process already slowed : now take extra century “momentous sprint.”many things wrong short paper, Nature showered replies. favorite Cambridge statistician (Authors Oxford, ça va sans dire); perfectly short venomous—good candidate Nobel prize Literature!Sir — . J. Tatem colleagues calculate women may outsprint men \nmiddle twenty-second century (Nature 431, 525; 2004). omit mention, however, (according analysis) far interesting race \noccur 2636, times less zero seconds recorded.\nintervening 600 years, authors may wish address obvious\nchallenges raised time-keeping teaching basic statistics.\n— Kenneth Rice","code":"\nolympics <- read_csv(\"data/100m_dash.csv\")\nggplot(data = olympics %>% filter(Year > 1899, Year < 2005)) + \n  aes(x = Year, y = Result, colour = Gender) + \n  geom_point() + geom_smooth(method = \"lm\")\nsummary(lm(Result ~ Year*Gender,\n  data = olympics %>% filter(Year > 1899, Year < 2005)))## \n## Call:\n## lm(formula = Result ~ Year * Gender, data = olympics %>% filter(Year > \n##     1899, Year < 2005))\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.38617 -0.05428 -0.00071  0.08239  0.32174 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  31.808278   2.179491  14.594  < 2e-16 ***\n## Year         -0.010997   0.001116  -9.855 1.24e-11 ***\n## GenderW      10.952646   4.371678   2.505   0.0170 *  \n## Year:GenderW -0.005011   0.002228  -2.249   0.0309 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1707 on 35 degrees of freedom\n## Multiple R-squared:  0.9304, Adjusted R-squared:  0.9244 \n## F-statistic: 155.9 on 3 and 35 DF,  p-value: < 2.2e-16\nggplot(data = olympics %>% filter(Year > 1899, Year < 2005)) + \n  aes(x = Year, y = Result, colour = Gender) + \n  geom_point() + geom_smooth(method = \"lm\", fullrange = TRUE, se = FALSE) +\n  xlim(c(1890, 2200)) + ylim(c(0, 13))\nggplot(data = olympics %>% filter(Year > 1899, Year < 2005)) + \n  aes(x = Year, y = Result, colour = Gender) + \n  geom_point() + geom_smooth(method = \"lm\", fullrange = TRUE, se = FALSE) +\n  xlim(c(-2000, 2200)) + ylim(c(0, 75))\nggplot(data = olympics %>% filter(Year > 1899)) + \n  aes(x = Year, y = Result, colour = Gender) + \n  geom_point() + geom_smooth(method = \"lm\", fullrange = TRUE, se = FALSE) +\n  xlim(c(1890, 2400)) + ylim(c(0, 13))"},{"path":"linear-models.html","id":"more-advanced-topics","chapter":"Lecture 9 Linear models","heading":"9.5 More advanced topics","text":"","code":""},{"path":"linear-models.html","id":"categorical-variables-in-linear-models","chapter":"Lecture 9 Linear models","heading":"9.5.1 Categorical variables in linear models","text":"example , built model:\\[\n \\log(\\text{NCIT} + 1) = \\beta_0 + \\beta_1 (\\log(\\text{} + 1))_i + \\beta_2 (\\text{SHARE})_i + \\epsilon_i\n\\]case, variable SHARE takes values 1 0. , data shared (SHARE = 0) model reduces previous one, \\(\\beta_2\\) absent. coefficient \\(\\beta_2\\) measures increase log citation count data shared.approach can taken whenever categorical values: R automatically create dummy variables encoding whether ith data point belongs particular category. example, suppose want predict height child based height father, also collected gender, three categories: F female, M male, U unknown. use information build model:\\[\n \\text{height}_i = \\beta_0 + \\beta_1 \\text{(height father)}_i + \\beta_2 (\\text{gender M})_i + \\beta_3 (\\text{gender U})_i + \\epsilon_i\n\\]variable gender M takes value 1 gender M 0 otherwise, gender U takes value 1 gender unknown 0 otherwise. , gender F variables zero, \\(\\beta_2\\) \\(\\beta_3\\) measure increase (decrease) height males unspecified gender, respectively. R automatically, understanding going “hood” essential interpreting results.","code":""},{"path":"linear-models.html","id":"interactions-in-linear-models","chapter":"Lecture 9 Linear models","heading":"9.5.2 Interactions in linear models","text":"Sometimes think explanatory variables “interact.” example, suppose want predict BMI people. available average caloric intake, height, gender, whether vegetarian, vegan, omnivores. simple model :\\[\n\\text{BMI}_i = \\beta_0 + \\beta_h \\text{height}_i + \\beta_c \\text{calories}_i + \\beta_g \\text{gender}_i + \\epsilon_i\n\\]add type diet factor:\\[\n\\text{BMI}_i = \\beta_0 + \\beta_h \\text{height}_i + \\beta_c \\text{calories}_i + \\beta_g \\text{gender}_i + \\beta_d \\text{diet}_i + \\epsilon_i\n\\]However, suppose believe type diet affect differentially men women. , like create “interaction” (e.g., paleo-female, vegan-male):\\[\n\\text{BMI}_i = \\beta_0 + \\beta_h \\text{height}_i + \\beta_c \\text{calories}_i + \\beta_g \\text{gender}_i + \\beta_d \\text{diet}_i + \\beta_{gd} \\text{gender:diet}_i + \\epsilon_i\n\\]colon signals “interaction.” R, coded lm(BMI ~ height + calories + gender * diet). simpler model one account gender:diet interaction, separate effects gender diet:\\[\n\\text{BMI}_i = \\beta_0 + \\beta_h \\text{height}_i + \\beta_c \\text{calories}_i + \\beta_{gd}\\text{gender:diet}_i + \\epsilon_i\n\\]R can coded lm(BMI ~ height + calories + gender:diet). Finally, models believe intercept 0 (note makes \\(R^2\\) statistics uninterpretable!). R, just put -1 end definition model (e.g., lm(BMI ~ height + calories + gender:diet - 1)).","code":""},{"path":"linear-models.html","id":"regression-diagnostics","chapter":"Lecture 9 Linear models","heading":"9.5.3 Regression diagnostics","text":"Now know mechanics linear regression, turn diagnostics: can make sure model fits data “well?” start analyzing data set assembled Anscombe (American Statistician, 1973)file comprised four data sets. perform linear regression using data set separately:can see, data set best fit line, intercept 3 slope \\(\\frac{1}{2}\\). Plotting data, however, shows situation complicated:Data_1 fitted quite well; Data_2 shows marked nonlinearity; points one Data_3 line, single outlier shifts line considerably; finally, Data_4 single point responsible fitting line: values X exactly . Inspecting graphs, conclude can trust model first case. performing multiple regression, however, hard see whether ’re case 1, one cases. R provides number diagnostic tools can help decide whether fit data good.","code":"\ndat <- read_csv(\"data/Anscombe_1973.csv\")\nlm(Y ~ X, data = dat %>% filter(Data_set == \"Data_1\"))## \n## Call:\n## lm(formula = Y ~ X, data = dat %>% filter(Data_set == \"Data_1\"))\n## \n## Coefficients:\n## (Intercept)            X  \n##      3.0001       0.5001\nlm(Y ~ X, data = dat %>% filter(Data_set == \"Data_2\"))## \n## Call:\n## lm(formula = Y ~ X, data = dat %>% filter(Data_set == \"Data_2\"))\n## \n## Coefficients:\n## (Intercept)            X  \n##       3.001        0.500\nlm(Y ~ X, data = dat %>% filter(Data_set == \"Data_3\"))## \n## Call:\n## lm(formula = Y ~ X, data = dat %>% filter(Data_set == \"Data_3\"))\n## \n## Coefficients:\n## (Intercept)            X  \n##      3.0025       0.4997\nlm(Y ~ X, data = dat %>% filter(Data_set == \"Data_4\"))## \n## Call:\n## lm(formula = Y ~ X, data = dat %>% filter(Data_set == \"Data_4\"))\n## \n## Coefficients:\n## (Intercept)            X  \n##      3.0017       0.4999\nggplot(data = dat) + aes(x = X, y = Y, colour = Data_set) + \n  geom_point() + geom_smooth(method = \"lm\", se = FALSE) + \n  facet_wrap(~Data_set)"},{"path":"linear-models.html","id":"plotting-the-residuals","chapter":"Lecture 9 Linear models","heading":"9.5.4 Plotting the residuals","text":"first thing want plot residuals function fitted values. plot make apparent whether data linear . package lindia (linear regression diagnostics) makes easy produce type plot using ggplot2:looking approximately flat line, meaning residuals approximately normally distributed mean zero fitted value. case data sets:","code":"\ngg_resfitted(lm(Y ~ X, data = dat %>% filter(Data_set == \"Data_1\"))) + geom_smooth(method = \"loess\")\ngg_resfitted(lm(Y ~ X, data = dat %>% \n                  filter(Data_set == \"Data_2\"))) + \n  geom_smooth(method = \"loess\")\ngg_resfitted(lm(Y ~ X, data = dat %>% \n                  filter(Data_set == \"Data_3\"))) + \n  geom_smooth(method = \"loess\")\ngg_resfitted(lm(Y ~ X, data = dat %>% \n                  filter(Data_set == \"Data_4\"))) + \n  geom_smooth(method = \"loess\")"},{"path":"linear-models.html","id":"q-q-plot","chapter":"Lecture 9 Linear models","heading":"9.5.5 Q-Q Plot","text":"can take , test whether residuals follow normal distribution. particular, can estimate density residuals, plot density normal distribution:, looking good match 1:1 line; outliers found far line (e.g., case 3).","code":"\ngg_qqplot(lm(Y ~ X, data = dat %>% filter(Data_set == \"Data_1\")))\ngg_qqplot(lm(Y ~ X, data = dat %>% filter(Data_set == \"Data_2\")))\ngg_qqplot(lm(Y ~ X, data = dat %>% filter(Data_set == \"Data_3\")))\ngg_qqplot(lm(Y ~ X, data = dat %>% filter(Data_set == \"Data_4\")))"},{"path":"linear-models.html","id":"cooks-distance","chapter":"Lecture 9 Linear models","heading":"9.5.6 Cook’s distance","text":"Another way detect outliers compute Cook’s distance every point. Briefly, statistic measures effect regression obtain remove point.","code":"\ngg_cooksd(lm(Y ~ X, data = dat %>% filter(Data_set == \"Data_1\")))\ngg_cooksd(lm(Y ~ X, data = dat %>% filter(Data_set == \"Data_2\")))\ngg_cooksd(lm(Y ~ X, data = dat %>% filter(Data_set == \"Data_3\")))\ngg_cooksd(lm(Y ~ X, data = dat %>% filter(Data_set == \"Data_4\")))"},{"path":"linear-models.html","id":"leverage","chapter":"Lecture 9 Linear models","heading":"9.5.7 Leverage","text":"Points strongly influence regression said much “leverage”:","code":"\ngg_resleverage(lm(Y ~ X, data = dat %>% filter(Data_set == \"Data_1\")))\ngg_resleverage(lm(Y ~ X, data = dat %>% filter(Data_set == \"Data_2\")))\ngg_resleverage(lm(Y ~ X, data = dat %>% filter(Data_set == \"Data_3\")))\ngg_resleverage(lm(Y ~ X, data = dat %>% filter(Data_set == \"Data_4\")))"},{"path":"linear-models.html","id":"running-all-diagnostics","chapter":"Lecture 9 Linear models","heading":"9.5.8 Running all diagnostics","text":"diagnostics available. run diagnostics given model, call","code":"\ngg_diagnose(lm(Y ~ X, data = dat %>% filter(Data_set == \"Data_2\")))"},{"path":"linear-models.html","id":"transforming-the-data","chapter":"Lecture 9 Linear models","heading":"9.6 Transforming the data","text":"Often, one needs transform data running linear regression, order fulfill assumptions. ’re going look salary professors University California show done.distribution salaries skewed — looks like log-normal distribution:set consider log pay, get closer normal:can try explain pay combination title location:note: Berkeley taken baseline location. Similarly, ASSOC-PROF AY taken baseline title.Q-Q plot shows terrible model! Now let’s try transformed data:Much better! Remember inspect explanatory response variables. Ideally, want response normally distributed. Sometimes one many covariates can nonlinear relationship response variable, transform prior analysis.","code":"\n# read the data\n# Original URL\ndt <- read_csv(\"https://raw.githubusercontent.com/dailybruin/uc-salaries/master/data/uc_salaries.csv\", \n               col_names = c(\"first_name\", \"last_name\", \"title\", \"a\", \"pay\", \"loc\", \"year\", \"b\", \"c\", \"d\")) %>% \n      select(first_name, last_name, title, loc, pay)\n# get only profs\ndt <- dt %>% filter(title %in% c(\"PROF-AY\", \"ASSOC PROF-AY\", \"ASST PROF-AY\", \n                                 \"PROF-AY-B/E/E\", \"PROF-HCOMP\", \"ASST PROF-AY-B/E/E\", \n                                 \"ASSOC PROF-AY-B/E/E\", \"ASSOC PROF-HCOMP\", \"ASST PROF-HCOMP\"))\n# remove those making less than 30k (probably there only for a period)\ndt <- dt %>% filter(pay > 30000)\ndt## # A tibble: 4,915 × 5\n##    first_name       last_name     title           loc               pay\n##    <chr>            <chr>         <chr>           <chr>           <dbl>\n##  1 CHRISTOPHER U    ABANI         PROF-AY         Riverside     151200 \n##  2 HENRY DON ISAAC  ABARBANEL     PROF-AY         San Diego     160450.\n##  3 ADAM R           ABATE         ASST PROF-HCOMP San Francisco  85305.\n##  4 KEVORK N.        ABAZAJIAN     ASST PROF-AY    Irvine         82400.\n##  5 M. ACKBAR        ABBAS         PROF-AY         Irvine        168700.\n##  6 ABUL K           ABBAS         PROF-HCOMP      San Francisco 286824.\n##  7 LEONARD J        ABBEDUTO      PROF-HCOMP      Davis         200385.\n##  8 DON P            ABBOTT        PROF-AY         Davis         106400.\n##  9 GEOFFREY WINSTON ABBOTT        PROF-HCOMP      Irvine        125001.\n## 10 KHALED A.S.      ABDEL-GHAFFAR PROF-AY-B/E/E   Davis         120100.\n## # … with 4,905 more rows\ndt %>% ggplot() + aes(x = pay) + geom_histogram(binwidth = 10000)\ndt %>% ggplot() + aes(x = log2(pay)) + geom_histogram(binwidth = 0.5)\nunscaled <- lm(pay ~ title + loc, data = dt)\nsummary(unscaled)## \n## Call:\n## lm(formula = pay ~ title + loc, data = dt)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -149483  -25197   -1679   18305  213684 \n## \n## Coefficients:\n##                          Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)                 98397       2003  49.133  < 2e-16 ***\n## titleASSOC PROF-AY-B/E/E    46898       3402  13.786  < 2e-16 ***\n## titleASSOC PROF-HCOMP       25428       3955   6.430 1.40e-10 ***\n## titleASST PROF-AY          -15060       2370  -6.356 2.26e-10 ***\n## titleASST PROF-AY-B/E/E     17405       3949   4.407 1.07e-05 ***\n## titleASST PROF-HCOMP         5545       4800   1.155  0.24805    \n## titlePROF-AY                46095       1719  26.815  < 2e-16 ***\n## titlePROF-AY-B/E/E          73586       2283  32.233  < 2e-16 ***\n## titlePROF-HCOMP            115094       2356  48.855  < 2e-16 ***\n## locDavis                   -19101       2304  -8.291  < 2e-16 ***\n## locIrvine                  -12240       2351  -5.206 2.01e-07 ***\n## locLos Angeles               7699       2082   3.697  0.00022 ***\n## locMerced                  -20940       4484  -4.669 3.10e-06 ***\n## locRiverside               -18333       2893  -6.337 2.56e-10 ***\n## locSan Diego               -11851       2227  -5.322 1.07e-07 ***\n## locSan Francisco           -15808       3493  -4.525 6.17e-06 ***\n## locSanta Barbara           -16579       2411  -6.877 6.89e-12 ***\n## locSanta Cruz              -24973       2930  -8.523  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 40970 on 4897 degrees of freedom\n## Multiple R-squared:  0.5058, Adjusted R-squared:  0.504 \n## F-statistic: 294.8 on 17 and 4897 DF,  p-value: < 2.2e-16\ngg_diagnose(lm(pay ~ title + loc, data = dt))\nscaled <- lm(log2(pay) ~ title + loc, data = dt)\nsummary(scaled)## \n## Call:\n## lm(formula = log2(pay) ~ title + loc, data = dt)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -2.23150 -0.22355  0.01801  0.25702  1.24529 \n## \n## Coefficients:\n##                          Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)              16.52889    0.02037 811.287  < 2e-16 ***\n## titleASSOC PROF-AY-B/E/E  0.52397    0.03461  15.141  < 2e-16 ***\n## titleASSOC PROF-HCOMP     0.34517    0.04023   8.579  < 2e-16 ***\n## titleASST PROF-AY        -0.29772    0.02411 -12.351  < 2e-16 ***\n## titleASST PROF-AY-B/E/E   0.18997    0.04017   4.729 2.32e-06 ***\n## titleASST PROF-HCOMP      0.06826    0.04883   1.398  0.16220    \n## titlePROF-AY              0.56942    0.01749  32.562  < 2e-16 ***\n## titlePROF-AY-B/E/E        0.81217    0.02322  34.971  < 2e-16 ***\n## titlePROF-HCOMP           1.12262    0.02397  46.841  < 2e-16 ***\n## locDavis                 -0.20826    0.02344  -8.886  < 2e-16 ***\n## locIrvine                -0.14533    0.02392  -6.075 1.33e-09 ***\n## locLos Angeles            0.06309    0.02118   2.979  0.00291 ** \n## locMerced                -0.24781    0.04562  -5.432 5.84e-08 ***\n## locRiverside             -0.22030    0.02943  -7.485 8.43e-14 ***\n## locSan Diego             -0.14584    0.02266  -6.437 1.33e-10 ***\n## locSan Francisco         -0.11260    0.03554  -3.168  0.00154 ** \n## locSanta Barbara         -0.20706    0.02453  -8.442  < 2e-16 ***\n## locSanta Cruz            -0.29716    0.02981  -9.969  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.4168 on 4897 degrees of freedom\n## Multiple R-squared:  0.5372, Adjusted R-squared:  0.5356 \n## F-statistic: 334.3 on 17 and 4897 DF,  p-value: < 2.2e-16\ngg_diagnose(lm(log2(pay) ~ title + loc, data = dt))"},{"path":"anova.html","id":"anova","chapter":"Lecture 10 ANOVA","heading":"Lecture 10 ANOVA","text":"","code":"\nlibrary(tidyverse) # our friend the tidyverse"},{"path":"anova.html","id":"analysis-of-variance","chapter":"Lecture 10 ANOVA","heading":"10.1 Analysis of variance","text":"ANOVA method testing hypothesis difference means subsets measurements grouped factors. Essentially, generalization linear regression categorical explanatory variables instead numeric variables, based similar assumptions.ANOVA perform best particular experimental design: ) divide population groups equal size (balanced design); b) assign “treatments” subjects random (randomized design); case multiple treatment combinations, perform experiment combination (factorial design); cases, “null” treatment (e.g., placebo).speak one-way ANOVA single axis variation treatment (e.g., intervention, option , option B), two-way ANOVA apply two treatments group (e.g., treatment, Ab, AB, aB), forth. Extensions include ANCOVA (ANalysis COVAriance) MANOVA (Multivariate ANalysis VAriance).example, want test whether COVID vaccine protects infection. can assign random population volunteers two classes (vaccine/placebo) contrast number people got sick within 3 months treatment two classes. course, can simply perform t-test. assign people different classes (e.g., M/F, /65 y/o), want contrast mean infection rate across classes?","code":""},{"path":"anova.html","id":"anova-assumptions","chapter":"Lecture 10 ANOVA","heading":"10.1.1 ANOVA assumptions","text":"ANOVA tests whether samples taken distributions mean:Null hypothesis: means different groups .Alternative hypothesis: least one sample mean equal others.Let \\(Y\\) indicate response variable, study simplest case one-way ANOVA. divided samples \\(k\\) classes size \\(J_1, \\ldots, J_k\\) \\(n=\\sum_i J_i\\). write equation \\(Y_{ij}\\) (\\(j\\)th observation group \\(\\)):\\[\nY_{ij} = \\mu + \\alpha_i + \\epsilon_{ij}\n\\]\\(\\mu\\) overall mean; \\(\\mu + \\alpha_i\\) mean group \\(\\)—can always choose parameters \\(\\sum_i \\alpha_i = 0\\); , finally, \\(\\epsilon_{ij}\\) deviation group mean. Practically, testing whether least one \\(\\alpha_i \\neq 0\\).Note making assumptions linear regression:observations obtained independently (randomly) population defined factor levels (groups)measurements factor level independent normally distributedThese normal distributions variance","code":""},{"path":"anova.html","id":"how-one-way-anova-works","chapter":"Lecture 10 ANOVA","heading":"10.1.2 How one-way ANOVA works","text":"\\(k\\) groups, define overall mean \\(\\bar{y} = \\sum_i \\sum_j Y_{ij} / J_{}\\), \\(J_i\\) sample size group \\(\\). total sum squared deviations (SSD) simply:\\[\nSSD = \\sum_{= 1}^k \\sum_{j = 1}^{J_{}} \\left(Y_{ij} - \\bar{y}\\right)^2\n\\]associated degrees freedom \\(n-1\\). car rewrite :\\[\nSSD = \\sum_{= 1}^k J_{} \\left(\\bar{y}_i - \\bar{y} \\right)^2 + \\sum_{j = 1}^{J_{}} \\left(Y_{ij} - \\bar{y}_i \\right)^2\n\\]now \\(\\bar{y}_i\\) mean samples treatment (factor, group) \\(\\). call first term r.h.s. treatment sum squares (SST) second term within treatment (residual) ssq (SSE). \\(SSD = SST + SSE\\). Similarly, can decompose SSD :\\[\nSSD = \\sum_{j = 1}^{J_{}} Y_{ij}^2 - n \\bar{y}^2 = TSS - SSA\n\\]\nnow TSS total sum squares SSA sum squares due average. Combining two equations, can rewrite TSS sum three components:\\[\nTSS = SSA + SST + SSE\n\\]Note degrees freedom associated term \\(1\\), \\(k-1\\) \\(n-k\\), respectively. remains proved conduct inference.","code":""},{"path":"anova.html","id":"inference-in-one-way-anova","chapter":"Lecture 10 ANOVA","heading":"10.2 Inference in one-way ANOVA","text":"null hypothesis true, expect -treatment “variance” SST, divided degrees freedom (\\(k-1\\)) within-treatment “variance” divided \\(n-k\\).Let’s look hypothesis closely. null hypothesis true, \\(SST\\) sum squares independent, normally distributed random variables mean zero variance \\(\\sigma^2\\). remember, distribution :\\[\nQ = \\sum_{=1}^d Z_i^2 \\sim \\chi^2(d)\n\\]called \\(\\chi^2\\) distribution \\(d\\) degrees freedom. , null hypothesis, \\(SST \\sim \\chi^2(k-1)\\) Similarly, \\(SSE \\sim \\chi^2(n-k)\\). Taking ratio (normalized using degrees freedom), obtain:\\[\n\\frac{SST}{k-1} \\frac{n-k}{SSE} = \\frac{MST}{MSE} \\sim F(k-1, n-k)\n\\]\n\\(F\\) F-distribution (R, can sample distribution using df(x, deg1, deg2)).","code":""},{"path":"anova.html","id":"example-of-comparing-diets","chapter":"Lecture 10 ANOVA","heading":"10.2.1 Example of comparing diets","text":"example, following data contains measurements weights individuals starting diet, 6 weeks dieting, type diet (1, 2, 3), variables.Write script using ggplot generate boxplots weights three different diets.can see weight loss outcomes vary diet, diet 3 seems produce larger effect average. difference means/medians actually due diet, produced sampling distribution, given see substantial variation within diet group?result running ANOVA given data set:","code":"\nlibrary(tidyverse)\n# Original URL: \"https://www.sheffield.ac.uk/polopoly_fs/1.570199!/file/stcp-Rdataset-Diet.csv\"\ndiet <- read_csv(\"https://tinyurl.com/ydzya2no\") \ndiet <- diet %>% mutate(weight.loss = pre.weight - weight6weeks) \nglimpse(diet)## Rows: 78\n## Columns: 8\n## $ Person       <dbl> 25, 26, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 27…\n## $ gender       <dbl> NA, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ Age          <dbl> 41, 32, 22, 46, 55, 33, 50, 50, 37, 28, 28, 45, 60, 48, 4…\n## $ Height       <dbl> 171, 174, 159, 192, 170, 171, 170, 201, 174, 176, 165, 16…\n## $ pre.weight   <dbl> 60, 103, 58, 60, 64, 64, 65, 66, 67, 69, 70, 70, 72, 72, …\n## $ Diet         <dbl> 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, …\n## $ weight6weeks <dbl> 60.0, 103.0, 54.2, 54.0, 63.3, 61.1, 62.2, 64.0, 65.0, 60…\n## $ weight.loss  <dbl> 0.0, 0.0, 3.8, 6.0, 0.7, 2.9, 2.8, 2.0, 2.0, 8.5, 1.9, 3.…\n# make diet into factors\ndiet <- diet %>% mutate(Diet = factor(Diet))\ndiet %>% ggplot() + \n  aes(y = weight.loss, \n      x = Diet, \n      fill = Diet) + \n  geom_boxplot()\ndiet_anova  <-  aov(weight.loss ~ Diet, data=diet) # note that this looks like lm!\nsummary(diet_anova)##             Df Sum Sq Mean Sq F value  Pr(>F)   \n## Diet         2   71.1   35.55   6.197 0.00323 **\n## Residuals   75  430.2    5.74                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nprint(diet_anova)## Call:\n##    aov(formula = weight.loss ~ Diet, data = diet)\n## \n## Terms:\n##                     Diet Residuals\n## Sum of Squares   71.0937  430.1793\n## Deg. of Freedom        2        75\n## \n## Residual standard error: 2.394937\n## Estimated effects may be unbalanced"},{"path":"anova.html","id":"comparison-of-theory-and-anova-output","chapter":"Lecture 10 ANOVA","heading":"10.2.2 Comparison of theory and ANOVA output","text":"Let’s compare calculations data set:Now numbers place, can compute F-statistics, associated p-value:Contrast output aov:first glance, process fitting parameters linear regression, based exactly assumptions: additive noise additive effect factors, difference factors numeric, effect one added separately. One can run linear regression calculate coefficients identical mean differences means computed ANOVA (note p-values !)","code":"\n# 1) compute the overall mean\nbar_y <- diet %>% \n  summarise(bar_y = mean(weight.loss)) %>% \n  as.numeric()\n# 2) compute means by diet and sample size by diet\nbar_y_i <- diet %>% \n  group_by(Diet) %>% \n  summarise(bar_y_i = mean(weight.loss),\n            J_i = n())\n#(NB: almost balanced!)\n\n# 3) compute degrees of freedom\nn <- nrow(diet)\nk <- nrow(diet %>% select(Diet) %>% distinct())\ndeg_freedom <- c(1, k - 1, n - k)\n# 4) compute SSA, SST and SSE\nSSA <- n * bar_y^2\nSST <- bar_y_i %>% \n  mutate(tmp = J_i * (bar_y_i - bar_y)^2) %>% \n  summarise(sst = sum(tmp)) %>% \n  as.numeric()\nSSE <- diet %>% \n  inner_join(bar_y_i, by = \"Diet\") %>% \n  mutate(tmp = (weight.loss - bar_y_i)^2) %>% \n  summarise(sse = sum(tmp)) %>% \n  as.numeric()\n# 5) show that TSS = SSA + SST + SSE\nTSS <- diet %>% \n  summarise(tss = sum(weight.loss^2)) %>% \n  as.numeric()\nprint(c(TSS, SSA + SST + SSE))## [1] 1654.35 1654.35\nFs <- (SST / (deg_freedom[2])) / (SSE / (deg_freedom[3]))\npval <- 1 - pf(Fs, deg_freedom[2], deg_freedom[3])\nprint(deg_freedom[-1])## [1]  2 75\nprint(c(SST, SSE))## [1]  71.09369 430.17926\nprint(c(Fs, pval))## [1] 6.197447453 0.003229014\nprint(summary(aov(weight.loss ~ Diet, data = diet)))##             Df Sum Sq Mean Sq F value  Pr(>F)   \n## Diet         2   71.1   35.55   6.197 0.00323 **\n## Residuals   75  430.2    5.74                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ndiet.lm <- lm(weight.loss ~ Diet, data=diet)\nsummary(diet.lm)## \n## Call:\n## lm(formula = weight.loss ~ Diet, data = diet)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.1259 -1.3815  0.1759  1.6519  5.7000 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   3.3000     0.4889   6.750 2.72e-09 ***\n## Diet2        -0.2741     0.6719  -0.408  0.68449    \n## Diet3         1.8481     0.6719   2.751  0.00745 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.395 on 75 degrees of freedom\n## Multiple R-squared:  0.1418, Adjusted R-squared:  0.1189 \n## F-statistic: 6.197 on 2 and 75 DF,  p-value: 0.003229\nprint(diet.lm$coefficients)## (Intercept)       Diet2       Diet3 \n##   3.3000000  -0.2740741   1.8481481"},{"path":"anova.html","id":"further-steps","chapter":"Lecture 10 ANOVA","heading":"10.3 Further steps","text":"","code":""},{"path":"anova.html","id":"post-hoc-analysis","chapter":"Lecture 10 ANOVA","heading":"10.3.1 Post-hoc analysis","text":"ANOVA F-test tells us whether difference values response variable groups, specify group(s) different. , post-hoc test used (Tukey’s “Honest Significant Difference”):compares three pairs groups reports p-value hypothesis particular pair difference response variable.","code":"\ntuk <- TukeyHSD(diet_anova)\ntuk##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = weight.loss ~ Diet, data = diet)\n## \n## $Diet\n##           diff        lwr      upr     p adj\n## 2-1 -0.2740741 -1.8806155 1.332467 0.9124737\n## 3-1  1.8481481  0.2416067 3.454690 0.0201413\n## 3-2  2.1222222  0.5636481 3.680796 0.0047819"},{"path":"anova.html","id":"example-of-plant-growth-data","chapter":"Lecture 10 ANOVA","heading":"10.3.2 Example of plant growth data","text":"Example taken : One-Way ANOVA Test RExercise: perform ANOVA Tukey’s HSD interpret results.","code":"\nmy_data <- PlantGrowth # import built-in data\nhead(my_data)##   weight group\n## 1   4.17  ctrl\n## 2   5.58  ctrl\n## 3   5.18  ctrl\n## 4   6.11  ctrl\n## 5   4.50  ctrl\n## 6   4.61  ctrl\n# Show the levels\nmy_data %>% select(group) %>% distinct()##   group\n## 1  ctrl\n## 2  trt1\n## 3  trt2\ngroup_by(my_data, group) %>%\n  summarise(\n    count = n(),\n    mean = mean(weight, na.rm = TRUE),\n    sd = sd(weight, na.rm = TRUE)\n  )## # A tibble: 3 × 4\n##   group count  mean    sd\n##   <fct> <int> <dbl> <dbl>\n## 1 ctrl     10  5.03 0.583\n## 2 trt1     10  4.66 0.794\n## 3 trt2     10  5.53 0.443\nmy_data %>% \n  ggplot() + \n  aes(y = weight, x = group, \n      fill = group) + \n  geom_boxplot()"},{"path":"anova.html","id":"two-way-anova","chapter":"Lecture 10 ANOVA","heading":"10.3.3 Two-way ANOVA","text":"One can compare effect two different factors simultaneously see considering explains variance one. equivalent multiple linear regression two interacting variables. interpret results?","code":"\ndiet.fisher <- aov(weight.loss ~ Diet * gender, data = diet)\nsummary(diet.fisher)##             Df Sum Sq Mean Sq F value  Pr(>F)   \n## Diet         2   60.5  30.264   5.629 0.00541 **\n## gender       1    0.2   0.169   0.031 0.85991   \n## Diet:gender  2   33.9  16.952   3.153 0.04884 * \n## Residuals   70  376.3   5.376                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## 2 observations deleted due to missingness"},{"path":"anova.html","id":"investigate-the-uc-salaries-dataset","chapter":"Lecture 10 ANOVA","heading":"10.4 Investigate the UC salaries dataset","text":"Plot distributions pay location title. approximately normal? , transform data.Plot distributions pay location title. approximately normal? , transform data.Run ANOVA pay dependent two factors separately, report variance means variance within groups, p-value null hypothesis.Run ANOVA pay dependent two factors separately, report variance means variance within groups, p-value null hypothesis.Run Tukey’s test multiple comparison means report group(s) substantially different rest, .Run Tukey’s test multiple comparison means report group(s) substantially different rest, .Run two-way ANOVA location title provide interpretation.Run two-way ANOVA location title provide interpretation.","code":"\n# read the data\n# Original URL\ndt <- read_csv(\"https://raw.githubusercontent.com/dailybruin/uc-salaries/master/data/uc_salaries.csv\", \ncol_names = c(\"first_name\", \"last_name\", \"title\", \"a\", \"pay\", \"loc\", \"year\", \"b\", \"c\", \"d\")) %>%  select(first_name, last_name, title, loc, pay)## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   first_name = col_character(),\n##   last_name = col_character(),\n##   title = col_character(),\n##   a = col_double(),\n##   pay = col_double(),\n##   loc = col_character(),\n##   year = col_double(),\n##   b = col_double(),\n##   c = col_double(),\n##   d = col_double()\n## )\n# get only profs\ndt <- dt %>% filter(title %in% c(\"PROF-AY\", \"ASSOC PROF-AY\", \"ASST PROF-AY\", \n                                 \"PROF-AY-B/E/E\", \"PROF-HCOMP\", \"ASST PROF-AY-B/E/E\", \n                                 \"ASSOC PROF-AY-B/E/E\", \"ASSOC PROF-HCOMP\", \"ASST PROF-HCOMP\"))\n# simplify titles\ndt <- dt %>% mutate(title = ifelse(grepl(\"ASST\", title), \"Assistant\", title))\ndt <- dt %>% mutate(title = ifelse(grepl(\"ASSOC\", title), \"Associate\", title))\ndt <- dt %>% mutate(title = ifelse(grepl(\"PROF\", title), \"Full\", title))\n# remove those making less than 50k (probably there only for a period)\ndt <- dt %>% filter(pay > 50000)\nglimpse(dt)## Rows: 4,795\n## Columns: 5\n## $ first_name <chr> \"CHRISTOPHER U\", \"HENRY DON ISAAC\", \"ADAM R\", \"KEVORK N.\", …\n## $ last_name  <chr> \"ABANI\", \"ABARBANEL\", \"ABATE\", \"ABAZAJIAN\", \"ABBAS\", \"ABBAS…\n## $ title      <chr> \"Full\", \"Full\", \"Assistant\", \"Assistant\", \"Full\", \"Full\", \"…\n## $ loc        <chr> \"Riverside\", \"San Diego\", \"San Francisco\", \"Irvine\", \"Irvin…\n## $ pay        <dbl> 151200.00, 160450.08, 85305.01, 82400.04, 168699.96, 286823…"},{"path":"anova.html","id":"a-word-of-caution-about-unbalanced-designs","chapter":"Lecture 10 ANOVA","heading":"10.4.1 A word of caution about unbalanced designs","text":"different number samples category, might encounter problems, order enter terms might matter: example, run aov(pay ~ title + loc) vs. aov(pay ~ loc + title), see sum squares two models differ. cases, might lead puzzling results—depending enter model, might determine treatment effect . Turns , three different ways account sum--squares ANOVA, testing slightly different hypotheses. balanced designs, return answer, different sizes, please read .","code":""},{"path":"time-series-modeling-and-forecasting.html","id":"time-series-modeling-and-forecasting","chapter":"Lecture 11 Time series: modeling and forecasting","heading":"Lecture 11 Time series: modeling and forecasting","text":"Prediction difficult, especially future.— Niels Bohr (apocryphally)","code":"\nlibrary(tsibble)\nlibrary(tsibbledata)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(fable)\nlibrary(feasts)\nlibrary(forecast)\nlibrary(fpp2)"},{"path":"time-series-modeling-and-forecasting.html","id":"goals","chapter":"Lecture 11 Time series: modeling and forecasting","heading":"11.1 Goals:","text":"Use current tools handling visualizing time seriesCalculate auto- cross-correlations time seriesDecompose time series componentsUse linear regression methods fitting forecasting","code":""},{"path":"time-series-modeling-and-forecasting.html","id":"time-series-format-and-plotting","chapter":"Lecture 11 Time series: modeling and forecasting","heading":"11.2 Time series format and plotting","text":"time series special data set observation associated time measurement. special R structure storing operating time series, called ts, illustrated :reads data set recording number births per month New York City, 1946 1958 (thousands births?) create time series, give function frequency, number time points year, starting value vector assigned start = c(1946, 1), first element year second month.several specialized data structures handling time series. use one new set R packages called tidyverts called tsibble. https://tsibble.tidyverts.org/ tsibbledata package contains several time series data sets load two : olympic_running, containing Olympic winning times different running events, pelt, containing classic data number Lynx Hare pelts sold Canada 19th early 20th centuries:can see tsibbles contain multiple variables, one time variable called index, one others denoted key, variables identify observations changing time. example, Olympic times data set, Sex Length keys, measurement ’re plotting time. pelt data set, key variable (make longer tsibble species key variable wanted.)","code":"\nbirths <- scan(\"http://robjhyndman.com/tsdldata/data/nybirths.dat\")\nbirthstimeseries <- ts(births, frequency = 12, start = c(1946, 1))\nbirthstimeseries##         Jan    Feb    Mar    Apr    May    Jun    Jul    Aug    Sep    Oct\n## 1946 26.663 23.598 26.931 24.740 25.806 24.364 24.477 23.901 23.175 23.227\n## 1947 21.439 21.089 23.709 21.669 21.752 20.761 23.479 23.824 23.105 23.110\n## 1948 21.937 20.035 23.590 21.672 22.222 22.123 23.950 23.504 22.238 23.142\n## 1949 21.548 20.000 22.424 20.615 21.761 22.874 24.104 23.748 23.262 22.907\n## 1950 22.604 20.894 24.677 23.673 25.320 23.583 24.671 24.454 24.122 24.252\n## 1951 23.287 23.049 25.076 24.037 24.430 24.667 26.451 25.618 25.014 25.110\n## 1952 23.798 22.270 24.775 22.646 23.988 24.737 26.276 25.816 25.210 25.199\n## 1953 24.364 22.644 25.565 24.062 25.431 24.635 27.009 26.606 26.268 26.462\n## 1954 24.657 23.304 26.982 26.199 27.210 26.122 26.706 26.878 26.152 26.379\n## 1955 24.990 24.239 26.721 23.475 24.767 26.219 28.361 28.599 27.914 27.784\n## 1956 26.217 24.218 27.914 26.975 28.527 27.139 28.982 28.169 28.056 29.136\n## 1957 26.589 24.848 27.543 26.896 28.878 27.390 28.065 28.141 29.048 28.484\n## 1958 27.132 24.924 28.963 26.589 27.931 28.009 29.229 28.759 28.405 27.945\n## 1959 26.076 25.286 27.660 25.951 26.398 25.565 28.865 30.000 29.261 29.012\n##         Nov    Dec\n## 1946 21.672 21.870\n## 1947 21.759 22.073\n## 1948 21.059 21.573\n## 1949 21.519 22.025\n## 1950 22.084 22.991\n## 1951 22.964 23.981\n## 1952 23.162 24.707\n## 1953 25.246 25.180\n## 1954 24.712 25.688\n## 1955 25.693 26.881\n## 1956 26.291 26.987\n## 1957 26.634 27.735\n## 1958 25.912 26.619\n## 1959 26.992 27.897\ndata(\"olympic_running\")\nglimpse(olympic_running)## Rows: 312\n## Columns: 4\n## Key: Length, Sex [14]\n## $ Year   <int> 1896, 1900, 1904, 1908, 1912, 1916, 1920, 1924, 1928, 1932, 193…\n## $ Length <int> 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100…\n## $ Sex    <chr> \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", \"men\", …\n## $ Time   <dbl> 12.00, 11.00, 11.00, 10.80, 10.80, NA, 10.80, 10.60, 10.80, 10.…\ndata(\"pelt\")\nglimpse(pelt)## Rows: 91\n## Columns: 3\n## $ Year <dbl> 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855,…\n## $ Hare <dbl> 19580, 19600, 19610, 11990, 28040, 58000, 74600, 75090, 88480, 61…\n## $ Lynx <dbl> 30090, 45150, 49150, 39520, 21230, 8420, 5560, 5080, 10170, 19600…"},{"path":"time-series-modeling-and-forecasting.html","id":"visualizing-the-data","chapter":"Lecture 11 Time series: modeling and forecasting","heading":"11.2.1 Visualizing the data","text":"straightforward way visualizing time series using time plot, can created using autoplot:autoplot works best timeseries objects, use tsibble can convert ts first, like :One can also use ggplot directly, example plot faceted graph olympic times different years:","code":"\nautoplot(birthstimeseries) +\n  ggtitle(\"Number of births in NYC\") +\n  ylab(\"Births (thousands)\") +\n  xlab(\"Year\")\nautoplot(as.ts(pelt)) +\n  ggtitle(\"Lynx and hare pelts\") +\n  ylab(\"Number of pelts\") +\n  xlab(\"Year\")\nolympic_running %>% as_tibble %>%\n  ggplot(aes(x=Year, y = Time, colour = Sex)) +\n  geom_line() +\n  facet_wrap(~ Length, scales = \"free_y\")"},{"path":"time-series-modeling-and-forecasting.html","id":"decomposition-of-time-series","chapter":"Lecture 11 Time series: modeling and forecasting","heading":"11.3 Decomposition of time series","text":"can see plots , time series often combination different time-dependent processes often useful think separately. One can distinguish three types time variation time series:Seasonality\nseasonal pattern occurs time series affected seasonal factors time year day week. Seasonality always fixed known frequency. monthly sales antidiabetic drugs shows seasonality induced partly change cost drugs end calendar year.Seasonality\nseasonal pattern occurs time series affected seasonal factors time year day week. Seasonality always fixed known frequency. monthly sales antidiabetic drugs shows seasonality induced partly change cost drugs end calendar year.Trend\ntrend exists long-term increase decrease data. linear. Sometimes refer trend “changing direction,” might go increasing trend decreasing trend.Trend\ntrend exists long-term increase decrease data. linear. Sometimes refer trend “changing direction,” might go increasing trend decreasing trend.Cyclic\ncycle occurs data exhibit rises falls fixed frequency. economics, fluctuations may due economic conditions often related “business cycle.”Cyclic\ncycle occurs data exhibit rises falls fixed frequency. economics, fluctuations may due economic conditions often related “business cycle.”","code":""},{"path":"time-series-modeling-and-forecasting.html","id":"decomposition-of-time-series-1","chapter":"Lecture 11 Time series: modeling and forecasting","heading":"11.3.1 Decomposition of time series","text":"two main types decompositions time series: additive multiplicative. Let us call \\(X_t\\) time series data, \\(T_t\\) trend (non-periodic component), \\(S_t\\) seasonal part (periodic component), \\(R_t\\) remainder.\\[\nX_t = T_t + S_t + R_t\n\\]\n\\[\nX_t = T_t \\times S_t \\times R_t\n\\]One simple way removing seasonality estimating trend using moving average, using \\(k\\) points \\(k\\) points point calculate trend:\n\\[\nT_t =  \\frac{1}{m} \\sum_{=-k}^k X_{t+}\n\\]\\(m\\) called order moving average defined \\(m = 2k+1\\). useful function ma() calculates averages allows plotted.Exercise: Change moving average window see can make seasonality (periodicity) vanish!even order periodicity requires asymmetric averaging window, create symmetric average, one can repeat moving average order two already-averaged data.","code":"\nm <- 6\ns_name <- paste(\"MA-\", m)\nautoplot(birthstimeseries, series = \"data\") +\n  autolayer(ma(birthstimeseries, m), series = \"MA\") +\n  xlab(\"Time (Year)\") + ylab(\"number of births (thousands)\") +\n  ggtitle(\"NYC births time series\") \nm <- 10\nLynx <- as.ts(pelt$Lynx)\ns_name <- paste(\"MA-\", m)\nautoplot(Lynx, series = \"data\") +\n  autolayer(ma(Lynx, m), series = \"MA\") +\n  xlab(\"Year\") + ylab(\"number of pelts\") +\n  ggtitle(\"Lynx time series\") \nm <- 10\nHare <- as.ts(pelt$Hare)\ns_name <- paste(\"MA-\", m)\nautoplot(Hare, series = \"data\") +\n  autolayer(ma(Hare, m), series = \"MA\") +\n  xlab(\"Year\") + ylab(\"number of pelts\") +\n  ggtitle(\"Hare time series\") "},{"path":"time-series-modeling-and-forecasting.html","id":"classic-decomposition","chapter":"Lecture 11 Time series: modeling and forecasting","heading":"11.3.2 Classic decomposition:","text":"Additive decomposition [1]:m even number, compute trend-cycle component \\(T_t\\) using 2×m-MA. m odd number, compute trend-cycle component \\(\\hat T_t\\) using m-MA.Calculate detrended series: \\(X_t - \\hat T_t\\)estimate seasonal component season, average detrended values season. example, monthly data, seasonal component March average detrended March values data. seasonal component values adjusted ensure add zero. seasonal component obtained stringing together monthly values, replicating sequence year data. gives \\(\\hat S_t\\).remainder component calculated subtracting estimated seasonal trend-cycle components: $ R_t = X_t - T_t - S_t$simple classical decomposition numerous flaws, better, modern methods preferred. particular, assumes constant seasonal term, tends -estimate variation trend, misses data first last data points, can sensitive outliers.","code":"\nbirthstimeseries %>% decompose(type=\"additive\") %>%\n  autoplot() + xlab(\"Year\") +\n  ggtitle(\"Classic additive decomposition\n    of the NYC births time series\")"},{"path":"time-series-modeling-and-forecasting.html","id":"stl-decomposition","chapter":"Lecture 11 Time series: modeling and forecasting","heading":"11.3.3 STL decomposition","text":"robust method called STL decomposition (Seasonal Trend decomposition using Loess). summarize advantages [1]:STL can handle type seasonality, monthly quarterly data.seasonal component allowed change time, rate change can controlled user.smoothness trend-cycle can also controlled user.can robust outliers (.e., user can specify robust decomposition), occasional unusual observations affect estimates trend-cycle seasonal components. , however, affect remainder component.Exercise: Apply two decomposition methods pelt time series data.","code":"\nbirthstimeseries %>%  stl(t.window=13, s.window=\"periodic\", robust=TRUE) %>%\n  autoplot()"},{"path":"time-series-modeling-and-forecasting.html","id":"relationships-within-and-between-time-series","chapter":"Lecture 11 Time series: modeling and forecasting","heading":"11.4 Relationships within and between time series","text":"","code":""},{"path":"time-series-modeling-and-forecasting.html","id":"visualizing-correlation-between-different-variables","chapter":"Lecture 11 Time series: modeling and forecasting","heading":"11.4.1 Visualizing correlation between different variables","text":"following data set contains number visitors (visitor nights) quarterly basis five regions New South Wales, Australia:One simple question whether different variables related . One simple way calculate Pearson correlation different time series, called cross-correlation (\\(\\bar X\\) stands mean X \\(\\text{Var}(X)\\) stands variance \\(X\\)):\\[\n\\text{Cor}(X,Y) = \\frac{\\sum_t (\\bar X - X_t)(\\bar Y - Y_t)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}}\n\\]data set multiple variables can handy examine cross-correlations pairs . ’s convenient function calculate visualize multiple variables, case hotel visitor nights 5 different regions NSW. plot show scatter plots pairs time series , along respective correlation coefficients:However, sometimes straightforward correlation may miss underestimate relationship obviously , delay. Lynx Hare variables pelts data set correlated :correlation coefficient 0.3, appears relationship stronger plots. function CCF feasts package calculates correlations pair variables shifted (lagged) resulting plot:","code":"\nautoplot(visnights[,1:5]) +\n  ylab(\"Number of visitor nights each quarter (millions)\")\nhead(visnights)##         NSWMetro NSWNthCo NSWSthCo NSWSthIn NSWNthIn  QLDMetro QLDCntrl\n## 1998 Q1 9.047095 8.565678 5.818029 2.679538 2.977507 12.106052 2.748374\n## 1998 Q2 6.962126 7.124468 2.466437 3.010732 3.477703  7.786687 4.040915\n## 1998 Q3 6.871963 4.716893 1.928053 3.328869 3.014770 11.380024 5.343964\n## 1998 Q4 7.147293 6.269299 2.797556 2.417772 3.757972  9.311460 4.260419\n## 1999 Q1 7.956923 9.493901 4.853681 3.224285 3.790760 12.671942 4.186113\n## 1999 Q2 6.542243 5.401201 2.759843 2.428489 3.395284  9.582965 4.237806\n##         QLDNthCo SAUMetro SAUCoast  SAUInner VICMetro  VICWstCo VICEstCo\n## 1998 Q1 2.137234 2.881372 2.591997 0.8948773 7.490382 2.4420048 3.381972\n## 1998 Q2 2.269596 2.124736 1.375780 0.9792509 5.198178 0.9605047 1.827940\n## 1998 Q3 4.890227 2.284870 1.079542 0.9803289 5.244217 0.7559744 1.351952\n## 1998 Q4 2.621548 1.785889 1.497664 1.5094343 6.274246 1.2716040 1.493415\n## 1999 Q1 2.483203 2.293873 2.247684 0.9635227 9.187422 2.3850583 2.896929\n## 1999 Q2 3.377830 2.197418 1.672802 0.9968803 4.992303 1.3288638 1.547901\n##         VICInner WAUMetro WAUCoast  WAUInner OTHMetro OTHNoMet\n## 1998 Q1 5.326655 3.075779 3.066555 0.6949954 3.437924 2.073469\n## 1998 Q2 4.441119 2.154929 3.334405 0.5576796 2.677081 1.787939\n## 1998 Q3 3.815645 2.787286 4.365844 1.0061844 3.793743 2.345021\n## 1998 Q4 3.859567 2.752910 4.521996 1.1725514 3.304231 1.943689\n## 1999 Q1 4.588755 3.519564 3.579347 0.3981829 3.510819 2.165838\n## 1999 Q2 4.070401 3.160430 3.408533 0.5960182 2.871867 1.803940\nGGally::ggpairs(as.data.frame(visnights[,1:5]))## Registered S3 method overwritten by 'GGally':\n##   method from   \n##   +.gg   ggplot2\nhead(pelt)## # A tsibble: 6 x 3 [1Y]\n##    Year  Hare  Lynx\n##   <dbl> <dbl> <dbl>\n## 1  1845 19580 30090\n## 2  1846 19600 45150\n## 3  1847 19610 49150\n## 4  1848 11990 39520\n## 5  1849 28040 21230\n## 6  1850 58000  8420\nGGally::ggpairs(pelt[,2:3])\npelt %>%\n  CCF(Lynx, Hare, lag_max = 8) %>%\n  autoplot()"},{"path":"time-series-modeling-and-forecasting.html","id":"autocorrelation","chapter":"Lecture 11 Time series: modeling and forecasting","heading":"11.4.2 Autocorrelation","text":"Just saw cross-correlations, time series can correlated shifted time set amount, also called lagged. can plot lagged correlations different visitorHere colors indicate quarter variable vertical axis, compared shifted (lagged variable horizontal axis, lines connect points chronological order. relationship strongly positive lags 4 8, reflecting strong seasonality data.suggests strong similarity time series , shifted certain time values. described autocorrelation, defined function lag \\(k\\):\\[\nr(k) = \\frac{\\sum_{t=k}^T (\\bar X - X_t)(\\bar X - X_{t-k})}{\\text{Var}(X)}\n\\]autocorrelation can calculated plotted example visitation nights New South Wales:cases, lag plots closest diagonal identity line lag 9 10. reflected autocorrelation plot:Notice periodicity autocorrelation, indicated periodicity time series.Autocorrelation measures memory signal - example, pure white noise uncorrelated even moment later, thus memory. , useful measure trend data - time series slowly decaying, positive autocorrelation, indicates pronounced trend, periodicity indicates seasonality data.Exercise: Use lag autocorrelation analysis describe patterns time series births NYC.","code":"\nvisnights[,1]##          Qtr1     Qtr2     Qtr3     Qtr4\n## 1998 9.047095 6.962126 6.871963 7.147293\n## 1999 7.956923 6.542243 6.330364 7.509212\n## 2000 7.662491 6.341802 7.827301 9.579562\n## 2001 8.270488 7.240427 6.640490 7.111875\n## 2002 6.827826 6.404992 6.615760 7.226376\n## 2003 7.589058 6.334527 5.996748 6.612846\n## 2004 7.758267 6.778836 5.854452 6.200214\n## 2005 7.163830 5.082204 5.673551 6.089906\n## 2006 8.525916 6.569684 5.771059 6.692897\n## 2007 8.158658 5.710082 5.402543 6.803494\n## 2008 7.866269 5.616704 5.886764 5.506298\n## 2009 6.787225 5.361317 4.699350 6.208784\n## 2010 7.148262 4.850217 6.029490 6.238903\n## 2011 7.597468 5.815930 6.183239 5.929030\n## 2012 7.986931 5.307871 6.054112 6.023897\n## 2013 7.028480 5.813450 6.322841 7.101691\n## 2014 7.897316 5.997468 6.033533 7.103398\n## 2015 8.725132 6.995875 6.294490 6.945476\n## 2016 7.373757 6.792234 6.530568 7.878277\nvisnights_smaller <- window(visnights[,2], start=2000, end = 2010)\ngglagplot(visnights_smaller) \nggAcf(visnights_smaller)\ngglagplot(pelt$Lynx, lags =12) + ggtitle(\"Lag plots for the Lynx pelt data\")\ngglagplot(pelt$Hare, lags = 12) + ggtitle(\"Lag plots for the Hare pelt data\")\nggAcf(pelt$Lynx) + ggtitle(\"Autocorrelation for the Lynx pelt data\")\nggAcf(pelt$Hare) + ggtitle(\"Autocorrelation for the Hare pelt data\")"},{"path":"time-series-modeling-and-forecasting.html","id":"the-perennial-warning-beware-of-spurious-correlations","chapter":"Lecture 11 Time series: modeling and forecasting","heading":"11.4.3 The perennial warning: beware of spurious correlations!","text":"two data sets, Australian air passengers rice production Guinea, strong positive correlation:However, notice residuals indicate strong trend, violates assumptions linear regression.number fun examples spurious time series correlations reference [5].","code":"\naussies <- window(ausair, end=2011)\nfit <- tslm(aussies ~ guinearice)\nsummary(fit)## \n## Call:\n## tslm(formula = aussies ~ guinearice)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.9448 -1.8917 -0.3272  1.8620 10.4210 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   -7.493      1.203  -6.229 2.25e-07 ***\n## guinearice    40.288      1.337  30.135  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.239 on 40 degrees of freedom\n## Multiple R-squared:  0.9578, Adjusted R-squared:  0.9568 \n## F-statistic: 908.1 on 1 and 40 DF,  p-value: < 2.2e-16\ncheckresiduals(fit)## \n##  Breusch-Godfrey test for serial correlation of order up to 8\n## \n## data:  Residuals from Linear regression model\n## LM test = 28.813, df = 8, p-value = 0.000342"},{"path":"time-series-modeling-and-forecasting.html","id":"modeling-and-forecasting","chapter":"Lecture 11 Time series: modeling and forecasting","heading":"11.5 Modeling and forecasting","text":"","code":""},{"path":"time-series-modeling-and-forecasting.html","id":"forecasting-using-smoothing-methods","chapter":"Lecture 11 Time series: modeling and forecasting","heading":"11.5.1 Forecasting using smoothing methods","text":"package fable part tidyverts bundle can used easily produce forecasts using several standard methods, ASNAIVE (Seasonal Naive), ETS (exponential smoothing), ARIMA (AutoRegressive Integrated Moving Average). code compares three forecast models births NYC data:","code":"\nas_tsibble(birthstimeseries) %>% \n  model(\n    ets = ETS(box_cox(birthstimeseries, 0.3)),\n   arima = ARIMA(log(birthstimeseries)),\n    snaive = SNAIVE(birthstimeseries)\n  ) %>%\n  forecast(h = \"2 years\") %>% \n  autoplot(birthstimeseries)"},{"path":"time-series-modeling-and-forecasting.html","id":"forecasting-using-linear-regression","chapter":"Lecture 11 Time series: modeling and forecasting","heading":"11.5.2 Forecasting using linear regression","text":"Let us analyze data set US quarterly economic data, specifically, percent change consumption, income, production, savings, unemployment.Let us use four variables predictors consumption calculate multiple linear regression model using function tslm():can produce plot predicted values together observed data consumption:useful check residuals regression model:","code":"\nhead(uschange)##         Consumption     Income Production   Savings Unemployment\n## 1970 Q1   0.6159862  0.9722610 -2.4527003 4.8103115          0.9\n## 1970 Q2   0.4603757  1.1690847 -0.5515251 7.2879923          0.5\n## 1970 Q3   0.8767914  1.5532705 -0.3587079 7.2890131          0.5\n## 1970 Q4  -0.2742451 -0.2552724 -2.1854549 0.9852296          0.7\n## 1971 Q1   1.8973708  1.9871536  1.9097341 3.6577706         -0.1\n## 1971 Q2   0.9119929  1.4473342  0.9015358 6.0513418         -0.1\nautoplot(uschange[,c(\"Consumption\",\"Income\")]) +\n  ylab(\"% change\") + xlab(\"Year\")\nuschange %>%\n  as.data.frame() %>%\n  ggplot(aes(x=Income, y=Consumption)) +\n    ylab(\"Consumption (quarterly % change)\") +\n    xlab(\"Income (quarterly % change)\") +\n    geom_point() +\n    geom_smooth(method=\"lm\", se=FALSE)\nuschange %>%\n  as.data.frame() %>%\n  GGally::ggpairs()\nfit.consMR <- tslm(\n  Consumption ~ Income + Production + Unemployment + Savings,\n  data=uschange)\nsummary(fit.consMR)## \n## Call:\n## tslm(formula = Consumption ~ Income + Production + Unemployment + \n##     Savings, data = uschange)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.88296 -0.17638 -0.03679  0.15251  1.20553 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   0.26729    0.03721   7.184 1.68e-11 ***\n## Income        0.71449    0.04219  16.934  < 2e-16 ***\n## Production    0.04589    0.02588   1.773   0.0778 .  \n## Unemployment -0.20477    0.10550  -1.941   0.0538 .  \n## Savings      -0.04527    0.00278 -16.287  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.3286 on 182 degrees of freedom\n## Multiple R-squared:  0.754,  Adjusted R-squared:  0.7486 \n## F-statistic: 139.5 on 4 and 182 DF,  p-value: < 2.2e-16\nautoplot(uschange[,'Consumption'], series=\"Data\") +\n  autolayer(fitted(fit.consMR), series=\"Fitted\") +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Percent change in US consumption expenditure\") +\n  guides(colour=guide_legend(title=\" \"))\ncheckresiduals(fit.consMR)## \n##  Breusch-Godfrey test for serial correlation of order up to 8\n## \n## data:  Residuals from Linear regression model\n## LM test = 14.874, df = 8, p-value = 0.06163"},{"path":"time-series-modeling-and-forecasting.html","id":"forecasting-based-on-regression-models","chapter":"Lecture 11 Time series: modeling and forecasting","heading":"11.5.3 forecasting based on regression models","text":"One can distinguish true forecasting, termed ex-ante () prediction, truly try predict unknown future, ex-post forecasts, true values predictors response variable known. latter still useful validating models comparing different methods.package forecast contains tools make calculating predicted values time series simple. One can use model forecast values future, based different scenarios. example, may want investigate prediction economic upturn downturn:","code":"\nfit.consBest <- tslm(\n  Consumption ~ Income + Savings + Unemployment,\n  data = uschange)\n\nh <- 4\nnewdata <- data.frame(\n    Income = c(1, 1, 1, 1),\n    Savings = c(0.5, 0.5, 0.5, 0.5),\n    Unemployment = c(0, 0, 0, 0))\nfcast.up <- forecast::forecast(fit.consBest, newdata = newdata)\nnewdata <- data.frame(\n    Income = rep(-1, h),\n    Savings = rep(-0.5, h),\n    Unemployment = rep(0, h))\nfcast.down <- forecast::forecast(fit.consBest, newdata = newdata)\nautoplot(uschange[, 1]) +\n  ylab(\"% change in US consumption\") +\n  autolayer(fcast.up, PI = TRUE, series = \"increase\")  +\n  autolayer(fcast.down, PI = TRUE, series = \"decrease\") +\n  guides(colour = guide_legend(title = \"Scenario\"))"},{"path":"time-series-modeling-and-forecasting.html","id":"references-and-further-reading","chapter":"Lecture 11 Time series: modeling and forecasting","heading":"11.6 References and further reading:","text":"Rob J Hyndman George Athanasopoulos. Forecasting: Principles PracticeJonathan Cryer Kung-Sik Chan Time Series Analysis Applications RCross-validation forecastingTime series nested cross-validationSpurious correlations","code":""},{"path":"generalized-linear-models.html","id":"generalized-linear-models","chapter":"Lecture 12 Generalized linear models","heading":"Lecture 12 Generalized linear models","text":"","code":""},{"path":"generalized-linear-models.html","id":"goal-3","chapter":"Lecture 12 Generalized linear models","heading":"12.1 Goal","text":"Learn Generalized Linear Models (GLMs), able decide model appropriate problem hand.Let’s load packages:","code":"\nlibrary(tidyverse) # our friend the tidyverse\nlibrary(MASS) # negative binom regression"},{"path":"generalized-linear-models.html","id":"introduction","chapter":"Lecture 12 Generalized linear models","heading":"12.2 Introduction","text":"linear regression ’ve explored past weeks attempts estimate expected value response (dependent) variable \\(Y\\) given predictors \\(X\\). assumes response variable changes continuously, errors normally distributed around mean. many cases, however:response variable support whole real line (e.g., binary, count, positive values)errors normally distributed (e.g., response variable can take positive values)variance changes mean (heteroscedasticity)cases, can use Generalized Linear Models (GLMs) fit data. simplest form GLMs,response variable modeled single-parameter distribution exponential family (Gaussian, Gamma, Binomial, Poisson, etc.)link function linearizes relationship fitted values predictors.Parameters estimated least squares algorithm.","code":""},{"path":"generalized-linear-models.html","id":"model-structure","chapter":"Lecture 12 Generalized linear models","heading":"12.2.1 Model structure","text":"practice, need determine three parts model:Random component entries response variable (\\(Y\\)) assumed independently drawn certain distribution (e.g., Binomial)—typically distribution can modeled using single parameter.Systematic component explanatory variables (\\(X_1\\), \\(X_2\\), \\(\\ldots\\)) combined linearly form linear predictor (e.g., \\(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots\\)). explanatory variables can continuous, categorical, mixed.Link function \\(g(u)\\) specifies random systematic components connected.","code":""},{"path":"generalized-linear-models.html","id":"binary-data","chapter":"Lecture 12 Generalized linear models","heading":"12.3 Binary data","text":"extreme case departure normality response variable can assume values 0 1 (/yes, survived/deceased, lost/won, etc.). Bernoulli random variable can take values 0 1, therefore provides Random component model:\\[\nP(Y_i = y_i | \\pi_i) = \\pi_i^{y_i} (1 - \\pi_i)^{1 - y_i}\n\\]Saying probability \\(P(Y_i = 1) = \\pi_i\\), \\(P(Y_i = 0) = 1 - \\pi_i\\). Now want relate parameter \\(\\pi_i\\) linear predictor (.e., choose link function). can accomplished number ways.","code":""},{"path":"generalized-linear-models.html","id":"logistic-regression","chapter":"Lecture 12 Generalized linear models","heading":"12.3.1 Logistic regression","text":"popular choice use Logit function link function:\\[\n\\text{Logit}(\\pi_i) = \\beta_0 + \\beta_1 x_i \n\\]function can written :\\[\n\\text{Logit}(\\pi_i) = \\log\\left( \\frac{\\pi_i}{1 - \\pi_i} \\right) = \\log(\\pi_i) - \\log(1 - \\pi_i)\n\\]Practically, means \\[\n\\pi_i = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1 + e^{\\beta_0 + \\beta_1 x_i}} = 1 - \\frac{1}{1 + e^{\\beta_0 + \\beta_1 x_i}}\n\\]Clearly, \\(\\beta_0 + \\beta_1 x_i = 0\\), probability \\(\\pi_i = 1/2\\), probability tends 1 \\((\\beta_0 + \\beta_1 x_i) \\\\infty\\) zero \\((\\beta_0 + \\beta_1 x_i) \\-\\infty\\). :can see, logistic curve, hence name. parameters \\(\\beta_0\\) \\(\\beta_1\\) control location inflection point steepness curve, allowing model binary response variables (, slight abuse error structure, proportions probabilities).choices link functions possible. example, economics probit function preferred:\\[\n\\text{Probit}(\\pi_i) = \\beta_0 + \\beta_1 x_i\n\\]\\[\n\\text{Probit}(\\pi_i) = \\Phi(\\pi_i)\n\\]\n\\(\\Phi(\\cdot)\\) cumulative distribution function standard normal normal distribution:\\[\n\\Phi(z) = \\frac{1}{\\sqrt{2 \\pi}}\\int_{-\\infty}^z e^{\\frac{-t^2}{2}} dt\n\\]\nClearly, alternatively use cumulative distribution function distribution support real line.","code":"\n# some random data\nX <- rnorm(100)\nbeta_0 <- 0.35\nbeta_1 <- -3.2\nlinear_predictor <- beta_0 + beta_1 * X\npredicted_pi_i <- exp(linear_predictor) / (1 + exp(linear_predictor))\nggplot(data = tibble(linear_predictor = linear_predictor, probability = predicted_pi_i)) + \n  aes(x = linear_predictor, y = probability) + \n  geom_point() + geom_line()"},{"path":"generalized-linear-models.html","id":"a-simple-example","chapter":"Lecture 12 Generalized linear models","heading":"12.3.2 A simple example","text":"want know whether first, second third class, well gender (women women first!) influenced probability survival Titanic disaster. start null model (passengers probability survival):Now let’s include gender:best-fitting probability survival male/female?Now let’s see whether can explain better data using class:woman first class survival probability:man third class:Consider alternative models Survived ~ Sex * factor(Pclass), Survived ~ Sex + Pclass, Survived ~ Sex * Pclass, Survived ~ Sex:factor(Pclass), Survived ~ Sex:Pclass. Explain model English.","code":"\nlibrary(titanic)\n# model 0: probability of survival in general\n# regress against an intercept\nmodel0 <- glm(Survived ~ 1, # only intercept\n              data = titanic_train, \n              family = \"binomial\") # logistic regression\nsummary(model0)## \n## Call:\n## glm(formula = Survived ~ 1, family = \"binomial\", data = titanic_train)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -0.9841  -0.9841  -0.9841   1.3839   1.3839  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -0.47329    0.06889   -6.87  6.4e-12 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1186.7  on 890  degrees of freedom\n## Residual deviance: 1186.7  on 890  degrees of freedom\n## AIC: 1188.7\n## \n## Number of Fisher Scoring iterations: 4\n# the best fitting (alpha) intercept should lead to \n# e^alpha / (1 + e^alpha) = mean(Survived)\nmean(titanic_train$Survived)## [1] 0.3838384\nexp(model0$coefficients) / (1 + exp(model0$coefficients))## (Intercept) \n##   0.3838384\nmodel1 <- glm(Survived ~ Sex, # one sex as baseline, the other modifies intercept\n              data = titanic_train,\n              family = \"binomial\")\nsummary(model1)## \n## Call:\n## glm(formula = Survived ~ Sex, family = \"binomial\", data = titanic_train)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.6462  -0.6471  -0.6471   0.7725   1.8256  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)   1.0566     0.1290   8.191 2.58e-16 ***\n## Sexmale      -2.5137     0.1672 -15.036  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1186.7  on 890  degrees of freedom\n## Residual deviance:  917.8  on 889  degrees of freedom\n## AIC: 921.8\n## \n## Number of Fisher Scoring iterations: 4\ncoeffs <- model1$coefficients\n# prob women\nas.numeric(1 - 1 / (1 + exp(coeffs[1])))## [1] 0.7420382\n# prob men\nas.numeric(1 - 1 / (1 + exp(coeffs[1] + coeffs[2])))## [1] 0.1889081\nmodel2 <- glm(Survived ~ Sex + factor(Pclass), # combine Sex and Pclass\n              data = titanic_train,\n              family = \"binomial\")\nsummary(model2)## \n## Call:\n## glm(formula = Survived ~ Sex + factor(Pclass), family = \"binomial\", \n##     data = titanic_train)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.1877  -0.7312  -0.4476   0.6465   2.1681  \n## \n## Coefficients:\n##                 Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)       2.2971     0.2190  10.490  < 2e-16 ***\n## Sexmale          -2.6419     0.1841 -14.351  < 2e-16 ***\n## factor(Pclass)2  -0.8380     0.2447  -3.424 0.000618 ***\n## factor(Pclass)3  -1.9055     0.2141  -8.898  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1186.66  on 890  degrees of freedom\n## Residual deviance:  826.89  on 887  degrees of freedom\n## AIC: 834.89\n## \n## Number of Fisher Scoring iterations: 4\ncoeffs <- model2$coefficients\n# prob women first class\nas.numeric(1 - 1 / (1 + exp(coeffs[1])))## [1] 0.9086385\nas.numeric(1 - 1 / (1 + exp(coeffs[1] + coeffs[2] + coeffs[4])))## [1] 0.09532814"},{"path":"generalized-linear-models.html","id":"exercise-in-class-college-admissions","chapter":"Lecture 12 Generalized linear models","heading":"12.3.3 Exercise in class: College admissions","text":"slight abuse notation, can fit probabilities using logistic regression (problem don’t know many values contributed calculations probabilities—.e., sample sizes). Read file admission_rates.csv, containing data admissions several universities. goal find good prediction (good combination predictors) Admission_rate. can use State, Ownership (public/private), Citytype (town, suburb, city), SAT (typical SAT score admits), AvgCost (tuition). Fit models using:(worry warning non-integer #successes binomial glm!).Plot fitted vs. observed admission rates, using different combinations predictors.example :Score models using AIC: single best predictor acceptance rate? (Note: see later week, lower AIC, better).best combination two predictors?","code":"\ndt <- read_csv(\"data/admission_rates.csv\")## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   Name = col_character(),\n##   State = col_character(),\n##   Ownership = col_character(),\n##   Citytype = col_character(),\n##   SAT = col_double(),\n##   AvgCost = col_double(),\n##   Admission_rate = col_double()\n## )\n# example\nlogit_1 <- glm(Admission_rate ~ AvgCost, data = dt, family = \"binomial\")## Warning in eval(family$initialize): non-integer #successes in a binomial glm!\nplot(dt$Admission_rate, logit_1$fitted.values)\nabline(c(0,1))\nAIC(logit_1)## [1] 220.7783"},{"path":"generalized-linear-models.html","id":"count-data","chapter":"Lecture 12 Generalized linear models","heading":"12.4 Count data","text":"","code":""},{"path":"generalized-linear-models.html","id":"poisson-regression","chapter":"Lecture 12 Generalized linear models","heading":"12.4.1 Poisson regression","text":"Suppose response variables non-negative integers. example, counting number eggs females lay function age, body size, etc. possible model case think response variable sampled Poisson distribution:\\[\nY_i \\sim \\text{Pois}(\\lambda_i)\n\\]logarithm parameter \\(\\lambda_i\\) depends linearly predictors:\\[\n\\mathbb E[\\lambda_i] = \\mathbb E[\\log(Y_i|X_i)] = \\beta_0 + \\beta_1 X_i\n\\]case, link function logarithm, transforming relationship fitted values predictors linear regression.","code":""},{"path":"generalized-linear-models.html","id":"exercise-in-class-number-of-genomes","chapter":"Lecture 12 Generalized linear models","heading":"12.4.2 Exercise in class: Number of genomes","text":"file data/genomes.csv contains year genome given animal published. file sequence_cost.csv estimated cost per sequencing Mb given year.Count number genomes published per year (store value n) store tibble num_genomes along values Year Dollars_per_Mb (note: need use inner_join pull );Fit number genomes published given year:\nusing intercept (predictions match mean) (Code: pois_1 <- glm(n ~ 1, data = num_genomes, family = \"poisson\"))\nusing year predictor\nusing cost sequencing predictor\nusing intercept (predictions match mean) (Code: pois_1 <- glm(n ~ 1, data = num_genomes, family = \"poisson\"))using year predictorusing cost sequencing predictorFor model, plot observed n vs predicted value, compute AIC. fit superior use Year Dollars_per_Mb?","code":""},{"path":"generalized-linear-models.html","id":"underdispersed-and-overdispersed-data","chapter":"Lecture 12 Generalized linear models","heading":"12.4.3 Underdispersed and Overdispersed data","text":"main feature Poisson distribution mean variance equal \\(\\lambda\\). might remember (Taylor expansion) :\\[\ne^x = \\sum_{n = 0}^{\\infty} \\frac{x^n}{n!}\n\\], \\(X\\) sampled Poisson distribution:\\[\n\\begin{aligned}\n\\mathbb E[X] &= \\sum_{x = 0}^{\\infty} x P(X = x) \\\\\n&= \\sum_{x = 0}^{\\infty} x e^{-\\lambda} \\frac{\\lambda^x}{x!} \\\\\n&= \\lambda e^{-\\lambda} \\sum_{(x - 1) = 0}^{\\infty} \\frac{\\lambda^{(x-1)}}{(x-1)!} \\\\\n&= \\lambda e^{-\\lambda}e^{\\lambda} \\\\\n&= \\lambda\n\\end{aligned}\n\\]Similarly, using\\[\n\\begin{aligned}\n\\mathbb V[X] &= \\mathbb E[X^2]-\\mathbb E[X]^2\\\\\n&= \\left(\\sum_{x = 0}^{\\infty} x^2 e^{-\\lambda} \\frac{\\lambda^x}{x!} \\right) - \\lambda^2 \\\\\n&= \\ldots\\\\\n&= \\lambda\n\\end{aligned}\n\\]fact variance equals mean hard constraint, rarely matched real data. encounter -dispersion (.e., variance data much larger assumed Poisson), need choose different model. happens often, main solution use Negative Binomial Regression (negative binomial distribution can thought Poisson scaled variance). practice, amounts fitting:\\[\n\\mathbb E[\\lambda_i] = \\beta_0 + \\beta_1 X_i\n\\]\n\\[\n\\mathbb E[\\lambda_i^2] - \\mathbb E[\\lambda_i]^2 = \\mathbb V[\\lambda_i] = \\phi \\lambda_i\n\\]\n\\(\\phi\\) controls dispersion data. value \\(\\phi > 1\\) signals -dispersion, (rare case ) \\(\\phi < 1\\) -dispersion. Poisson regression appropriate \\(\\phi \\approx 1\\). simple way test dispersion fit quasipoisson model, returns dispersion parameter (anything larger 1 means -dispersion).","code":""},{"path":"generalized-linear-models.html","id":"exercise-in-class-number-of-genomes-1","chapter":"Lecture 12 Generalized linear models","heading":"12.4.4 Exercise in class: Number of genomes","text":"models , change family quasipoisson check dispersion (e.g., qpois_1 <- glm(n ~ 1, data = num_genomes, family = \"quasipoisson\")).-dispersion?data -dispersed, fit using glm.nb (negative binomial regression model provided package MASS).","code":""},{"path":"generalized-linear-models.html","id":"separate-distribution-for-the-zeros","chapter":"Lecture 12 Generalized linear models","heading":"12.4.5 Separate distribution for the zeros","text":"several biologically-relevant cases, excess zeros. example, might animals, , reach age 1, go live number years—say well-described Poisson distribution. However, mortality immediately birth high. cases, can use zero-inflated zero-hurdle models.zero-inflated models, can think conditional branching: probability \\(p_z\\) count zero; (prob. \\(1-p_z\\)) sampled given distribution. count zero can stem two different processes: either got zero first step, sampled zero distribution.Zero-hurdle models slightly different: first decide whether ’re going zero; , sample data truncated distribution, sample zero second source.Zero-inflated zero-hurdle models examples mixture models.","code":""},{"path":"generalized-linear-models.html","id":"other-glms","chapter":"Lecture 12 Generalized linear models","heading":"12.5 Other GLMs","text":"Historically, GLMs defined canonical families:Gaussian: linear regressionGamma Inverse Gaussian: Positive, continuousPoisson: count dataNegative Binomial: count data (fit ancillary parameter -dispersion)Binary/Binomial (logistic): binary responses; number successes; probabilities/proportions (slight abuse).However, basic idea led development “non-canonical” GLMs:Log-normal: Positive, continuousLog-gamma: survival modelsProbit: binaryand many others. Fitting models can done using Maximum Likelihoods, Bayesian framework (typically, MCMC).","code":""},{"path":"generalized-linear-models.html","id":"readings-and-homework","chapter":"Lecture 12 Generalized linear models","heading":"12.6 Readings and homework","text":"two useful swirls course Regression Models: Binary Outcomes Count OutcomesAn excellent book GLMs RRegression Models Count Data R","code":""},{"path":"model-selection.html","id":"model-selection","chapter":"Lecture 13 Model Selection","heading":"Lecture 13 Model Selection","text":"Cchiù longa è pinsata cchiù grossa è minchiata[longer thought, bigger bullshit]— Sicilian proverb","code":""},{"path":"model-selection.html","id":"goal-4","chapter":"Lecture 13 Model Selection","heading":"13.1 Goal","text":"data might want fit, several competing statistical models seem fairly good job. model use ?goal model selection provide disciplined way choose among competing models. consensus single technique perform model selection (examine alternative paradigms ), techniques inspired Occam’s razor: given models similar explanatory power, choose simplest.“simplest” mean? Measuring model’s “complexity” far trivial, hence different schools thought. approaches simply count number free parameters, penalize models parameters; others take account much parameter “fine-tuned” fit data; approaches based entirely different premises.choose simplest model? First, simpler models easier analyze, example make analytical headway mechanics process want model; simpler models also considered beautiful. Second, want avoid -fitting: biological data set—however carefully crafted—noisy, want fit signal, noise. include much flexibility model, get looks like excellent fit specific data set, unable fit data sets model also apply.","code":"\nlibrary(tidyverse) # our friend \nlibrary(BayesFactor) # Bayesian model selection\nlibrary(tidymodels)  # for the parsnip package, along with the rest of tidymodels\nlibrary(palmerpenguins)\n# Helper packages\n#library(readr)       # for importing data\nlibrary(broom.mixed) # for converting bayesian models to tidy tibbles\nlibrary(dotwhisker)  # for visualizing regression results"},{"path":"model-selection.html","id":"problems","chapter":"Lecture 13 Model Selection","heading":"13.2 Problems","text":"-fitting can lead wrong inference. (problem similar spurious correlations).-fitting can lead wrong inference. (problem similar spurious correlations).Identifiability parameters. Sometimes hard/impossible find best value set parameters. example, parameters appear sums products model. general, difficult prove set parameters leading maximum likelihood unique.Identifiability parameters. Sometimes hard/impossible find best value set parameters. example, parameters appear sums products model. general, difficult prove set parameters leading maximum likelihood unique.Finding best estimates. complex models, might difficult find best estimates set parameters. example, several areas parameter space yield good fit, good sets parameters separated areas poor fit. , might get “stuck” sub-optimal region parameters space.Finding best estimates. complex models, might difficult find best estimates set parameters. example, several areas parameter space yield good fit, good sets parameters separated areas poor fit. , might get “stuck” sub-optimal region parameters space.","code":""},{"path":"model-selection.html","id":"approaches-based-on-maximum-likelihoods","chapter":"Lecture 13 Model Selection","heading":"13.3 Approaches based on maximum-likelihoods","text":"start examining methods based maximum likelihoods. data set model, find best fitting parameters (maximizing likelihood). parameters said maximum-likelihood estimate.","code":""},{"path":"model-selection.html","id":"likelihood-function","chapter":"Lecture 13 Model Selection","heading":"13.3.1 Likelihood function","text":"notation:\\(D \\\\) observed data\\(\\theta \\\\) free parameter(s) statistical model\\(L(\\theta \\vert D) \\\\) likelihood function, read “likelihood \\(\\theta\\) given data”\\(\\hat{\\theta} \\\\) maximum-likelihood estimates (m.l.e.) parameters\\(\\mathcal L(\\theta \\vert D) = \\log L(\\theta \\vert D) \\\\) log-likelihood\\(L(\\hat{\\theta} \\vert D) \\\\) maximum likelihood","code":""},{"path":"model-selection.html","id":"discrete-probability-distributions-1","chapter":"Lecture 13 Model Selection","heading":"13.3.2 Discrete probability distributions","text":"simplest case probability distribution function takes discrete values. , likelihood \\(\\theta\\) given data simply probability obtaining data parameterizing model parameters \\(\\theta\\):\\[L(\\theta \\vert x_j) = P(X = x_j; \\theta)\\]Finding m.l.e. \\(\\theta\\) simply means finding value(s) maximizing probability recovering data model.","code":""},{"path":"model-selection.html","id":"continuous-probability-distributions-1","chapter":"Lecture 13 Model Selection","heading":"13.3.3 Continuous probability distributions","text":"definition complex continuous variables (\\(P(X = x; \\theta) = 0\\) infinitely many values…). commonly done use density function \\(f(x; \\theta)\\) considering probability obtaining value \\(x \\[x_j, x_j + h]\\), \\(x_j\\) observed data point, \\(h\\) small. :\\[\nL(\\theta \\vert x_j) = \\lim_{h \\0^+} \\frac{1}{h} \\int_{x_j}^{x_j + h} f(x ; \\theta) dx = f(x_j ; \\theta)\n\\]\nNote , contrary probabilities, density values can take values greater 1. , dispersion small, one end values likelihood greater 1 (positive log-likelihoods). fact, likelihood function proportional necessarily equal probability generating data given parameters: \\(L(\\theta\\vert X) \\propto P(X; \\theta)\\).many cases, maximizing likelihood equivalent minimizing sum square errors (residuals).","code":""},{"path":"model-selection.html","id":"likelihoods-for-linear-regression","chapter":"Lecture 13 Model Selection","heading":"13.4 Likelihoods for linear regression","text":"remember, considered normal equations:\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\n\\]\nresiduals variance \\(\\sigma^2\\). likelihood parameters simply product likelihood point:\\[\nL(\\beta_0, \\beta_1, \\sigma^2 \\vert Y) = \\prod_i L(\\beta_0, \\beta_1, \\sigma^2 \\vert Y_i) = \\prod_i f(Y_i; \\beta_0, \\beta_1, \\sigma^2) = \n\\prod_i \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(Y_i - (\\beta_0 + \\beta_1 X_i))^2}{2 \\sigma^2}\\right)\n\\]\nwant choose parameters maximize likelihood. logarithm monotonic maximizing likelihood equivalent maximizing log-likelihood:\\[\n\\mathcal L(\\beta_0, \\beta_1, \\sigma^2 \\vert Y) = -\\log\\left(\\sqrt{2 \\pi \\sigma^2}\\right) -\\frac{1}{{2 \\sigma^2}} \\sum_i {(Y_i - (\\beta_0 + \\beta_1 X_i))^2}\n\\]\nShowing minimizing sum squares, maximizing likelihood.","code":""},{"path":"model-selection.html","id":"likelihood-ratio-tests","chapter":"Lecture 13 Model Selection","heading":"13.5 Likelihood-ratio tests","text":"approaches contrast two models taking ratio maximum likelihoods sample data based models (.e., evaluate likelihood setting parameters m.l.e.). two models usually termed null model (.e., “simpler” model), alternative model. ratio \\(L_a / L_n\\) tells us many times likely data alternative model vs. null model. want determine whether ratio large enough reject null model favor alternative.Likelihood-ratio especially easy perform nested models.","code":""},{"path":"model-selection.html","id":"two-nested-models","chapter":"Lecture 13 Model Selection","heading":"13.5.0.1 Two nested models","text":"Nested means model \\(\\mathcal M_1\\) parameters \\(\\theta_1\\), model \\(\\mathcal M_2\\) parameters \\(\\theta_2\\), \\(\\theta_1 \\\\theta_2\\) — setting parameters \\(\\mathcal M_2\\) particular values, recover \\(\\mathcal M_1\\).example, suppose want model height trees. measure response variable (height tree \\(\\), \\(h_i\\)) well girth (\\(g_i\\)). actually data set ships R contains exactly type data:Height cherry trees measured feet; Girth diameter inches, Volume measuring amount timber cubic feet. Let’s add Radius measured feet:Let’s look distribution three heights:possible simple model one says tree heights heights taken Gaussian distribution given mean. context linear regression, can write model \\(\\mathcal M_0\\):\\[\nh_i = \\theta_0 + \\epsilon_i\n\\]\nassume errors \\(\\epsilon_i \\overset{\\text{iid}}{\\sim} \\mathcal N(0, \\sigma^2)\\). Now fit model, obtaining \\(\\hat{\\theta_0}\\), compute maximum log-likelihood \\(\\mathcal L_0(\\hat{\\theta_0}, \\hat{\\sigma}^2 \\vert h)\\).R, call:Now let’s plot height trees vs. radius:compute correlation:Given positive correlation radius height, can build complex model height also depends radius (\\(\\mathcal M_1\\)):\\[\nh_i = \\theta_0 + \\theta_1 r_i + \\epsilon_i\n\\]\nmodel \\(\\mathcal M_0\\), fit parameters (note \\(\\hat{\\theta_0}\\) model \\(\\mathcal M_0\\) general different \\(\\hat{\\theta_0}\\) model \\(\\mathcal M_1\\)), compute \\(\\mathcal L_1(\\hat{\\theta_0},\\hat{\\theta_1},\\hat{\\sigma}^2 \\vert h)\\). two models nested, setting \\(\\theta_1 = 0\\) recover \\(\\mathcal M_0\\).R:model use? can see adding extra parameter improved likelihood somewhat.Enter likelihood-ratio test. want know whether ’s worth using complex model, need calculate likelihood-ratio statistics. ’re helped Wilks’ theorem: sample size \\(n \\\\infty\\), test statistics \\(2 \\log(L_1 / L_0)\\) asymptotically \\(\\chi^2\\) distributed degrees freedom equal difference number parameters \\(\\mathcal M_1\\) \\(\\mathcal M_0\\).many caveats [^1] method commonly used practice.case, likelihood-ratio test favor use complex model.Pros: Straightforward; well-studied nested models.Cons: Difficult generalize complex cases.","code":"\ndata(trees)\nhead(trees)##   Girth Height Volume\n## 1   8.3     70   10.3\n## 2   8.6     65   10.3\n## 3   8.8     63   10.2\n## 4  10.5     72   16.4\n## 5  10.7     81   18.8\n## 6  10.8     83   19.7\ntrees <- trees %>% mutate (Radius = Girth / (2 * 12)) # diameter to radius; inches to feet\ntrees %>% ggplot(aes(x = Height)) + geom_density()\nM0 <- lm(data = trees, Height ~ 1) # only intercept\n# the m.l.e. of theta_0\ntheta0_M0 <- M0$coefficients[1]\ntheta0_M0## (Intercept) \n##          76\n# log likelihood\nlogLik(M0)## 'log Lik.' -100.8873 (df=2)\ntrees %>% ggplot(aes(x = Radius, y = Height)) + \n  geom_point()\ncor(trees$Radius, trees$Height)## [1] 0.5192801\nM1 <- lm(data = trees, Height ~ Radius) # intercept and slope\ntheta0_M1 <- M1$coefficients[1]\ntheta1_M1 <- M1$coefficients[2]\n# note that now theta_0 takes a different value:\nprint(c(theta0_M1, theta0_M1))## (Intercept) (Intercept) \n##    62.03131    62.03131\n# the log likelihood should improve\nlogLik(M1)## 'log Lik.' -96.01663 (df=3)\n# 2 * log-likelihood ratio\nlrt <- as.numeric(2 * (logLik(M1) - logLik(M0)))\nprint(\"2 log(L1 / L0)\")## [1] \"2 log(L1 / L0)\"\nprint(lrt)## [1] 9.74125\n# difference in parameters\ndf0 <- length(M0$coefficients)\ndf1 <- length(M1$coefficients)\nk <- df1 - df0\nprint(\"Number of extra parameters\")## [1] \"Number of extra parameters\"\nprint(k)## [1] 1\n# calculate (approximate) p-value\nres <- pchisq(lrt, k, lower.tail = FALSE)\nprint(paste(\"p-value using Chi^2 with\", k, \"degrees of freedom\"))## [1] \"p-value using Chi^2 with 1 degrees of freedom\"\nprint(round(res, 4))## [1] 0.0018"},{"path":"model-selection.html","id":"adding-more-models","chapter":"Lecture 13 Model Selection","heading":"13.5.0.2 Adding more models","text":"data also contains column volume. Let’s take look:look correlationWe can build another model:Compute log likelihood:test whether ’s better (nested) model 0:Also case, likelihood-ratio test favor use complex model. can contrast two complex models \\(\\mathcal M_1\\) \\(\\mathcal M_2\\)? nested!fact, can even concoct another model uses mix radius volume. assume trees cylinders, \\(V = \\pi r^2 h\\), \\(h = V / (\\pi r^2)\\). can test whether good approximation creating new variable:Pretty good! Let’s add list models:","code":"\ntrees %>% ggplot() + aes(x = Volume, y = Height) + geom_point()\ncor(trees$Volume, trees$Height)## [1] 0.5982497\nM2 <- lm(data = trees, Height ~ Volume) # intercept and slope\nlogLik(M2)## 'log Lik.' -94.02052 (df=3)\n# 2 * log-likelihood ratio\nlrt <- as.numeric(2 * (logLik(M2) - logLik(M0)))\nprint(\"2 log(L2 / L0)\")## [1] \"2 log(L2 / L0)\"\nprint(lrt)## [1] 13.73348\n# difference in parameters\ndf0 <- length(M0$coefficients)\ndf1 <- length(M2$coefficients)\nk <- df1 - df0\nprint(\"Number of extra parameters\")## [1] \"Number of extra parameters\"\nprint(k)## [1] 1\n# calculate (approximate) p-value\nres <- pchisq(lrt, k, lower.tail = FALSE)\nprint(paste(\"p-value using Chi^2 with\", k, \"degrees of freedom\"))## [1] \"p-value using Chi^2 with 1 degrees of freedom\"\nprint(round(res, 4))## [1] 2e-04\ntrees <- trees %>% mutate(Guess = Radius^2)\ntrees %>% ggplot() + aes(x = Guess, y = Height) + geom_point()\ncor(trees$Guess, trees$Height)## [1] 0.5084267\nM3 <- lm(Height ~ Guess, data = trees)\nlogLik(M3)## 'log Lik.' -96.25156 (df=3)"},{"path":"model-selection.html","id":"aic","chapter":"Lecture 13 Model Selection","heading":"13.6 AIC","text":"course, cases models want contrast need nested. , can try penalize models according number free parameters, complex models (many free parameters) associated much better likelihoods favored.early 1970s, Hirotugu Akaike proposed “information criterion” (AIC, now known Akaike’s Information Criterion), based, name implies, information theory. Basically, AIC measuring (asymptotically) information loss using model lieu actual data. Philosophically, rooted idea “true model” generated data, several possible models can serve approximation. Practically, easy compute:\\[AIC = -2 \\mathcal L(\\theta \\vert D) + 2 k\\]\\(k\\) number free parameters (e.g., 3 simplest linear regression [intercept, slope, variance residuals]). R, many models provide way access AIC score:can see AIC favors cylinder model others. Typically, difference 2 considered “significant,” though course really depends size data, values AIC, etc.Pros: Easy calculate; popular.Cons: Sometimes difficult “count” parameters; parameter cost , different effects likelihood?","code":"\nAIC(M0) # only intercept## [1] 205.7745\nAIC(M1) # use radius## [1] 198.0333\nAIC(M2) # use volume## [1] 194.041\nAIC(M3) # use cylinder## [1] 198.5031"},{"path":"model-selection.html","id":"other-information-based-criteria","chapter":"Lecture 13 Model Selection","heading":"13.7 Other information-based criteria","text":"approach spearheaded Akaike followed number researchers, giving rise many similar criteria model selection. Without getting much details, pointers:Bayesian Information Criterion \\(BIC = -2 \\mathcal L(\\theta \\vert D) + k \\log(n)\\) \\(n\\) number data points. Penalizes parameters strongly much data.Hannan–Quinn information criterion \\(HQC = -2 \\mathcal L(\\theta \\vert D) + k \\log(\\log(n))\\)","code":""},{"path":"model-selection.html","id":"bayesian-approaches-to-model-selection","chapter":"Lecture 13 Model Selection","heading":"13.8 Bayesian approaches to model selection","text":"approaches ’ve examined based “point-estimates,” .e., consider parameters maximum likelihood estimate. Bayesian approaches, hand, consider distributions parameters. , parameters give high likelihoods restricted range values deemed “expensive” (“important” need “fine-tuned”) yielding likelihood wide range values.","code":""},{"path":"model-selection.html","id":"marginal-likelihoods","chapter":"Lecture 13 Model Selection","heading":"13.8.1 Marginal likelihoods","text":"beautiful approach based marginal likelihoods, .e., likelihoods obtained integrating parameters . Unfortunately, calculation becomes difficult perform hand complex models, provides good approach simple models. general, want assess “goodness” model. , using Bayes’ rule:\\[\n  P(M\\vert D) = \\frac{P(D\\vert M) P(M)}{P(D)}\n\\]\\(P(M\\vert D)\\) probability model given data; \\(P(D)\\) “probability data” (don’t worry, need calculated), \\(P(M)\\) prior (probability choose model seeing data). \\(P(D\\vert M)\\) marginal likelihood: compute directly, model requires parameters \\(\\theta\\), however, can write\\[\nP(D\\vert M) = \\int P(D\\vert M,\\theta)P(\\theta\\vert M) d\\theta\n\\]\\(P(D\\vert M,\\theta)\\) likelihood, \\(P(\\theta\\vert M)\\) distribution parameter values (typically, priors).example, let’s compute marginal likelihood case flip coin \\(n = + b\\) times, obtain \\(\\) heads \\(b\\) tails. Call \\(\\theta\\) probability obtaining head, suppose \\(P(\\theta\\vert M)\\) uniform distribution. :\\[\nP(,b\\vert M) = \\int_0^1 P(,b\\vert M,\\theta) d\\theta = \\int_0^1 \\binom{+b}{} \\theta^{} (1-\\theta)^{b} d\\theta  = \\frac{1}{+b+1} = \\frac{1}{n+1}\n\\]Interestingly, marginal likelihood can interpreted expected likelihood parameters sampled prior.","code":""},{"path":"model-selection.html","id":"bayes-factors","chapter":"Lecture 13 Model Selection","heading":"13.8.2 Bayes factors","text":"Take two models, assume initially preference \\(P(M_1) = P(M_2)\\), :\\[\n  \\frac{P(M_1\\vert D)}{P(M_2\\vert D)} = \\frac{P(D\\vert M_1)P(M_1)}{P(D\\vert M_2)P(M_2)} = \\frac{P(D\\vert M_1)}{P(D\\vert M_2)}\n\\]ratio called “Bayes factor” provides rigorous way perform model selection.","code":""},{"path":"model-selection.html","id":"bayes-factors-in-practice","chapter":"Lecture 13 Model Selection","heading":"13.8.3 Bayes factors in practice","text":"practice, Bayes Factors can estimated MCMC. ’re going get , can use package ) automatically sets priors variables (close philosophy known “Objective Bayes”); b) performs calculation Bayes Factors us.Let’s build many models. Load data:build models:Perform selection among models nested lm_all:ratios measure many times probable model compared intercept (assuming initially models equiprobable). Note Bayes Factors automatically penalize overly complex models (triplets/quadruplets ranked pairs even Guess).Pros: Elegant, straightforward interpretation.Cons: Difficult compute complex models; requires priors.","code":"\ndata(trees)\nhead(trees)##   Girth Height Volume\n## 1   8.3     70   10.3\n## 2   8.6     65   10.3\n## 3   8.8     63   10.2\n## 4  10.5     72   16.4\n## 5  10.7     81   18.8\n## 6  10.8     83   19.7\ntrees$Radius <- trees$Girth / (2 * 12)\ntrees$Guess <- trees$Volume / trees$Radius^2\nlm_all <- lm(Height ~ ., data = trees) # . means use all cols besides Height\nsummary(lm_all)## \n## Call:\n## lm(formula = Height ~ ., data = trees)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -6.7669 -2.4752 -0.2354  1.9335 10.5319 \n## \n## Coefficients: (1 not defined because of singularities)\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  22.6671    16.2947   1.391 0.175562    \n## Girth         1.5127     1.2278   1.232 0.228543    \n## Volume       -0.2045     0.2572  -0.795 0.433505    \n## Radius            NA         NA      NA       NA    \n## Guess         0.4291     0.1034   4.152 0.000296 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.023 on 27 degrees of freedom\n## Multiple R-squared:  0.6413, Adjusted R-squared:  0.6014 \n## F-statistic: 16.09 on 3 and 27 DF,  p-value: 3.391e-06\nlogLik(lm_all)## 'log Lik.' -84.99667 (df=5)\nbf_analysis <- regressionBF(Height ~ ., data = trees)\nplot(bf_analysis)"},{"path":"model-selection.html","id":"using-tidymodels-for-modeling-and-cross-validation","chapter":"Lecture 13 Model Selection","heading":"13.9 Using tidymodels for modeling and cross-validation","text":"excellent suite packages called tidymodels offers beautiful streamlined tools building models, training , evaluating results. use Palmer penguins data application, aim building predictive model bill length penguins. Let us first examine data graphically see relationship body mass bill length:certainly appears including sex species result better fit. Let us try compare models build using syntax tidymodels.First, need create model use pipeline. created like :Next, clean data split training testing sets, create recipe specifies data set response variable want model. variables left predictors, can take consideration changing role variables “ID.” recipe, predictor (explanatory) variable data set body_mass_g.can now combine recipe data model create workflow training data model, use create fit:Finally, can extract sorts information, best-fit parameters, errors, p-values, likelihoods generated fit:tidy glance functions return different summaries information; first one information fitted parameters, second R-squared likelihood model.Let us now modify recipe include species save fitting results different object fit2:R-squared well log likelihood improved substantially AIC lower.Now let us see can improve model quality incorporating sex another explanatory variable:Adding sex improves R-squared log-likelihood, AIC drops .","code":"\nlibrary(palmerpenguins)\ndata(\"penguins\")\npenguins %>% ggplot(aes( x= body_mass_g, y= bill_length_mm)) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE) +\n  scale_color_viridis_d(option = \"plasma\", end = .7)## `geom_smooth()` using formula 'y ~ x'## Warning: Removed 2 rows containing non-finite values (stat_smooth).## Warning: Removed 2 rows containing missing values (geom_point).\npenguins %>% filter (!is.na(sex)) %>% ggplot(aes( x= body_mass_g, y= bill_length_mm, color = sex)) +\n  geom_point() +\n  geom_smooth(method = lm, se = FALSE) +\n  facet_wrap(~species, scales = 'free') +\n  scale_color_viridis_d(option = \"plasma\", end = .7)## `geom_smooth()` using formula 'y ~ x'\nlm_mod <- \n  linear_reg() %>% \n  set_engine(\"lm\")\ndata(\"penguins\")\npen_clean <- penguins %>% filter(!is.na(bill_length_mm), !is.na(sex), !is.na(species))\n# Fix the random numbers by setting the seed  for reproducibility\nset.seed(314)\n# Put 3/4 of the data into the training set \ndata_split <- initial_split(pen_clean, prop = 3/4)\n\n# Create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n\npen_recipe <- \n  recipe(bill_length_mm ~ ., data = train_data) %>% \n  #update_role(sex, island, year, species, bill_depth_mm, flipper_length_mm, new_role = \"ID\")\n  update_role(sex, island, species, bill_depth_mm, flipper_length_mm, new_role = \"ID\") \n# create workflow\npen_wflow <- \n  workflow() %>% \n  add_model(lm_mod) %>% \n  add_recipe(pen_recipe)\n# fit the model to the data\npen_fit <- \n  pen_wflow %>% \n  fit(data = train_data)\nfit1 <- pen_fit %>% \n  extract_fit_parsnip() \ntidy(fit1)## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept) 26.9      1.52          17.8 4.71e-46\n## 2 body_mass_g  0.00406  0.000351      11.6 5.41e-25\nglance(fit1) # ## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.351         0.348  4.42      134. 5.41e-25     1  -723. 1451. 1462.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\npen_recipe2 <- \n  recipe(bill_length_mm ~ ., data = train_data) %>% \n  #update_role(sex, island, year, bill_depth_mm, flipper_length_mm, new_role = \"ID\")\n  update_role(sex, island, bill_depth_mm, flipper_length_mm, new_role = \"ID\") \n\n# create workflow\npen_wflow2 <- \n  workflow() %>% \n  add_model(lm_mod) %>% \n  add_recipe(pen_recipe2)\n# fit the model to the data\npen_fit2 <- \n  pen_wflow2 %>% \n  fit(data = train_data)\n# summarise the fit\nfit2 <- pen_fit2 %>% \n  extract_fit_parsnip() \ntidy(fit2)## # A tibble: 4 × 5\n##   term             estimate std.error statistic  p.value\n##   <chr>               <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)      25.1      1.26         20.0  2.04e-53\n## 2 speciesChinstrap 10.0      0.412        24.3  3.64e-67\n## 3 speciesGentoo     3.51     0.572         6.13 3.44e- 9\n## 4 body_mass_g       0.00371  0.000331     11.2  8.05e-24\nglance(fit2) ## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.810         0.807  2.41      347. 6.33e-88     3  -570. 1150. 1167.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\npen_recipe3 <- \n  recipe(bill_length_mm ~ ., data = train_data) %>% \n  #update_role(island, year, bill_depth_mm, flipper_length_mm, new_role = \"ID\")\n  update_role(island,  bill_depth_mm, flipper_length_mm, new_role = \"ID\") \n\n# create workflow\npen_wflow3 <- \n  workflow() %>% \n  add_model(lm_mod) %>% \n  add_recipe(pen_recipe3)\n# fit the model to the data\npen_fit3 <- \n  pen_wflow3 %>% \n  fit(data = train_data)\n# summarise the fit\nfit3 <- pen_fit3 %>% \n  extract_fit_parsnip() \ntidy(fit3)## # A tibble: 5 × 5\n##   term             estimate std.error statistic  p.value\n##   <chr>               <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)      31.0      1.48         21.0  1.80e-56\n## 2 speciesChinstrap 10.0      0.382        26.3  4.35e-73\n## 3 speciesGentoo     6.28     0.683         9.19 1.76e-17\n## 4 body_mass_g       0.00176  0.000432      4.08 6.15e- 5\n## 5 sexmale           2.57     0.400         6.43 6.68e-10\nglance(fit3) ## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.837         0.835  2.23      314. 6.76e-95     4  -550. 1113. 1134.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>"},{"path":"model-selection.html","id":"prediction-and-cross-validation","chapter":"Lecture 13 Model Selection","heading":"13.9.1 Prediction and cross-validation","text":"Now let us three trained models predict values bill length test data set aside:function metrics package yardstick returns several related measures agreement prediction data test set. Probably common root mean squared error, root sum squared differences predictions observations. Notice rmse drops successive variable add model.Exercise Add one several variables list predictors modifying recipe, calculate predictions, compare performance (e.g. rmse test data) simpler models.","code":"\nbill_fit1 <- predict(fit1, test_data)\nbill_fit2 <- predict(fit2, test_data)\nbill_fit3 <- predict(fit3, test_data)\n\nprediction1 <- augment(fit1, test_data)\nglimpse(prediction1)## Rows: 84\n## Columns: 8\n## $ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n## $ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Biscoe, …\n## $ bill_length_mm    <dbl> 40.3, 36.7, 38.9, 41.1, 35.3, 40.5, 37.9, 39.5, 37.2…\n## $ bill_depth_mm     <dbl> 18.0, 19.3, 17.8, 17.6, 18.9, 17.9, 18.6, 16.7, 18.1…\n## $ flipper_length_mm <int> 195, 193, 181, 182, 187, 187, 172, 178, 178, 196, 18…\n## $ body_mass_g       <int> 3250, 3450, 3625, 3200, 3800, 3200, 3150, 3250, 3900…\n## $ sex               <fct> female, female, female, female, female, female, fema…\n## $ .pred             <dbl> 40.13685, 40.94866, 41.65899, 39.93390, 42.36932, 39…\nggplot(prediction1, aes(x=.pred, y=bill_length_mm)) + geom_point() + geom_smooth() + geom_abline(slope = 1, intercept = 0)## `geom_smooth()` using method = 'loess' and formula 'y ~ x'\nmetrics(prediction1, truth = bill_length_mm, estimate = .pred)## # A tibble: 3 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard       4.43 \n## 2 rsq     standard       0.329\n## 3 mae     standard       3.57\nbill_fit2 <- predict(fit2, test_data)\n\nprediction2<- augment(fit2, test_data)\nglimpse(prediction2)## Rows: 84\n## Columns: 8\n## $ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n## $ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Biscoe, …\n## $ bill_length_mm    <dbl> 40.3, 36.7, 38.9, 41.1, 35.3, 40.5, 37.9, 39.5, 37.2…\n## $ bill_depth_mm     <dbl> 18.0, 19.3, 17.8, 17.6, 18.9, 17.9, 18.6, 16.7, 18.1…\n## $ flipper_length_mm <int> 195, 193, 181, 182, 187, 187, 172, 178, 178, 196, 18…\n## $ body_mass_g       <int> 3250, 3450, 3625, 3200, 3800, 3200, 3150, 3250, 3900…\n## $ sex               <fct> female, female, female, female, female, female, fema…\n## $ .pred             <dbl> 37.20539, 37.94811, 38.59798, 37.01972, 39.24785, 37…\nggplot(prediction2, aes(x=.pred, y=bill_length_mm)) + geom_point() + geom_smooth() + geom_abline(slope = 1, intercept = 0)## `geom_smooth()` using method = 'loess' and formula 'y ~ x'\nmetrics(prediction2, truth = bill_length_mm, estimate = .pred)## # A tibble: 3 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard       2.46 \n## 2 rsq     standard       0.793\n## 3 mae     standard       2.08\nbill_fit3 <- predict(fit3, test_data)\n\nprediction3 <- augment(fit3, test_data)\nglimpse(prediction3)## Rows: 84\n## Columns: 8\n## $ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n## $ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Biscoe, …\n## $ bill_length_mm    <dbl> 40.3, 36.7, 38.9, 41.1, 35.3, 40.5, 37.9, 39.5, 37.2…\n## $ bill_depth_mm     <dbl> 18.0, 19.3, 17.8, 17.6, 18.9, 17.9, 18.6, 16.7, 18.1…\n## $ flipper_length_mm <int> 195, 193, 181, 182, 187, 187, 172, 178, 178, 196, 18…\n## $ body_mass_g       <int> 3250, 3450, 3625, 3200, 3800, 3200, 3150, 3250, 3900…\n## $ sex               <fct> female, female, female, female, female, female, fema…\n## $ .pred             <dbl> 36.72639, 37.07855, 37.38670, 36.63834, 37.69484, 36…\nggplot(prediction3, aes(x=.pred, y=bill_length_mm)) + geom_point() + geom_smooth() + geom_abline(slope = 1, intercept = 0)## `geom_smooth()` using method = 'loess' and formula 'y ~ x'\nmetrics(prediction3, truth = bill_length_mm, estimate = .pred)## # A tibble: 3 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard       2.35 \n## 2 rsq     standard       0.811\n## 3 mae     standard       1.93"},{"path":"model-selection.html","id":"other-approaches","chapter":"Lecture 13 Model Selection","heading":"13.10 Other approaches","text":"","code":""},{"path":"model-selection.html","id":"minimum-description-length","chapter":"Lecture 13 Model Selection","heading":"13.10.1 Minimum description length","text":"Another completely different way perform model selection based idea “Minimum Description Length,” models seen way “compress” data, model leading strongest compression favored. cover , can read [4].","code":""},{"path":"model-selection.html","id":"cross-validation","chapter":"Lecture 13 Model Selection","heading":"13.10.2 Cross validation","text":"One robust method perform model selection, often used machine learning, cross-validation. idea simple: split data three parts: small data set exploring; large set fitting; small set testing (example, 5%, 75%, 20%). can use first data set explore freely get inspired good model. data discarded. use largest data set accurately fitting model(s). Finally, validate model select competing models using last data set.haven’t used test data fitting, dramatically reduce risk -fitting. downside ’re wasting precious data. less expensive methods cross validation, much data, data cheap, virtue fairly robust.","code":""},{"path":"model-selection.html","id":"exercise-do-shorter-titles-lead-to-more-citations","chapter":"Lecture 13 Model Selection","heading":"13.10.2.1 Exercise: Do shorter titles lead to more citations?","text":"test power cross-validation, going examine bold claim Letchford et al., 2015: papers shorter titles attract citations longer titles. going use original data:Letchford , Moat HS, Preis T (2015) advantage short paper titles. Royal Society Open Science 2(8): 150266.data set reports information top 20000 articles year 2007 2013. Author’s claim shorter titles lead citations:can see, title length anti-correlated (using rank correlation) number citations.several problems claim:authors selected papers based citations. claim need stated “among top-cited papers correlation.”journals cover wide array disciplines. title length reflect different publishing cultures.importantly, different journals different requirements title lengths. example, Nature requires titles less 90 characters:, effect Authors reporting due fact high-profile journals mandate short titles? Let’s see whether claims hold water considering specific journals:seems several medical journals (NEJM, Circulation, J Clin Oncology) longer titles fare better shorter ones. Nature PNAS see negative correlation, Science gives clear trend.Let’s look mean standard deviation citations journal/year","code":"\n# original URL\n# https://datadryad.org/stash/dataset/doi:10.5061/dryad.hg3j0\ndt <- read_csv(\"data/LMP2015.csv\")## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   year = col_double(),\n##   journal = col_character(),\n##   title_length = col_double(),\n##   cites = col_double()\n## )\ndt %>% \n  group_by(year) %>% \n  summarise(correlation = cor(title_length, cites, method = \"kendall\"))## # A tibble: 7 × 2\n##    year correlation\n##   <dbl>       <dbl>\n## 1  2007     -0.0535\n## 2  2008     -0.0687\n## 3  2009     -0.0560\n## 4  2010     -0.0655\n## 5  2011     -0.0525\n## 6  2012     -0.0528\n## 7  2013     -0.0451\ndt%>% filter(journal %in% c(\"Nature\", \"Science\")) %>% \n  ggplot() + aes(x = journal, y = title_length) + geom_violin()\n# only consider journals with more than 1000 papers in the data set\ndt <- dt %>% \n  group_by(journal) %>% \n  mutate(num_papers = n())%>% \n  filter(num_papers > 1000) %>% \n  ungroup()\n# now compute correlation and plot\ndt %>% \n  group_by(year, journal) %>% \n  summarise(correlation = cor(title_length, cites, method = \"kendall\")) %>% \n  ggplot() + \n  aes(x = reorder(substr(journal, 1, 30), (correlation)), y = correlation) + \n  geom_boxplot() + \n  geom_hline(yintercept = 0, colour = \"red\", linetype = 2) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  # rotate labels x axis\n  xlab(\"\")## `summarise()` has grouped output by 'year'. You can override using the `.groups` argument.\ndt %>% \n  group_by(journal, year) %>% \n  summarize(mean = mean(log(cites + 1)), sd = sd(log(cites + 1))) %>% \n  ggplot() + \n  aes(x = year, y = mean) + \n  geom_point() + \n  facet_wrap(~journal)## `summarise()` has grouped output by 'journal'. You can override using the `.groups` argument.\ndt %>% \n  group_by(journal, year) %>% \n  summarize(mean = mean(log(cites + 1)), sd = sd(log(cites + 1))) %>% \n  ggplot() + \n  aes(x = year, y = sd) + \n  geom_point() + \n  facet_wrap(~journal)## `summarise()` has grouped output by 'journal'. You can override using the `.groups` argument."},{"path":"model-selection.html","id":"two-models","chapter":"Lecture 13 Model Selection","heading":"13.10.2.2 Two models","text":"Let’s consider two competing models.Model1: journal year mean\\(\\log(\\text{cits} + 1) \\sim \\text{journal}:\\text{year}\\)Model2: length titles influences citations\\(\\log(\\text{cits} + 1) \\sim \\text{journal}:\\text{year} + \\text{title-length}\\)going fit model using 90% data; going use remaining data cross-validation.Now fit models:Now let’s try predict --fit data haven’t used:gain anything including information titles.Pros: Easy use; quite general; asymptotically equivalent AIC.Cons: Sensitive data split (can average multiple partitions); need much data (instability parameter estimates due “data loss”)","code":"\nset.seed(4)\ndt <- dt %>% mutate(logcit = log(cites + 1))\n# sample 10% of the data\ndata_test <- dt %>% sample_frac(0.3)\ndata_fit  <- anti_join(dt, data_test) # get all those not in data_test## Joining, by = c(\"year\", \"journal\", \"title_length\", \"cites\", \"num_papers\", \"logcit\")\nM1 <- lm(logcit ~ factor(year)*journal, data = data_fit)\nM2 <- lm(logcit ~ factor(year)*journal + title_length, data = data_fit)\nM1_predictions <- predict(M1, newdata = data_test)\nSSQ_M1 <- sum((log(data_test$cites + 1) - M1_predictions)^2)\nM2_predictions <- predict(M2, newdata = data_test)\nSSQ_M2 <- sum((log(data_test$cites + 1) - M2_predictions)^2)\nprint(SSQ_M1)## [1] 2465.712\nprint(SSQ_M2)## [1] 2465.96"},{"path":"model-selection.html","id":"references-and-further-reading-1","chapter":"Lecture 13 Model Selection","heading":"13.11 References and further reading:","text":"Pinheiro, José C.; Bates, Douglas M. (2000), Mixed-Effects Models S S-PLUS, Springer-Verlag, pp. 82–93Pinheiro, José C.; Bates, Douglas M. (2000), Mixed-Effects Models S S-PLUS, Springer-Verlag, pp. 82–93Tidymodels tutorialTidymodels tutorialEmil Hvitfeldt, Tidymodels Introduction Statistical Learning REmil Hvitfeldt, Tidymodels Introduction Statistical Learning RMark H Hansen Bin Yu Model Selection Principle Minimum Description Length.Mark H Hansen Bin Yu Model Selection Principle Minimum Description Length.","code":""},{"path":"principal-component-analysis.html","id":"principal-component-analysis","chapter":"Lecture 14 Principal Component Analysis","heading":"Lecture 14 Principal Component Analysis","text":"GoalIntroduce Principal Component Analysis (PCA), one popular techniques perform “dimensionality reduction” complex data sets. see data points high-dimensional space, can project data onto new set coordinates first coordinate captures largest share variance data, second coordinates captures largest share remaining variance . way, can project large-dimensional data sets onto low-dimensional spaces lose least information data.","code":"\nlibrary(tidyverse)\nlibrary(ggmap) # for ggimage\nlibrary(ggfortify) # for autoplot"},{"path":"principal-component-analysis.html","id":"input","chapter":"Lecture 14 Principal Component Analysis","heading":"14.1 Input","text":"collected \\(n \\times m\\) data matrix \\(X\\) (typically, \\(n \\gg m\\)), rows samples columns \\(m\\) measures samples. row matrix defines point Euclidean space \\(\\mathbb R^m\\), .e., point space potential sample. Naturally, samples similar measurements “close” space, samples different “far.” However, \\(m\\) can quite large, therefore easily visualize position points. One way think PCA best projection points \\(r\\)-dimensional space (\\(r \\leq m\\)), visualization clustering.example, take iris data set:can separate clusters better finding best projection 2D:","code":"\ndata(\"iris\")\nir <- iris %>% select(-Species)\nsp <- iris %>% select(Species)\npairs(ir, col = sp$Species)\nautoplot(prcomp(ir, center = TRUE), \n         data = iris, \n         colour = \"Species\",\n         scale = FALSE) + \n  coord_equal()## Warning: `select_()` was deprecated in dplyr 0.7.0.\n## Please use `select()` instead.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_warnings()` to see where this warning was generated."},{"path":"principal-component-analysis.html","id":"singular-value-decomposition","chapter":"Lecture 14 Principal Component Analysis","heading":"14.2 Singular Value Decomposition","text":"hearth PCA particular matrix decomposition (factorization): represent matrix \\(X\\) product matrices (, equivalently, sum matrices). particular, SVD defined equation:\\[\nX = U \\Sigma V^T\n\\]\\(X\\) \\(n \\times m\\) matrix, \\(U\\) \\(n \\times n\\) orthogonal, unitary matrix \\(V\\) \\(m \\times m\\) orthogonal, unitary matrix, \\(\\Sigma\\) \\(m \\times n\\) rectangular, diagonal matrix non-negative values diagonal. \\(V\\) (real) unitary matrix, \\(VV^T = I_m\\) (\\(m \\times m\\) identity matrix), \\(U\\) also unitary, \\(UU^T = I_n\\). Another way put \\(U^{-1} = U^T\\).(Note: defines “full” SVD \\(\\); equivalently, one can perform “thin,” “reduced” SVD \\(U\\) dimension \\(n \\times p\\), \\(\\Sigma\\) \\(V\\) dimension \\(p \\times p\\), \\(p \\leq m\\) rank \\(\\)—default R returns “thin” SVD; read details ).values diagonal \\(\\Sigma\\) singular values \\(X\\), .e., nonzero eigenvalues \\(XX^T\\) (\\(X^T X\\)). context, matrix \\(U\\) contains left singular vectors \\(X\\) \\(V\\) right singular vectors. Let’s rearrange rows/cols \\(\\Sigma\\), \\(U\\) \\(V\\) singular values decreasing order: \\(\\text{diag}(\\Sigma) = (\\sigma_1, \\sigma_2, \\ldots, \\sigma_m)\\).SVD, matrix \\(X\\) can seen sum \\(m\\) matrices:\\[\nX = \\sum_{= 1}^m U_i \\Sigma_{ii} V_i^T = X_1 + X_2 + X_3 + \\ldots\n\\]\\(U_i\\) \\(\\)th column \\(U\\). importantly, can prove step (\\(r\\)), computing “best” approximation \\(X\\) sum \\(r\\) rank-1 matrices. .e., \\(r\\) \\(\\| X - (X_1 + X_2 + \\ldots + X_r) \\|\\) small possible (Eckart–Young–Mirsky theorem).Let’s look concrete example. monochromatic image can represented matrix entries pixels taking values (example, using 8 bits) \\(0, 1, \\ldots, 255\\):Now let’s perform SVD, show indeed factorized image:Now can visualize approximation ’re making take first singular values. ’re going plot \\(X_k\\) (left), \\(\\sum_{=1}^k X_i\\) (right). Even iterations (7, 255) obtain recognizable image:","code":"\nstefano <- as.matrix(read.csv(\"data/stefano.txt\"))\n# invert y axis and transpose for visualization\nstefano <- t(stefano[,ncol(stefano):1])\n# rescale values to suppress warning from ggimage\nstefano <- stefano / max(stefano)\nggimage(stefano)\ns_svd <- svd(stefano)\nU <- s_svd$u\nV <- s_svd$v\nSigma <- diag(s_svd$d)\n# this should be equal to the original matrix\nstefano_2 <- U %*% Sigma %*% t(V)\n# let's plot the difference\nggimage(round(stefano - stefano_2, 10))\nr <- 7\nXdec <- array(0, c(dim(stefano), r))\nXsum <- array(0, c(dim(stefano), r))\n# store the first matrix\nXdec[,,1] <- (U[,1] %*% t(V[,1])) * Sigma[1,1]\n# the first term in the sum is the matrix itself\nXsum[,,1] <- Xdec[,,1]\n# store the other rank one matrices, along with the partial sum\nfor (i in 2:r){\n  Xdec[,,i] <- (U[,i] %*% t(V[,i])) * Sigma[i,i]\n  Xsum[,,i] <- Xsum[,,i - 1] + Xdec[,,i]\n}\n# now plot all matrices and their sum\nplots <- list()\nfor (i in 1:r){\n  plots[[length(plots) + 1]] <- ggimage(Xdec[,,i])\n  plots[[length(plots) + 1]] <- ggimage(Xsum[,,i])\n}\ngridExtra::grid.arrange(grobs = plots, ncol = 2)"},{"path":"principal-component-analysis.html","id":"svd-and-pca","chapter":"Lecture 14 Principal Component Analysis","heading":"14.3 SVD and PCA","text":"Let’s go back data matrix \\(X\\), representation \\(n\\) points (samples) \\(m\\) dimensions (measurements). moment, consider case column \\(X\\) sums zero (.e., measurement, removed mean—called “centering”). like represent data best possible dimensions, ) axes orthogonal; b) axes aligned principal sources variation data. precisely, PCA orthogonal linear transformation transforms data new coordinate system direction greatest variance data aligned first coordinate, second greatest second coordinate, .example, let’s take Petal.Lenght Petal.Width iris:can see now points centered (0,0).practice, want produce new “data matrix” \\(Y\\):\\[\nY = XW\n\\]\\(W\\) appropriate change basis, transforming data directions main variation exposed. choose \\(m \\times m\\) matrix, want ) \\(W\\) orthogonal (.e., “rotation” data), b) columns \\(W\\) unit vectors (stretching data).new columns (.e., transformed “measurements”) \\(Y_i\\) can written :\\[\nY_{} = X W_i\n\\]\\(Y_i\\) ith column \\(Y\\) \\(W_i\\) ith column \\(W\\). Let’s start first column \\(Y_1\\): want choose \\(W_1\\) variance \\(Y_i\\) maximized. mean column \\(X\\) zero, also mean \\(Y_i\\) zero. Thus, variance simply \\(\\frac{1}{n-1}\\sum_{j =1}^{n} Y_{ij}^2 =\\frac{1}{n-1} \\|Y_i\\|\\). can write matrix form:\\[\\frac{1}{n-1}\\|Y_i\\| = \\frac{1}{n-1}\\|XW_i \\| = \\frac{1}{n-1} W_i^TX^T X W_i\\]Note \\(S = \\frac{1}{n-1} X^T X\\) \\(m \\times m\\) sample covariance matrix \\(X\\). \\(\\|W_i\\| = 1\\), can rewrite :\\[\n\\frac{1}{n}\\|Y_i\\| = \\frac{W_i^T S W_i}{W_i^T W_i}\n\\]maximized (\\(W_i\\)) \\(W_i\\) eigenvector \\(S\\) associated largest eigenvalue (see Rayleigh quotient), case:\\[\n\\frac{1}{n-1}\\|Y_i\\| = \\frac{W_i^T S W_i}{W_i^T W_i} = \\lambda_1\n\\]Therefore, first column \\(Y\\) given projection data first eigenvector \\(S\\). variance captured first axis given largest eigenvalue \\(S\\). find columns \\(Y\\), can subtract \\(X\\) matrix \\(Y_1 W_1^T\\) repeat.Note first axis captures \\(\\lambda_1 / \\sum_{= 1}^m \\lambda_i\\) total variance \\(X\\). typically reported PCA “loadings” various components.Therefore, PCA amounts simply taking eigenvectors \\(S\\) ordered corresponding eigenvalues. can use SVD accomplish task efficiently:\\[\nX = U \\Sigma V^T\n\\]\\[\n\\begin{aligned}\n(n-1) S = X^T X &= (V \\Sigma^T U^T) (U \\Sigma V^T)\\\\\n&= V \\Sigma^T \\Sigma V^T\\\\\n&= V \\widetilde{\\Sigma}^2 V^T\n\\end{aligned}\n\\]\\(\\widetilde{\\Sigma}^2 = \\Sigma^T \\Sigma\\) (, equivalently square square version \\(\\Sigma\\)). contrasting \\(S = W \\Lambda W^T\\) \\(S = V (\\widetilde{\\Sigma}^2 / (m-1))V^T\\) see \\(V = W\\). Finally, :\\[\nY = X W = U \\Sigma V^T V = U\\Sigma\n\\]Therefore, can perform PCA efficiently decomposing \\(X\\) using SVD.","code":"\nX <- iris %>% select(Petal.Length, Petal.Width) %>% as.matrix()\nX <- scale(X, center = TRUE, scale = FALSE) # remove mean\ncolors <- iris$Species\nplot(X, col = colors)\n# build sample covariance matrix\nS <- (1 / (nrow(X) - 1)) * t(X) %*% X\n# compute eigenvalues and eigenvectors\neS <- eigen(S, symmetric = TRUE)\n# W is the matrix of eigenvectors\nW <- eS$vectors\n# check \nY <- X %*% W\nplot(Y, col = colors)\neS$values## [1] 3.66123805 0.03604607\napply(Y, 2, var)## [1] 3.66123805 0.03604607"},{"path":"principal-component-analysis.html","id":"pca-in-rfrom-scratch","chapter":"Lecture 14 Principal Component Analysis","heading":"14.3.1 PCA in R—from scratch","text":"Pretty good! Let’s see poorly classified points:can also scale variables turning sample covariance matrix \\(S\\) correlation matrix (useful variance different measurements varies substantially).","code":"\ndt <- read_csv(\"data/handwritten_digits.csv\") %>% \n  arrange(id, x, y)## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   id = col_double(),\n##   label = col_double(),\n##   pixel = col_double(),\n##   value = col_double(),\n##   x = col_double(),\n##   y = col_double()\n## )\nhead(dt)## # A tibble: 6 × 6\n##      id label pixel value     x     y\n##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1     1     0     0     0     1     1\n## 2     1     0    16     0     1     2\n## 3     1     0    32     0     1     3\n## 4     1     0    48     0     1     4\n## 5     1     0    64     0     1     5\n## 6     1     0    80     0     1     6\n# make into a data matrix with pixels as cols\ndt_wide <- pivot_wider(dt %>% select(-x, -y), \n                       names_from = pixel, \n                       values_from = value)\nX <- (as.matrix(dt_wide %>% select(-id, -label)))\n# make col means = 0\nXs <- scale(X, center = TRUE, scale = FALSE)\n# compute SVD\nX_svd <- svd(Xs)\n# Y = US is the transformed data\nY <- X_svd$u %*% diag(X_svd$d)\nPCA_1 <- dt_wide %>% \n  select(id, label) %>% \n  mutate(label = as.character(label)) %>% \n  add_column(PC1 = Y[,1], PC2 = Y[,2])\nggplot(PCA_1) + \n  aes(x = PC1, y = PC2, label = id, group = label, colour = label) + \n  geom_text()\n# This should be a 0\nggimage(matrix(X[122,], 16, 16, byrow = FALSE), fullpage = FALSE)\n# This should be a 1\nggimage(matrix(X[141,], 16, 16, byrow = FALSE), fullpage = FALSE)\n# This should be a 5\nggimage(matrix(X[322,], 16, 16, byrow = FALSE), fullpage = FALSE)"},{"path":"principal-component-analysis.html","id":"pca-in-r-the-easy-way","chapter":"Lecture 14 Principal Component Analysis","heading":"14.3.2 PCA in R — the easy way","text":"","code":"\nlibrary(ggfortify)\n# for prcomp, you need only numeric data\nX <- dt_wide %>% select(-id, -label)\nPCA_3 <- prcomp(X)\nautoplot(PCA_3, \n         data = dt_wide %>% mutate(label = as.character(label)), \n         colour = \"label\",\n         frame = TRUE, frame.type = 'norm')"},{"path":"principal-component-analysis.html","id":"multidimensional-scaling","chapter":"Lecture 14 Principal Component Analysis","heading":"14.4 Multidimensional scaling","text":"input matrix dissimilarities \\(D\\), potentially representing distances \\(d_{ij} = d(x_i, x_j)\\). distance function “metric” :\\(d(x_i, x_j) \\geq 0\\) (non-negativity)\\(d(x_i, x_j) = 0\\) \\(x_i = x_j\\) (identity)\\(d(x_i, x_j) = d(x_j, x_i)\\) (symmetry)\\(d(x_i, x_k) \\leq d(x_i, x_j) + d(x_j, x_k)\\) (triangle inequality)Given set dissimilarities, can therefore ask whether distances, particularly whether represent Euclidean distances.","code":""},{"path":"principal-component-analysis.html","id":"goal-of-mds","chapter":"Lecture 14 Principal Component Analysis","heading":"14.4.1 Goal of MDS","text":"Given \\(n \\times n\\) matrix \\(D\\), find set coordinates \\(x_i, \\ldots x_n \\\\mathbb R^p\\), \\(d_{ij} \\approx \\lVert x_i - x_j \\rVert_2\\) (close possible). operator \\(\\lVert \\cdot \\rVert_2\\) Euclidean norm, measuring Euclidean distance., can find perfect solution, dissimilarities can mapped Euclidean distances \\(k\\)-dimensional space.","code":""},{"path":"principal-component-analysis.html","id":"classic-mds","chapter":"Lecture 14 Principal Component Analysis","heading":"14.4.2 Classic MDS","text":"Suppose elements \\(D\\) measure Euclidean distances \\(n\\) points, \\(k\\) coordinates:\\[\nX = \\begin{bmatrix}\n    x_{11} & x_{12} &  \\dots  & x_{1k} \\\\\n    x_{21} & x_{22} &  \\dots  & x_{2k} \\\\\n    \\vdots & \\vdots &  \\ddots & \\vdots \\\\\n    x_{n1} & x_{n2} &  \\dots  & x_{nk}\n\\end{bmatrix}\n\\]\nconsider centered coordinates:\\[\n\\sum_i x_{ij} = 0\n\\]\nmatrix \\(B = X X^t\\), whose coefficients \\(B_{ij} = \\sum_k x_{ik} x_{jk}\\). can write square distance point \\(\\) \\(j\\) :\\[ d_{ij}^2 = \\sum_k (x_{ik} - x_{jk})^2  = \\sum_k x_{ik}^2 + \\sum_k x_{jk}^2 -2 \\sum_k x_{ik} x_{jk} = B_{ii} + B_{jj} - 2 B_{ij}\\]Note , centering:\\[\n\\sum_i B_{ij} = \\sum_i \\sum_k x_{ik} x_{jk} = \\sum_k x_{jk} \\sum_i x_{ik} = 0\n\\]Now compute:\\[\n\\sum_i d_{ij}^2 = \\sum_i (B_{ii} + B_{jj} - 2 B_{ij}) = \\sum_i B_{ii} + \\sum_i B_{jj} - 2 \\sum_i B_{ij} = \\text{Tr}(B) + n B_{jj} \n\\]Similarly (distances symmetric):\\[\n\\sum_j d_{ij}^2 = \\text{Tr}(B) + n B_{ii} \n\\], finally:\\[\n\\sum_i \\sum_j d_{ij}^2 = 2 n \\text{Tr}(B)\n\\]three equations, obtain:\\[\nB_{ii} = \\frac{\\sum_j d_{ij}^2}{n} - \\frac{\\sum_i \\sum_j d_{ij}^2 }{2 n^2}\n\\]\\[\nB_{jj} = \\frac{\\sum_i d_{ij}^2}{n} - \\frac{\\sum_i \\sum_j d_{ij}^2 }{2 n^2}\n\\]Therefore:\\[ \nB_{ij} = -\\frac{1}{2}(d_{ij}^2 - B_{ii} - B_{jj}) = -\\frac{1}{2}\\left(d_{ij}^2 - \\frac{\\sum_i d_{ij}^2}{n} - \\frac{\\sum_j d_{ij}^2}{n}  + \\frac{\\sum_i \\sum_j d_{ij}^2 }{n^2} \\right)\n\\]algebra, one can show equivalent :\\[B = -\\frac{1}{2} C D^{(2)} C\\]\\(D^{(2)}\\) matrix squared distances, \\(C\\) centering matrix \\(C = 1 - \\frac{1}{n}\\mathcal O\\) (\\(\\mathcal O\\) matrix ones). Thus, can obtain \\(B\\) directly distance matrix. ’ve done , \\(X\\) can found taking eigenvalue decomposition:\\[\nB = X X^t = Q \\Lambda Q^t\n\\](\\(Q\\) matrix eigenvectors \\(B\\), \\(\\Lambda\\) diagonal matrix eigenvalues \\(B\\)). Therefore:\\[ X = Q \\Lambda^{\\frac{1}{2}}\\]example, let’s look driving distance km cities US:perform classic MDS using two dimensions:","code":"\n# read distances US\nusa <- read_csv(\"data/dist_US.csv\")## \n## ── Column specification ────────────────────────────────────────────────────────\n## cols(\n##   from = col_character(),\n##   to = col_character(),\n##   dist = col_double()\n## )\n# make into a matrix of distances\nM <- usa %>% pivot_wider(names_from = to, values_from = `dist`) %>% \n  select(-from) %>% \n  as.matrix()\nM[is.na(M)] <- 0 \nrownames(M) <- colnames(M)\n# make symmetric\nM <- M + t(M)\nM[1:2, 1:2]##                                        Abilene, TX, United States\n## Abilene, TX, United States                                   0.00\n## Ahwatukee Foothills, AZ, United States                    1487.19\n##                                        Ahwatukee Foothills, AZ, United States\n## Abilene, TX, United States                                            1487.19\n## Ahwatukee Foothills, AZ, United States                                   0.00\nmds_fit <- cmdscale(M, k = 2) # k is the dimension of the embedding\nmds_fit <- tibble(id = rownames(M), \n                  x = mds_fit[,1], y = mds_fit[,2])\npl <- mds_fit %>% \n  ggplot() + \n  aes(x = x, y = y) + \n  geom_point() + \n  xlim(2 * range(mds_fit$x))\n\nshow(pl)\n# highlight some major cities\nhh <- c(122, 175, 177, 373, 408, 445, 572, 596, 691)\nmds_highlight <- mds_fit %>% slice(hh)\nshow(pl + geom_point(data  = mds_highlight, aes(colour = rownames(M)[hh])))"},{"path":"principal-component-analysis.html","id":"readings-1","chapter":"Lecture 14 Principal Component Analysis","heading":"14.5 Readings","text":"SVD important decomposition, several interesting variations proposed data science. Read cool paper face recognition using Non-negative Matrix Factorization.","code":""},{"path":"principal-component-analysis.html","id":"exercise-pca-sommelier","chapter":"Lecture 14 Principal Component Analysis","heading":"14.5.1 Exercise: PCA sommelier","text":"file Wine.csv contains several measures made 178 wines Piedmont, produced using three different grapes (column Grape, 1 = Barolo, 2 = Grignolino, 3 = Barbera). Use 13 measured variables (.e., Grape) perform PCA. First, “hard way” using SVD, , calling prcomp function. Can recover right classification grapes?","code":""},{"path":"clustering.html","id":"clustering","chapter":"Lecture 15 Clustering","heading":"Lecture 15 Clustering","text":"GoalsLearn partitional clusteringLearn hierarchical clusteringUse clustering validation methodsApply different methods larger data setsThe goal clustering classify data points groups (clusters) without giving algorithm knowledge correct classification. type approach called unsupervised learning appropriate “truth” data classification unavailable difficult obtain.truth unknown, need way deciding data points belong together. One common set approaches relies measure closeness, distance points. , classic K-means approach straightforward.","code":"\nlibrary(tidyverse) \nlibrary(ggfortify) \nlibrary(factoextra) \nlibrary(NbClust)\nlibrary(fpc)\nlibrary(clustertend)\nlibrary(palmerpenguins)"},{"path":"clustering.html","id":"k-means-algorithm","chapter":"Lecture 15 Clustering","heading":"15.1 K-means algorithm","text":"divide data K clusterscalculate centroids eachgo data point nothing changes\ncalculate distance centroid\nassign nearest centroid\nrecalculate centroids two affected clusters\ncalculate distance centroidassign nearest centroidrecalculate centroids two affected clustersLet us apply k-means algorithm well-studied penguin data set. script , remove NAs, select categorical variables, directly useful distance-based algorithm, leaving four numeric variables define similarity individuals. question , cluster penguins according species?tThe plot produced performing PCA reduce number variables 4 2, helping present data points way optimizes visual separation. Notice clusters well separated, compared actual classification given species, well.However, four measurements different variances, try scaling make equal variance one:Now get much better separation, well much better prediction quality. However, run code several times, see different results, k-means starts random selection centroids. cases like , obvious clusters, may converge different classifications. , trials see good prediction quality three species, times two species commingled.","code":"\n#set.seed(20)\nglimpse(penguins)## Rows: 344\n## Columns: 7\n## $ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n## $ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n## $ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n## $ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n## $ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n## $ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n## $ sex               <fct> male, female, female, NA, female, male, female, male…\npen_data <- penguins %>% drop_na() \n#pen_train <- pen_data  %>% dplyr::select(-species,-island, -sex, -year) # remove species (the true labels)\npen_train <- pen_data  %>% dplyr::select(-species,-island, -sex) # remove species (the true labels)\npen_km <- kmeans(pen_train, 3) #k-means with 3 clusters\npen_km## K-means clustering with 3 clusters of sizes 80, 113, 140\n## \n## Cluster means:\n##   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n## 1       48.66250      15.39750          219.9875    5365.938\n## 2       44.24336      17.44779          201.5487    4310.619\n## 3       41.12214      17.94643          189.6286    3461.250\n## \n## Clustering vector:\n##   [1] 3 3 3 3 3 3 2 3 3 2 3 3 2 3 2 3 3 3 2 3 3 3 3 3 2 3 2 3 2 3 2 2 3 3 2 3 2\n##  [38] 3 2 3 2 3 3 2 3 2 3 2 3 3 3 3 3 3 3 2 3 2 3 2 3 2 3 2 3 2 3 2 3 2 3 2 3 2\n##  [75] 3 2 3 2 3 3 3 3 2 3 3 2 3 2 3 2 3 2 3 2 3 2 3 2 3 3 3 2 3 2 3 2 3 2 2 2 3\n## [112] 3 3 3 3 3 3 3 3 2 3 2 3 2 3 3 3 2 3 2 3 2 3 2 3 3 3 3 3 3 2 3 3 3 3 2 2 1\n## [149] 2 1 1 2 2 1 2 1 2 1 2 1 2 1 2 1 2 1 1 1 2 1 1 1 1 2 1 1 2 1 1 1 1 1 1 2 1\n## [186] 2 1 2 2 1 1 2 1 1 1 1 1 2 1 1 1 2 1 2 1 2 1 2 1 2 1 1 2 1 2 1 1 1 2 1 2 1\n## [223] 2 1 2 1 2 1 2 1 2 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 2 1 2 1 1 1 2 1 2 1\n## [260] 1 1 1 1 1 1 3 2 3 3 3 2 3 3 2 3 3 3 3 2 3 2 3 3 3 2 3 3 3 3 3 2 3 3 3 2 3\n## [297] 2 3 2 3 2 3 2 3 2 2 3 3 3 3 2 3 2 3 3 3 2 3 2 3 3 3 2 3 3 2 3 3 2 3 3 2 3\n## \n## Within cluster sum of squares by cluster:\n## [1] 9718829 9318036 9724809\n##  (between_SS / total_SS =  86.6 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\ntable(pen_km$cluster, pen_data$species)##    \n##     Adelie Chinstrap Gentoo\n##   1      0         0     80\n##   2     52        22     39\n##   3     94        46      0\nfviz_cluster(list(data = pen_train, cluster = pen_km$cluster),\nellipse.type = \"norm\", geom = \"point\", stand = FALSE, palette = \"jco\", ggtheme = theme_classic())\npen_data <- penguins %>% drop_na()\n#pen_scaled <- scale(pen_data %>% dplyr::select(-species, -island, -sex, -year) )\npen_scaled <- scale(pen_data %>% dplyr::select(-species, -island, -sex) )\npen_km <- kmeans(pen_scaled, 3)\npen_km## K-means clustering with 3 clusters of sizes 129, 119, 85\n## \n## Cluster means:\n##   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n## 1     -1.0452359     0.4858944        -0.8803701  -0.7616078\n## 2      0.6537742    -1.1010497         1.1607163   1.0995561\n## 3      0.6710153     0.8040534        -0.2889118  -0.3835267\n## \n## Clustering vector:\n##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 3 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n##  [38] 1 3 1 1 1 1 3 1 1 1 3 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 3 1 1 1 3 1 3 1 1 1 3\n##  [75] 1 3 1 1 1 1 1 1 1 1 1 3 1 1 1 3 1 1 1 3 1 3 1 1 1 1 1 1 1 3 1 3 1 3 1 3 1\n## [112] 1 1 1 1 1 1 1 1 1 1 1 1 3 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 2 2\n## [149] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n## [186] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n## [223] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n## [260] 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 1 3 3 3 3 3 3 3 1\n## [297] 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3\n## \n## Within cluster sum of squares by cluster:\n## [1] 120.7030 139.4684 109.4813\n##  (between_SS / total_SS =  72.2 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\ntable(pen_km$cluster, pen_data$species)##    \n##     Adelie Chinstrap Gentoo\n##   1    124         5      0\n##   2      0         0    119\n##   3     22        63      0\nfviz_cluster(list(data = pen_scaled, cluster = pen_km$cluster),\nellipse.type = \"norm\", geom = \"point\", stand = FALSE, palette = \"jco\", ggtheme = theme_classic())"},{"path":"clustering.html","id":"assumptions-of-k-means-algorithm","chapter":"Lecture 15 Clustering","heading":"15.1.1 Assumptions of K-means algorithm","text":"meaningful distance measureClusters roughly sphericalClusters similar size","code":"\n# Generate random data which will be first cluster\nclust1 <- data_frame(x = rnorm(200), y = rnorm(200))\n# Generate the second cluster which will ‘surround’ the first cluster\nclust2 <- data_frame(r = rnorm(200, 15, .5), \n                     theta = runif(200, 0, 2 * pi),\n                 x = r * cos(theta), y = r * sin(theta)) %>%\n  dplyr::select(x, y)\n#Combine the data\ndataset_cir <- rbind(clust1, clust2)\n#see the plot\ndataset_cir %>% ggplot() + aes(x = x, y = y) + geom_point()\n#Fit the k-means model\nk_clust_spher1 <- kmeans(dataset_cir, centers=2)\n#Plot the data and clusters\nfviz_cluster(list(data = dataset_cir, \n                  cluster = k_clust_spher1$cluster),\n             ellipse.type = \"norm\", \n             geom = \"point\", stand = FALSE, \n             palette = \"jco\", \n             ggtheme = theme_classic())\n# Make the first cluster with 200 random values\nclust1 <- data_frame(x = rnorm(200), \n                     y = rnorm(200))\n# Keep 10 values together to make the second cluster\nclust2 <- data_frame(x=c(5,5.1,5.2,5.3,5.4),\n                     y=c(5,5,5,5,5))\n#Combine the data\ndataset_uneven <- rbind(clust1,clust2)\ndataset_uneven %>% ggplot() + aes(x = x, y = y) + geom_point()\nk_clust_spher3 <- kmeans(dataset_uneven, centers=2)\nfviz_cluster(list(data = dataset_uneven, \n                  cluster = k_clust_spher3$cluster),\n             ellipse.type = \"norm\", \n             geom = \"point\", \n             stand = FALSE, \n             palette = \"jco\", \n             ggtheme = theme_classic())"},{"path":"clustering.html","id":"hierarchical-clustering","chapter":"Lecture 15 Clustering","heading":"15.2 Hierarchical clustering","text":"Hierarchical clustering different approach k-means, although also based notion distance. goal create tree, akin phylogeny, based proximity different points , divide groups cutting tree certain depth root.","code":""},{"path":"clustering.html","id":"agglomerative-clustering","chapter":"Lecture 15 Clustering","heading":"15.2.1 Agglomerative clustering","text":"Start single data points “clusters,” iteratively combine closest pair clusters. closeness may defined following ways:Single Linkage: single linkage, define distance two clusters minimum distance single data point first cluster single data point second cluster.Single Linkage: single linkage, define distance two clusters minimum distance single data point first cluster single data point second cluster.Complete Linkage: complete linkage, define distance two clusters maximum distance single data point first cluster single data point second cluster.Complete Linkage: complete linkage, define distance two clusters maximum distance single data point first cluster single data point second cluster.Average Linkage: average linkage, define distance two clusters average distance data points first cluster data points second cluster.Average Linkage: average linkage, define distance two clusters average distance data points first cluster data points second cluster.Centroid Method: centroid method, distance two clusters distance two mean vectors clusters.Centroid Method: centroid method, distance two clusters distance two mean vectors clusters.Ward’s Method: method directly define measure distance two points clusters. ANOVA based approach. One-way univariate ANOVAs done variable groups defined clusters stage process. stage, two clusters merge provide smallest increase combined error sum squares.Ward’s Method: method directly define measure distance two points clusters. ANOVA based approach. One-way univariate ANOVAs done variable groups defined clusters stage process. stage, two clusters merge provide smallest increase combined error sum squares.","code":"\n# Use hcut() which compute hclust and cut the tree\ncir_hc <- hcut(dataset_cir, k = 2, hc_method = \"single\")\n# Visualize dendrogram\nfviz_dend(cir_hc, show_labels = FALSE, rect = TRUE)\n# Visualize cluster\nfviz_cluster(cir_hc, ellipse.type = \"convex\")\n# Use hcut() which compute hclust and cut the tree\nuneven_hc <- hcut(dataset_uneven, k = 2, hc_method = \"single\")\n# Visualize dendrogram\nfviz_dend(uneven_hc, show_labels = FALSE, rect = TRUE)\n# Visualize cluster\nfviz_cluster(uneven_hc, ellipse.type = \"convex\")"},{"path":"clustering.html","id":"clustering-penguin-data-using-hierarchical-methods","chapter":"Lecture 15 Clustering","heading":"15.2.2 Clustering penguin data using hierarchical methods","text":"Try different methods see one generates best resultsExercise Try using different clustering methods!","code":"\n# Hierarchical clustering\n# ++++++++++++++++++++++++\n# Use hcut() which compute hclust and cut the tree\npen_hc <- hcut(pen_scaled, k = 3, hc_method = \"complete\")\n# Visualize dendrogram\nfviz_dend(pen_hc)\n# Visualize cluster\nfviz_cluster(pen_hc)\ntable(pen_hc$cluster, pen_data$species)##    \n##     Adelie Chinstrap Gentoo\n##   1    145         6      0\n##   2      1        62      0\n##   3      0         0    119"},{"path":"clustering.html","id":"clustering-analysis-and-validation","chapter":"Lecture 15 Clustering","heading":"15.3 Clustering analysis and validation","text":"","code":""},{"path":"clustering.html","id":"hopkins-statistic","chapter":"Lecture 15 Clustering","heading":"15.3.1 Hopkins statistic","text":"Comparing mean nearest-neighbor distance uniformly generated sample points mean nearest-neighbor distance within data set.\n\\[\nH = 1 - \\frac{\\sum u^d_i}{\\sum u^d_i + \\sum w^d_i}\n\\]\nquantifies “clustering tendency” data set.H 0.5 reject null hypothesis, data generated Poisson point process (uniformly distributed.)","code":"\n# Check Cluster Tendency--Hopkins Statistic\nhopkins(pen_scaled, n = 30) # n should be about 20% of the data## $H\n## [1] 0.1740598\n# run a couple times to sample repeatedly\n# Visual Assessment of Cluster Tendency\nfviz_dist(dist(pen_scaled), show_labels = FALSE)+ labs(title = \"Scaled penguin data\")"},{"path":"clustering.html","id":"elbow-method","chapter":"Lecture 15 Clustering","heading":"15.3.2 Elbow method","text":"","code":"\n# Elbow method\nfviz_nbclust(pen_scaled, kmeans, method = \"wss\") + geom_vline(xintercept = 2, linetype = 2)+\nlabs(subtitle = \"Elbow method for K-means of the scaled penguin data\")"},{"path":"clustering.html","id":"silhouette-plot","chapter":"Lecture 15 Clustering","heading":"15.3.3 Silhouette Plot","text":"Measures similar object \\(\\) objects cluster versus objects outside cluster; \\(S_i\\) values range -1 1. Close 1 means similar objects group dissimilar others","code":"\n# Silhouette method\nfviz_nbclust(pen_scaled, kmeans, method = \"silhouette\")+ labs(subtitle = \"Silhouette method for k-means\")"},{"path":"clustering.html","id":"lazy-way-use-all-the-methods","chapter":"Lecture 15 Clustering","heading":"15.3.4 Lazy way: use all the methods!","text":"","code":"\nnb <- NbClust(pen_scaled, distance = \"euclidean\", min.nc = 2,\n        max.nc = 10, method = \"kmeans\")## *** : The Hubert index is a graphical method of determining the number of clusters.\n##                 In the plot of Hubert index, we seek a significant knee that corresponds to a \n##                 significant increase of the value of the measure i.e the significant peak in Hubert\n##                 index second differences plot. \n## ## *** : The D index is a graphical method of determining the number of clusters. \n##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex\n##                 second differences plot) that corresponds to a significant increase of the value of\n##                 the measure. \n##  \n## ******************************************************************* \n## * Among all indices:                                                \n## * 9 proposed 2 as the best number of clusters \n## * 6 proposed 3 as the best number of clusters \n## * 1 proposed 4 as the best number of clusters \n## * 1 proposed 5 as the best number of clusters \n## * 6 proposed 6 as the best number of clusters \n## * 1 proposed 10 as the best number of clusters \n## \n##                    ***** Conclusion *****                            \n##  \n## * According to the majority rule, the best number of clusters is  2 \n##  \n##  \n## *******************************************************************\nfviz_nbclust(nb)## Among all indices: \n## ===================\n## * 2 proposed  0 as the best number of clusters\n## * 9 proposed  2 as the best number of clusters\n## * 6 proposed  3 as the best number of clusters\n## * 1 proposed  4 as the best number of clusters\n## * 1 proposed  5 as the best number of clusters\n## * 6 proposed  6 as the best number of clusters\n## * 1 proposed  10 as the best number of clusters\n## \n## Conclusion\n## =========================\n## * According to the majority rule, the best number of clusters is  2 ."},{"path":"clustering.html","id":"validation-using-bootstrapping","chapter":"Lecture 15 Clustering","heading":"15.3.5 Validation using bootstrapping","text":"One common approach validating clustering use approach called bootstrapping involves repeatedly sampling data set, running clustering algorithm comparing results. One algorithm uses Jaccard coefficient quantify similarity sets, defined number points intersection two sets (sets), divided number points union two sets (point either one set):\\[\nJ = \\frac{ \\vert \\cap B \\vert }{\\vert \\cup B \\vert}\n\\]\nvertical lines indicate number points (cardinality) set.","code":"\nk <- 3\ncboot.hclust <- clusterboot(pen_scaled, clustermethod=kmeansCBI, k= k)## boot 1 \n## boot 2 \n## boot 3 \n## boot 4 \n## boot 5 \n## boot 6 \n## boot 7 \n## boot 8 \n## boot 9 \n## boot 10 \n## boot 11 \n## boot 12 \n## boot 13 \n## boot 14 \n## boot 15 \n## boot 16 \n## boot 17 \n## boot 18 \n## boot 19 \n## boot 20 \n## boot 21 \n## boot 22 \n## boot 23 \n## boot 24 \n## boot 25 \n## boot 26 \n## boot 27 \n## boot 28 \n## boot 29 \n## boot 30 \n## boot 31 \n## boot 32 \n## boot 33 \n## boot 34 \n## boot 35 \n## boot 36 \n## boot 37 \n## boot 38 \n## boot 39 \n## boot 40 \n## boot 41 \n## boot 42 \n## boot 43 \n## boot 44 \n## boot 45 \n## boot 46 \n## boot 47 \n## boot 48 \n## boot 49 \n## boot 50 \n## boot 51 \n## boot 52 \n## boot 53 \n## boot 54 \n## boot 55 \n## boot 56 \n## boot 57 \n## boot 58 \n## boot 59 \n## boot 60 \n## boot 61 \n## boot 62 \n## boot 63 \n## boot 64 \n## boot 65 \n## boot 66 \n## boot 67 \n## boot 68 \n## boot 69 \n## boot 70 \n## boot 71 \n## boot 72 \n## boot 73 \n## boot 74 \n## boot 75 \n## boot 76 \n## boot 77 \n## boot 78 \n## boot 79 \n## boot 80 \n## boot 81 \n## boot 82 \n## boot 83 \n## boot 84 \n## boot 85 \n## boot 86 \n## boot 87 \n## boot 88 \n## boot 89 \n## boot 90 \n## boot 91 \n## boot 92 \n## boot 93 \n## boot 94 \n## boot 95 \n## boot 96 \n## boot 97 \n## boot 98 \n## boot 99 \n## boot 100\nprint(cboot.hclust)## * Cluster stability assessment *\n## Cluster method:  kmeans \n## Full clustering results are given as parameter result\n## of the clusterboot object, which also provides further statistics\n## of the resampling results.\n## Number of resampling runs:  100 \n## \n## Number of clusters found in data:  3 \n## \n##  Clusterwise Jaccard bootstrap (omitting multiple points) mean:\n## [1] 0.6022332 0.7337821 0.6386541\n## dissolved:\n## [1] 56  0 25\n## recovered:\n## [1] 26 28 28\n#cboot.hclust <- clusterboot(bcdata, clustermethod=hclustCBI,\n       #                    method=\"single\", k=2)"},{"path":"clustering.html","id":"application-to-breast-cancer-data","chapter":"Lecture 15 Clustering","heading":"15.4 Application to breast cancer data","text":"following measurements based biopsy data patients suspected breast cancer (see [5]). contains several measurements cell characteristics, well classification biopsy malignant benign (2 4). Let us see using clustering","code":"\n# Import Breast Cancer Data Set\nfulldata <- read_csv(\"data/Wisconsin_Breast_Cancers.csv\")\nbcdata <- fulldata %>% drop_na() %>% dplyr::select(-Sample, -Class)\nglimpse(fulldata)## Rows: 684\n## Columns: 11\n## $ Sample                      <dbl> 1000025, 1002945, 1015425, 1016277, 101702…\n## $ Clump_Thickness             <dbl> 5, 5, 3, 6, 4, 8, 1, 2, 2, 4, 1, 2, 5, 1, …\n## $ Size_Uniformity             <dbl> 1, 4, 1, 8, 1, 10, 1, 1, 1, 2, 1, 1, 3, 1,…\n## $ Shape_Uniformity            <dbl> 1, 4, 1, 8, 1, 10, 1, 2, 1, 1, 1, 1, 3, 1,…\n## $ Marginal_Adhesion           <dbl> 1, 5, 1, 1, 3, 8, 1, 1, 1, 1, 1, 1, 3, 1, …\n## $ Single_Epithelial_Cell_Size <dbl> 2, 7, 2, 3, 2, 7, 2, 2, 2, 2, 1, 2, 2, 2, …\n## $ Bare_Nuclei                 <dbl> 1, 10, 2, 4, 1, 10, 10, 1, 1, 1, 1, 1, 3, …\n## $ Bland_Chromatin             <dbl> 3, 3, 3, 3, 3, 9, 3, 3, 1, 2, 3, 2, 4, 3, …\n## $ Normal_Nucleoli             <dbl> 1, 2, 1, 7, 1, 7, 1, 1, 1, 1, 1, 1, 4, 1, …\n## $ Mitoses                     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, …\n## $ Class                       <dbl> 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 4, 2, …\n# Visually Inspect Data (PCA)\nfviz_pca_ind(prcomp(bcdata), title = \"PCA - Breast Cancer data\", geom = \"point\", ggtheme = theme_classic())\nbc_km <- kmeans(scale(bcdata), 2)\nbc_km## K-means clustering with 2 clusters of sizes 453, 231\n## \n## Cluster means:\n##   Clump_Thickness Size_Uniformity Shape_Uniformity Marginal_Adhesion\n## 1      -0.4973081      -0.6104358       -0.6062297        -0.5191788\n## 2       0.9752406       1.1970884        1.1888401         1.0181299\n##   Single_Epithelial_Cell_Size Bare_Nuclei Bland_Chromatin Normal_Nucleoli\n## 1                  -0.5133379  -0.5896356      -0.5498977       -0.531641\n## 2                   1.0066757   1.1562984       1.0783707        1.042569\n##      Mitoses\n## 1 -0.3070638\n## 2  0.6021640\n## \n## Clustering vector:\n##   [1] 1 2 1 2 1 2 1 1 1 1 1 1 1 1 2 2 1 1 2 1 2 2 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1\n##  [38] 2 2 2 2 2 2 1 2 1 1 2 2 1 2 2 2 2 2 1 1 1 2 1 2 1 1 2 1 2 2 1 1 2 1 2 2 1\n##  [75] 1 1 1 1 1 1 1 1 2 2 2 2 1 1 1 1 1 1 1 1 1 1 2 2 2 1 1 1 2 2 2 2 1 2 1 2 2\n## [112] 2 1 1 1 2 1 1 1 1 2 2 2 1 2 1 2 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 2 1 2\n## [149] 2 1 1 2 1 1 2 2 1 1 1 1 2 2 1 1 1 1 1 2 2 2 1 2 1 1 1 1 1 2 2 1 2 2 2 1 2\n## [186] 2 1 1 1 1 2 1 1 1 2 2 1 1 1 2 2 1 1 1 2 2 1 2 2 2 1 1 2 1 1 2 1 2 2 1 2 2\n## [223] 1 2 2 2 1 2 1 2 2 2 2 1 1 1 1 1 1 2 1 1 1 2 2 2 2 2 1 1 1 2 2 2 2 2 2 1 2\n## [260] 2 2 1 2 1 2 1 1 1 1 1 2 1 1 2 2 2 2 2 1 2 2 1 1 2 2 2 1 2 2 1 2 1 2 2 1 1\n## [297] 2 1 1 1 2 1 1 2 2 1 2 2 1 2 1 1 1 1 2 2 2 1 1 2 2 1 2 1 1 2 2 1 1 1 2 1 1\n## [334] 1 1 2 1 1 2 2 1 1 1 2 2 2 2 2 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1\n## [371] 1 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 2 1 2 1 2 1 1 1 1 2\n## [408] 1 1 1 2 1 2 1 1 1 1 1 1 2 2 2 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 2 2 1\n## [445] 1 1 1 1 1 1 2 2 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2 2 1 1 1 2 2 1 1 2 1 2 1 1\n## [482] 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 2 1 1 1 2 1 1 2 2 1 1 1 1 1 1 2 1 1\n## [519] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2 2\n## [556] 2 2 1 1 2 1 1 1 1 1 1 2 2 1 1 1 2 1 2 1 2 2 2 1 2 1 1 1 1 1 1 1 1 2 2 2 1\n## [593] 1 2 1 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1\n## [630] 1 1 1 2 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 2 2 2 1 1 1 1 1 1 1 1 1 2 2\n## [667] 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 2 2 2\n## \n## Within cluster sum of squares by cluster:\n## [1]  573.108 2156.785\n##  (between_SS / total_SS =  55.6 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\ntable(bc_km$cluster, fulldata$Class)##    \n##       2   4\n##   1 434  19\n##   2  10 221\n#irisCluster$cluster <- as.factor(irisCluster$cluster)\n#ggplot(iris, aes(Petal.Length, Petal.Width, color = iris$cluster)) + geom_point()\n\nfviz_cluster(list(data = bcdata, cluster = bc_km$cluster),\nellipse.type = \"norm\", geom = \"point\", stand = FALSE, palette = \"jco\", ggtheme = theme_classic())\n# Use hcut() which compute hclust and cut the tree\nbc_hc <- hcut(scale(bcdata), k = 2, hc_method = \"ward\")\n# Visualize dendrogram\nfviz_dend(bc_hc, show_labels = FALSE, rect = TRUE)\n# Visualize cluster\nfviz_cluster(bc_hc, ellipse.type = \"convex\")\ntable(bc_hc$cluster, fulldata$Class)##    \n##       2   4\n##   1 412   2\n##   2  32 238"},{"path":"clustering.html","id":"references-1","chapter":"Lecture 15 Clustering","heading":"15.5 References:","text":"https://www.r-bloggers.com/exploring-assumptions--k-means-clustering-using-r/https://onlinecourses.science.psu.edu/stat505/node/143/https://github.com/hhundiwala/hierarchical-clusteringhttps://www.r-bloggers.com/bootstrap-evaluation--clusters/https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)","code":""},{"path":"building-phylogeneric-trees.html","id":"building-phylogeneric-trees","chapter":"Lecture 16 Building phylogeneric trees","heading":"Lecture 16 Building phylogeneric trees","text":"Let’s import libraries:","code":"\nlibrary(ape) # most important library for phylogeny## \n## Attaching package: 'ape'## The following object is masked from 'package:rsample':\n## \n##     complement\nlibrary(phangorn) # tree reconstruction## \n## Attaching package: 'phangorn'## The following object is masked from 'package:yardstick':\n## \n##     mcc"},{"path":"building-phylogeneric-trees.html","id":"introduction-1","chapter":"Lecture 16 Building phylogeneric trees","heading":"16.1 Introduction","text":"Darwin’s treeSince Darwin’s “think…” biologists constructing trees connecting species common ancestors. goal brief tutorial introduce basic terminology, illustrate common approaches building trees.tutorial based freely available book Mathematics Phylogenetics, Allman Rhodes.","code":""},{"path":"building-phylogeneric-trees.html","id":"input-1","chapter":"Lecture 16 Building phylogeneric trees","heading":"16.2 Input","text":"input assumed series sequences, one taxon. simplicity, deal DNA sequences, assume taxa different species. However, methods introduced can extended types sequences (e.g., amino acids, RNA, morphological features) taxa (e.g., individuals species, even cells individual). assume DNA sequences orthologous (.e., descended common ancestral sequence) aligned (.e., many bases matching across sequences taxa).DNA composed four nucleotides bases: adenine (), guanine (G), cytosine (C), thymine (T). G purines, C T pyrimidines. DNA copied, small errors (mutations) can introduced: speak substitutions base changed, deletions one consecutive bases copied, insertions new bases introduced sequence. tutorial, concentrate substitutions, believed common type mutations. distinguish transition (purine purine, pyrimidine pyrimidine) transversion (one type ). Many models assume transitions frequent transversions (due smaller changes chemical stability DNA).","code":""},{"path":"building-phylogeneric-trees.html","id":"representing-trees","chapter":"Lecture 16 Building phylogeneric trees","heading":"16.3 Representing trees","text":"graph collection vertices (\\(V\\)) edges (\\(E\\)). consider simple, undirected graph tutorial; simple means self loops allowed two nodes can connected one edge ; undirected distinguish \\(\\b\\) \\(b \\\\). graph connected way go node following edges. cycle closed path connecting node . tree connected graph contains cycles.distinguish rooted unrooted trees: root particular vertex taxa descend (MRCA, recent common ancestor taxa). Note placing root different places, can obtain dramatically different-looking trees:Typically, can observe tips leaves tree (extant species); common ancestors internal nodes tree, inferred.unrooted tree \\(n > 1\\) leaves contains exactly \\(2n -2\\) nodes, \\(2n -3\\) edges. rooted tree one extra node (\\(2n -1\\)) one extra edge (\\(2n - 2\\)). many trees one can form \\(n\\) leaves: can count \\((2n -3)!!\\) rooted trees (double factorial becomes \\(1 \\cdot 3 \\cdot 5 \\cdots (2n -3)\\)).can add “length” edge, measuring much change accumulated two vertices. case “molecular clock” trees, assume lengths measuring time, leaves distance (computed summing lengths along branches) root. Trees leaves distance root called ultrametric.Mathematically, make assumption “molecular clock” (.e., mutations neutral occur predictable rate), identify root tree without using extra information. Typically, done also align sequence(s) stemming -group, .e., taxon believed distantly related taxa want connect tree. , root connect -group taxon rest tree.Many methods tree reconstruction based unrooted, unweighted trees, root lengths can determined one chosen topology tree.simple way represent tree provided Newick notation: string (((, b), c), (d, e)) represents tree two nodes enclosed parentheses connected common ancestor (internal node). Note notation readily discriminate among identical trees (e.g., ((d, e), (c, (b, )))) spotting two identical trees difficult eye.","code":""},{"path":"building-phylogeneric-trees.html","id":"building-trees-from-sequence-data","chapter":"Lecture 16 Building phylogeneric trees","heading":"16.4 Building trees from sequence data","text":"Provided set orthologous, aligned sequences, want build tree explaining observed mutations. methods build trees sequences based Occam’s razor: given alternative histories evolution sequences, take “simplest” probable.","code":""},{"path":"building-phylogeneric-trees.html","id":"maximum-parsimony","chapter":"Lecture 16 Building phylogeneric trees","heading":"16.4.1 Maximum Parsimony","text":"Going back example:think tree \\(((1, 2), (3, 4))\\) consistent changes position 4 8; changes position 9, however, require mutation branch. can count number changes sequences required tree, maximum parsimony can summarized “best tree infer data one requiring fewest changes.”can propose tree, edge connecting nodes \\((u, v)\\), can count number changes required go \\(u\\) \\(v\\). can sum changes (called parsimony score) choose tree minimum score.ParsimonyComputing minimum score given tree sequences leaves called “small parsimony problem” computing minimum parsimony trees “large parsimony problem.” small problem can solved efficiently (e.g., using Fitch-Hartigan algorithm). large problem computationally difficult.","code":""},{"path":"building-phylogeneric-trees.html","id":"example-primates","chapter":"Lecture 16 Building phylogeneric trees","heading":"16.4.2 Example: primates","text":"Now let’s build random tree compute parsimony:can compute parsimony score tree:Meaning recover observed sequences ancestral one, need assume 921 changes occurred. Clearly, doesn’t seem especially promising tree (example, Humans Chimpanzees quite far, know close). Let’s try another random tree:even worse! can try “tweak” structure tree reduce parsimony score calling optim.parsimony. example:Trying random tree:shows starting tree matters. fact, find especially good solutions even calling optim.parsimony. Now going try starting tree based distances (explained ).","code":"\n# read alignment primates\nfdir <- system.file(\"extdata/trees\", package = \"phangorn\")\nprimates <- read.phyDat(file.path(fdir, \"primates.dna\"), format = \"interleaved\")\nprimates$Human##   [1] 1 2 2 2 2 1 2 4 2 1 2 2 2 1 4 1 2 1 1 1 2 1 1 2 1 2 2 1 2 4 2 4 2 2 2 2 4\n##  [38] 1 1 4 4 1 2 1 1 4 4 4 1 1 2 2 4 2 2 2 1 2 2 4 4 2 1 3 1 1 2 4 3 1 1 2 3 2\n##  [75] 2 1 1 4 2 4 2 1 4 1 1 2 2 1 1 2 1 2 1 2 2 2 2 1 4 2 1 1 1 3 2 1 2 2 2 2 4\n## [112] 2 2 1 1 2 1 2 1 2 2 2 3 2 1 2 1 2 2 4 2 2 1 2 2 2 2 2 2 4 2 3 4 2 4 1 2 3\n## [149] 2 4 4 1 2 2 1 2 3 4 2 4 2 2 2 4 2 2 2 4 2 4 2 1 2 1 2 2 4 4 1 2 4 2 1 2 2\n## [186] 4 4 2 4 2 2 2 1 1 1 2 3 1 2 4 4 2 3 2 1 2 2 1 2 1 1 2 3 2 2 1 2\nset.seed(0)\n# generate random tree\nmy_tree <- rtree(14, rooted = FALSE, tip.label = names(primates))\nplot(my_tree)\nparsimony(my_tree, primates)## [1] 944\n# try with another one\nmy_tree2 <- rtree(14, rooted = FALSE, tip.label = names(primates))\nplot(my_tree2)\nparsimony(my_tree2, primates)## [1] 931\n# try to find a good tree\noptim_tree <- optim.parsimony(my_tree, primates)## Final p-score 751 after  21 nni operations\nplot(optim_tree)\noptim_tree2 <- optim.parsimony(my_tree2, primates)## Final p-score 746 after  14 nni operations\nplot(optim_tree2)\n# compute distances\ndm <- dist.ml(primates)\n# build tree using distances\ntreeNJ <- NJ(dm)\ntreeUPGMA <- upgma(dm)\nparsimony(treeNJ, primates)## [1] 746\noptim_tree3 <- optim.parsimony(treeNJ, primates)## Final p-score 746 after  0 nni operations\nplot(optim_tree3)\nparsimony(treeUPGMA, primates)## [1] 869\noptim_tree4 <- optim.parsimony(treeUPGMA, primates)## Final p-score 746 after  15 nni operations\nplot(optim_tree4)"},{"path":"building-phylogeneric-trees.html","id":"parsimonygate","chapter":"Lecture 16 Building phylogeneric trees","heading":"16.4.3 #ParsimonyGate","text":"Maximum parsimony often used, several problems (fact, method perfect, strengths limitations). example, typically exist several trees score.journal Cladistics published editorial February 2016 stating “Phylogenetic data sets submitted journal analysed using parsimony. alternative methods give different results author prefers unparsimonious topology, welcome present result, prepared defend philosophical grounds” (emphasis mine). mention philosophy justify scientific results led twitter storm, ignited Jonathan Eisen (entertaining account, see ).","code":""},{"path":"building-phylogeneric-trees.html","id":"distance-methods","chapter":"Lecture 16 Building phylogeneric trees","heading":"16.5 Distance Methods","text":"second class methods based measuring “dissimilarity” (“distance”) sequences. idea preferentially connect common ancestors species “close.” similar ’ve done MDS, basic notion distance . start computing matrix dissimilarities (distances), use algorithm build tree.simple method build tree given dissimilarity matrix called unweighted pair-group method arithmetic means (UPGMA). idea simple: start joining two leaves smallest distance. remove leaves add new node average two leaves. repeat operation built tree.UPGMANote UPGMA builds trees branch lengths assumes data produced molecular clock. condition often violated, need choose another algorithm whenever suspect dissimilarities stemming ultrametric tree. practice, “Neighbor Joining” algorithm (NJ) often used.algorithm based -called four-point condition: given four taxa \\(, b, c, d\\) (might include repetitions), metric tree \\[\n\\delta(, b) + \\delta(c, d) \\leq \\max\\{\\delta(, c) + \\delta(b, d), \\delta(, d) + \\delta(b, c)\\}\n\\]\ninequality used choose two leaves join, process repeated UPGMA. Despite complex, NJ often used practice produce good starting tree. Note NJ builds unrooted tree.","code":"\nplot(treeNJ, type = \"unrooted\", lab4ut = \"axial\")"},{"path":"building-phylogeneric-trees.html","id":"likelihood-based-methods","chapter":"Lecture 16 Building phylogeneric trees","heading":"16.6 Likelihood based methods","text":"goal likelihood-based methods compute probability observed given set sequences given tree. , often make strong assumption independence: site behaves independently, bases taken distribution (..d.).assumption, care exact sequence, number bases kind. example, can assume root distribution vector \\[\np_\\rho = (p_A, p_G, p_C, p_T)\n\\]elements summing one.Now can model probability base root mutate another base substitution along edge \\(e\\). can use transition matrix (Markov matrix):\\[\nM_e = \\begin{pmatrix}\np_{AA} & p_{AG} & p_{AC} & p_{}\\\\\np_{GA} & p_{GG} & p_{GC} & p_{GT}\\\\\np_{CA} & p_{CG} & p_{CC} & p_{CT}\\\\\np_{TA} & p_{TG} & p_{TC} & p_{TT}\n\\end{pmatrix}\n\\]non-negative entries rows summing one. matrix encodes probability base ancestor substituted descendant along edge \\(e\\). can compute matrix given edge?Clearly, edge give rise Markov matrix, making difficult infer tree sequences. Instead, can link matrices along edges making function matrix rates length edge. solution build matrix rates \\(Q\\) (rows summing zero, -diagonal elements positive). elements matrix \\(Q\\) describe instantaneous rate substitution. want model time elapsed along one edge. solve differential equation:\\[\n\\frac{d p(t)}{dt} = p(t) Q\n\\]linear system ODEs:\\[\np(t) = p(0) e^{Qt} = p(0) S e^{\\Lambda t} S^{-1}\n\\]\\(S\\) matrix eigenvectors, \\(\\Lambda\\) diagonal matrix eigenvalues \\(Q\\).edge \\(e\\) corresponding time \\(t_e\\), choose \\(M_e = M(t_e) = e^{Qt_e}\\) matrix projecting proportion bases ancestor number bases descendant. practice, computational convenience, often choose \\(Q\\) special structure.","code":""},{"path":"building-phylogeneric-trees.html","id":"jukes-cantor-model","chapter":"Lecture 16 Building phylogeneric trees","heading":"16.6.0.1 Jukes-Cantor Model","text":"simplest model base substitution. assumes ancestral sequence, bases occur probability:\\[\np_\\rho = (1/4, 1/4, 1/4, 1/4)\n\\]\nbase substituted equal rates:\\[\nQ = \\begin{pmatrix}\n-\\alpha & \\alpha / 3 & \\alpha / 3 & \\alpha / 3 \\\\\n\\alpha / 3 & -\\alpha &  \\alpha / 3 & \\alpha / 3 \\\\\n\\alpha / 3 & \\alpha / 3 &  -\\alpha &  \\alpha / 3 \\\\\n\\alpha / 3 & \\alpha / 3 &   \\alpha / 3 & -\\alpha\n\\end{pmatrix}\n\\]\n, total rate specific base substituted (three) \\(\\alpha\\). calculation, find :\\[\nS = \\begin{pmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & -1 & 1 & -1 \\\\\n1 & 1 & -1 & -1 \\\\\n1 & -1 & -1 & 1\n\\end{pmatrix}\n\\]\n\\[\n\\Lambda = \\begin{pmatrix}\n0 & 0 & 0 & 0\\\\\n0 & -\\frac{4}{3} \\alpha & 0 & 0\\\\\n0 & 0 & -\\frac{4}{3} \\alpha & 0\\\\\n0 & 0 & 0 & -\\frac{4}{3} \\alpha\\\\\n\\end{pmatrix}\n\\]:\\[\nM(t) = e^{Qt} = S e^{\\Lambda t} S^{-1} = \\begin{pmatrix}\n1- (t) & (t) / 3 & (t) / 3 & (t) / 3 \\\\\n(t) / 3 & 1 -(t) &  (t) / 3 & (t) / 3 \\\\\n(t) / 3 & (t) / 3 &  1 -(t) &  (t) / 3 \\\\\n(t) / 3 & (t) / 3 &   (t) / 3 & 1-(t)\n\\end{pmatrix}\n\\]\\((t) = \\frac{3}{4}\\left(1 - e^{-\\frac{4}{3} \\alpha t}\\right)\\). Clearly, larger \\(t\\) \\(\\alpha\\), substitutions expect occur. Note however model encodes stable base distribution vertices tree: \\(p_\\rho M(t) = p_\\rho\\).","code":""},{"path":"building-phylogeneric-trees.html","id":"other-models","chapter":"Lecture 16 Building phylogeneric trees","heading":"16.6.1 Other models","text":"models use larger number parameters, attempting model mutations precisely. example, Kimura 2-parameter model uses matrix:\\[\nQ = \\begin{pmatrix}\n-\\beta - 2 \\gamma & \\beta & \\gamma & \\gamma \\\\\n\\beta & -\\beta - 2 \\gamma & \\gamma & \\gamma \\\\\n\\gamma & \\gamma & -\\beta - 2 \\gamma & \\beta  \\\\\n\\gamma & \\gamma & \\beta & -\\beta - 2 \\gamma\\\\\n\\end{pmatrix}\n\\]can treated way , leading 2-parameter \\(M(t)\\).","code":""},{"path":"building-phylogeneric-trees.html","id":"maximum-likelihood","chapter":"Lecture 16 Building phylogeneric trees","heading":"16.6.2 Maximum likelihood","text":"Armed definitions , now want fit length edge stemming root. sequences \\(S0\\) \\(S1\\), build matrix \\(N\\) whose elements \\(n_{ij}\\) counts number bases type \\(\\) ancestral state, \\(j\\) descendant.example:\\[\nS0: ATTACGGTT\n\\]\n\\[\nS1: AAGGTCGTT\n\\]matrix \\(N\\) (order always AGCT) becomes:\\[\nN = \\begin{pmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n1 & 1 & 0 & 2\n\\end{pmatrix}\n\\]want know maximum likelihood estimate time \\(S0\\) \\(S1\\) given model.example, choose Jukes-Cantor model, assume \\(p_0 = (1/4, 1/4, 1/4, 1/4)\\), probabilities observing given transition :\\[\nP(t) = \\text{diag}(p_0) M(t) = \\begin{pmatrix}\n(1- (t)) / 4  & (t) / 12 & (t) / 12 & (t) / 12 \\\\\n(t) / 12 & (1 -(t)) / 4 &  (t) / 12 & (t) / 12 \\\\\n(t) / 12 & (t) / 12 &  (1 -(t))/4 &  (t) / 12 \\\\\n(t) / 12 & (t) / 12 &   (t) / 12 & (1-(t)) / 4 \n\\end{pmatrix}\n\\]likelihood given edge length, \\(t\\) :\\[\nL(t \\vert N ) = \\prod_{,j} p_{ij}^{n_{ij}}\n\\]Taking log-likelihood:\\[\n\\log L(t \\vert N ) = \\sum_{,j} {n_{ij} \\log p_{ij}} = \\log(/ 12) \\sum_{j\\neq } n_{ij} + \\log((1-) / 4) \\sum_i n_{ii}\n\\]Taking derivative respect \\(t\\), setting zero, massaging equation obtain:\\[\n(\\hat{t}) = \\frac{\\sum_{j\\neq } n_{ij}}{\\sum_{j, } n_{ij}}\n\\]example, matrix \\((\\hat{t}) = 5/9\\). \\((t) = 3/4 (1 - e^{(-t 4/3)})\\), get \\(\\hat{t} \\approx 1.0124\\), setting maximum likelihood estimate length branch.Using method, can find maximum likelihood estimate branches tree. can propose tree, compute product likelihoods. Given many possible trees, need smart way explore space.\nalgorithm Felsenstein (1981, Evolutionary trees DNA sequences: maximum likelihood approach) first allow efficient search space trees, first propose method find optimal branch lengths.Compute using Jukes-Cantor model:Try optimizing allowing changes topology:","code":"\n# compute likelihoods\ntest_NJ <- pml(treeUPGMA, data=primates, model = \"JC\")\nprint(test_NJ)## \n##  loglikelihood: -3594.713 \n## \n## unconstrained loglikelihood: -1230.335 \n## \n## Rate matrix:\n##   a c g t\n## a 0 1 1 1\n## c 1 0 1 1\n## g 1 1 0 1\n## t 1 1 1 0\n## \n## Base frequencies:  \n## 0.25 0.25 0.25 0.25\nfitJC <- optim.pml(test_NJ, data=primates,\n                    rearrangement = \"NNI\")## Warning: I unrooted the tree## optimize edge weights:  -3594.713 --> -3246.1 \n## optimize edge weights:  -3246.1 --> -3246.097 \n## optimize topology:  -3246.097 --> -3232.503 \n## optimize topology:  -3232.503 --> -3192.642 \n## optimize topology:  -3192.642 --> -3156.934 \n## 4 \n## optimize edge weights:  -3156.934 --> -3156.934 \n## optimize topology:  -3156.934 --> -3135.999 \n## optimize topology:  -3135.999 --> -3127.961 \n## optimize topology:  -3127.961 --> -3117.217 \n## 4 \n## optimize edge weights:  -3117.217 --> -3117.217 \n## optimize topology:  -3117.217 --> -3114.172 \n## optimize topology:  -3114.172 --> -3114.172 \n## 1 \n## optimize edge weights:  -3114.172 --> -3114.172 \n## optimize topology:  -3114.172 --> -3114.172 \n## 0 \n## optimize edge weights:  -3114.172 --> -3114.172\nprint(fitJC)## \n##  loglikelihood: -3114.172 \n## \n## unconstrained loglikelihood: -1230.335 \n## \n## Rate matrix:\n##   a c g t\n## a 0 1 1 1\n## c 1 0 1 1\n## g 1 1 0 1\n## t 1 1 1 0\n## \n## Base frequencies:  \n## 0.25 0.25 0.25 0.25\nplot(fitJC)"},{"path":"building-phylogeneric-trees.html","id":"bayesian-methods","chapter":"Lecture 16 Building phylogeneric trees","heading":"16.6.3 Bayesian methods","text":"methods can extended include priors. can use flat prior ancestral sequence, base substitution model, space possible trees , via MCMC, construct posterior distribution parameters. hope find strong support (.e., posterior probability close one) single tree; alternatively, many trees high posterior can summarized consensus tree.","code":""}]
